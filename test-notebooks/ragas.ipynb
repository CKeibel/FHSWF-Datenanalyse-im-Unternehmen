{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "389955a8-021c-4449-ba9d-a389d5457a8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701843cab4344c41866d10a724d935e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ragas/executor.py\", line 93, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ragas/executor.py\", line 81, in _aresults\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ragas/executor.py\", line 76, in _aresults\n",
      "    r = await future\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ragas/executor.py\", line 36, in sema_coro\n",
      "    return await coro\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ragas/executor.py\", line 109, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ragas/metrics/base.py\", line 91, in ascore\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ragas/metrics/base.py\", line 87, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ragas/metrics/_answer_relevance.py\", line 136, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ragas/llms/base.py\", line 141, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 544, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 408, in generate\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 398, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 577, in _generate_with_cache\n",
      "    return self._generate(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 462, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 663, in create\n",
      "    return self._post(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/_base_client.py\", line 1200, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/_base_client.py\", line 889, in request\n",
      "    return self._request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/_base_client.py\", line 965, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/_base_client.py\", line 1013, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/_base_client.py\", line 965, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/_base_client.py\", line 1013, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/_base_client.py\", line 980, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-e57VZzlYB0OjTKb1j4xseguE on tokens per min (TPM): Limit 60000, Used 59571, Requested 3149. Please try again in 2.72s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m ds \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(df)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# evaluate with ragas\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m     45\u001b[0m result_rows\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ragas/evaluation.py:228\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluation_group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[1;32m    226\u001b[0m         evaluation_rm\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m     result \u001b[38;5;241m=\u001b[39m Result(\n\u001b[1;32m    231\u001b[0m         scores\u001b[38;5;241m=\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_list(scores),\n\u001b[1;32m    232\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m    233\u001b[0m         binary_columns\u001b[38;5;241m=\u001b[39mbinary_metrics,\n\u001b[1;32m    234\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ragas/evaluation.py:210\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mresults()\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# convert results to dataset_like\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n",
      "\u001b[0;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    ")\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "open_ai_auth = os.getenv(\"OPEN_AI\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = open_ai_auth\n",
    "\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "]\n",
    "\n",
    "results_path = Path(\"./results/\")\n",
    "\n",
    "result_rows = []\n",
    "for file in results_path.glob(\"*csv\"):\n",
    "    # read results csv\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # setup name\n",
    "    setup = file.name\n",
    "    \n",
    "    # model name\n",
    "    model = df[\"model\"].values[0]\n",
    "    \n",
    "    # df to ds transformation\n",
    "    df = pd.read_csv(file)\n",
    "    df = df[[\"question\", \"context\", \"answer\", \"correct_answer\"]]\n",
    "    df.columns = ['question','contexts','answer', \"ground_truth\"]\n",
    "    df[\"contexts\"] = df[\"contexts\"].apply(lambda c: [c])\n",
    "    \n",
    "    # convert df to ds\n",
    "    ds = Dataset.from_pandas(df)\n",
    "    \n",
    "    # evaluate with ragas\n",
    "    result = evaluate(ds, metrics=metrics)\n",
    "    result[\"model\"] = model\n",
    "    result_rows.append(result)\n",
    "    \n",
    "pd.DataFrame(result_rows).to_csv(\"ragas_results.csv\")\n",
    "pd.DataFrame(result_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6fa0cec-4910-433f-8263-037c4004e4a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

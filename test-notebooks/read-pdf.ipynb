{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a1a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.cleaners.core import clean_non_ascii_chars\n",
    "import fitz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f2897a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = Path(\"./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b38f79",
   "metadata": {},
   "source": [
    "# Reading pdfs with unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfc49c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675b80f2eeeb4a49bc7657399c8f9935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "files = dict()\n",
    "\n",
    "for file in files_path.glob(\"*.pdf\"):\n",
    "    file_contents = partition_pdf(file, extract_images_in_pdf=False)\n",
    "    cleaned_texts = \"\"\n",
    "    for element in file_contents:\n",
    "        cleaned_text = clean_non_ascii_chars(element.text)\n",
    "        cleaned_texts += f\"\\n{cleaned_text}\"\n",
    "    files[file.name] = cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efb8a008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['MTEB - Massive Text Embedding Benchmark.pdf', 'MarginMSELoss.pdf', 'Attention over pre-trained Sentence Embeddings for Long Document Classification.pdf'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea87126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## MTEB - Massive Text Embedding Benchmark.pdf ######## \n",
      "\n",
      "\n",
      "3 2 0 2\n",
      "r a\n",
      "M 9 1\n",
      "] L C . s c [\n",
      "3 v 6 1 3 7 0 . 0 1 2 2 : v i X r a\n",
      "MTEB: Massive Text Embedding Benchmark\n",
      "Niklas Muennighoff1, Nouamane Tazi1, Loc Magne1, Nils Reimers2*\n",
      "1Hugging Face\n",
      "2cohere.ai\n",
      "1firstname@hf.co\n",
      "2info@nils-reimers.de\n",
      "Abstract\n",
      "Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art em- beddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the eld difcult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we intro- duce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks cov- ering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We nd that no particular text embedding method dominates across all tasks. This suggests that the eld has yet to converge on a universal text embedding method and scale it up sufciently to provide state-of-the-art results on all embed- ding tasks. MTEB comes with open-source code and a public leaderboard at https: //github.com/embeddings-benchm ark/mteb.\n",
      "1\n",
      "Introduction\n",
      "Natural language embeddings power a variety of use cases from clustering and topic representa- tion (Aggarwal and Zhai, 2012; Angelov, 2020) to search systems and text mining (Huang et al., 2020; Zhu et al., 2021; Nayak, 2019) to feature representations for downstream models (Saharia et al., 2022; Borgeaud et al., 2022). Using gener- ative language models or cross-encoders for these applications is often intractable, as they may re- quire exponentially more computations (Reimers and Gurevych, 2019).\n",
      "their possible use cases. For example, Sim- CSE (Gao et al., 2021b) or SBERT (Reimers and Gurevych, 2019) solely evaluate on STS and clas- sication tasks, leaving open questions about the transferability of the embedding models to search or clustering tasks. STS is known to poorly corre- late with other real-world use cases (Neelakantan et al., 2022; Wang et al., 2021). Further, evaluating embedding methods on many tasks requires imple- menting multiple evaluation pipelines. Implemen- tation details like pre-processing or hyperparam- eters may inuence the results making it unclear whether performance improvements simply come from a favorable evaluation pipeline. This leads to the blind application of these models to new use cases in industry or requires incremental work to reevaluate them on different tasks.\n",
      "The Massive Text Embedding Benchmark (MTEB) aims to provide clarity on how models perform on a variety of embedding tasks and thus serves as the gateway to nding universal text em- beddings applicable to a variety of tasks. MTEB consists of 58 datasets covering 112 languages from 8 embedding tasks: Bitext mining, classi- cation, clustering, pair classication, reranking, retrieval, STS and summarization. MTEB software is available open-source1 enabling evaluation of any embedding model by adding less than 10 lines of code. Datasets and the MTEB leaderboard are available on the Hugging Face Hub2.\n",
      "We evaluate over 30 models on MTEB with addi- tional speed and memory benchmarking to provide a holistic view of the state of text embedding mod- els. We cover both models available open-source as well as models accessible via APIs, such as the OpenAI Embeddings endpoint. We nd there to be no single best solution, with different models dom-\n",
      "However, the evaluation regime of current text embedding models rarely covers the breadth of\n",
      "1https://github.com/embeddings-benchm\n",
      "ark/mteb\n",
      "Most of the work done while at Hugging Face. Corre-\n",
      "2https://huggingface.co/spaces/mteb/l\n",
      "spondence to n.muennighoff@gmail.com.\n",
      "eaderboard\n",
      "inating different tasks. Our benchmarking sheds light on the weaknesses and strengths of individual models, such as SimCSEs (Gao et al., 2021b) low performance on clustering and retrieval despite its strong performance on STS. We hope our work makes selecting the right embedding model easier and simplies future embedding research.\n",
      "2 Related Work\n",
      "2.1 Benchmarks\n",
      "Benchmarks, such as (Super)GLUE (Wang et al., 2018, 2019) or Big-BENCH (Srivastava et al., 2022), and evaluation frameworks (Gao et al., 2021a) play a key role in driving NLP progress. Yearly released SemEval datasets (Agirre et al., 2012, 2013, 2014, 2015, 2016) are commonly used as the go-to benchmark for text embeddings. Se- mEval datasets correspond to the task of semantic textual similarity (STS) requiring models to embed similar sentences with geometrically close embed- dings. Due to the limited expressivity of a single Se- mEval dataset, SentEval (Conneau and Kiela, 2018) aggregates multiple STS datasets. SentEval focuses on ne-tuning classiers on top of embeddings. It lacks tasks like retrieval or clustering, where em- beddings are directly compared without additional classiers. Further, the toolkit was proposed in 2018 and thus does not provide easy support for recent trends like text embeddings from transform- ers (Reimers and Gurevych, 2019). Due to the insufciency of STS benchmarking, USEB (Wang et al., 2021) was introduced consisting mostly of reranking tasks. Consequently, it does not cover tasks like retrieval or classication. Meanwhile, the recently released BEIR Benchmark (Thakur et al., 2021) has become the standard for the evaluation of embeddings for zero-shot information retrieval. MTEB unies datasets from different embed- ding tasks into a common, accessible evaluation framework. MTEB incorporates SemEval datasets (STS11 - STS22) and BEIR alongside a variety of other datasets from various tasks to provide a holis- tic performance review of text embedding models.\n",
      "2.2 Embedding Models\n",
      "Text embedding models like Glove (Pennington et al., 2014) lack context awareness and are thus commonly labeled as Word Embedding Models. They consist of a layer mapping each input word to a vector often followed by an averaging layer to provide a nal embedding invariant of input length.\n",
      "Transformers (Vaswani et al., 2017) inject context awareness into language models via self-attention and form the foundation of most recent embed- ding models. BERT (Devlin et al., 2018) uses the transformer architecture and performs large-scale self-supervised pre-training. The resulting model can directly be used to produce text embeddings via an averaging operation alike Glove. Build- ing on InferSent (Conneau et al., 2017), SBERT (Reimers and Gurevych, 2019) demonstrated it to be benecial to perform additional ne-tuning of the transformer for competitive embedding perfor- mance. Most recent ne-tuned embedding models use a contrastive loss objective to perform super- vised ne-tuning on positive and negative text pairs (Gao et al., 2021b; Wang et al., 2021; Ni et al., 2021b; Muennighoff, 2022). Due to the large va- riety of available pre-trained transformers (Wolf et al., 2020), there is an at least equally large va- riety of potential text embedding models to be ex- plored. This leads to confusion about which model provides practitioners with the best performance for their embedding use case.\n",
      "We benchmark both word embedding and trans- former models on MTEB quantifying gains pro- vided by often much slower context aware models.\n",
      "3 The MTEB Benchmark\n",
      "3.1 Desiderata\n",
      "MTEB is built on a set of desiderata: (a) Diversity: MTEB aims to provide an understanding of the usability of embedding models in various use cases. The benchmark comprises 8 different tasks, with up to 15 datasets each. Of the 58 total datasets in MTEB, 10 are multilingual, covering 112 differ- ent languages. Sentence-level and paragraph-level datasets are included to contrast performance on short and long texts. (b) Simplicity: MTEB pro- vides a simple API for plugging in any model that given a list of texts can produce a vector for each list item with a consistent shape. This makes it possible to benchmark a diverse set of models. (c) Extensibility: New datasets for existing tasks can be benchmarked in MTEB via a single le that species the task and a Hugging Face dataset name where the data has been uploaded (Lhoest et al., 2021). New tasks require implementing a task in- terface for loading the data and an evaluator for benchmarking. We welcome dataset, task or metric contributions from the community via pull requests to continue the development of MTEB. (d) Repro-\n",
      "ArguAna\n",
      "ArxivS2S\n",
      "STS13\n",
      "TwitterURLCorpus\n",
      "MTEB\n",
      "MassiveScenario\n",
      "Summarization\n",
      "Retrieval\n",
      "AskUbuntuDupQuestions\n",
      "MedrxivP2P\n",
      "BUCC\n",
      "STSB\n",
      "BiorxivS2S\n",
      "MTOPDomain\n",
      "Imdb\n",
      "TRECCOVID\n",
      "StackExchangeP2P\n",
      "STS16\n",
      "FEVER\n",
      "DBPedia\n",
      "SICK-R\n",
      "ArxivP2P\n",
      "BIOSESS\n",
      "STS22\n",
      "MSMARCO\n",
      "HotpotQA\n",
      "Reranking\n",
      "AmazonPolarity\n",
      "Clustering\n",
      "BiorxivP2P\n",
      "STS14\n",
      "STS15\n",
      "TweetSentimentExtraction\n",
      "StackOverFlowDupQuestions\n",
      "SciFact\n",
      "MassiveIntent\n",
      "CQADupstackRetrieval\n",
      "MindSmallReranking\n",
      "Banking77\n",
      "Emotion\n",
      "8 Tasks\n",
      "TwitterSemEval2015\n",
      "STS17\n",
      "Tatoeba\n",
      "MedrxivS2S\n",
      "58 DatasetsMassive Text Embedding Benchmark\n",
      "Touche2020\n",
      "AmazonReviews\n",
      "NFCorpus\n",
      "RedditP2P\n",
      "SCIDOCS\n",
      "StackExchange\n",
      "STS12\n",
      "SummEval\n",
      "TwentyNewsgroup\n",
      "AmazonCounterfactual\n",
      "NQ\n",
      "FiQA2018\n",
      "STS\n",
      "SprintDuplicateQuestions\n",
      "ClimateFEVER\n",
      "Pair Classification\n",
      "Classification\n",
      "STS11\n",
      "SciDocsRR\n",
      "Reddit\n",
      "Bitext Mining\n",
      "ToxicConversations\n",
      "Quora\n",
      "MTOPIntent\n",
      "Figure 1: An overview of tasks and datasets in MTEB. Multilingual datasets are marked with a purple shade.\n",
      "ducibility: Through versioning at a dataset and software level, we aim to make it easy to repro- duce results in MTEB. JSON les corresponding to all results available in this paper have been made available together with the MTEB benchmark3.\n",
      "3.2 Tasks and Evaluation\n",
      "Figure 1 provides an overview of tasks and datasets available in MTEB. Dataset statistics are available in Table 2. The benchmark consists of the follow- ing 8 task types:\n",
      "Bitext Mining Inputs are two sets of sentences from two different languages. For each sentence in the rst set, the best match in the second set needs to be found. The matches are commonly translations. The provided model is used to embed each sentence and the closest pairs are found via cosine similarity. F1 serves as the main metric for bitext mining. Accuracy, precision and recall are also computed.\n",
      "Clustering Given a set of sentences or para- graphs, the goal is to group them into meaning- ful clusters. A mini-batch k-means model with batch size 32 and k equal to the number of dif- ferent labels (Pedregosa et al., 2011) is trained on the embedded texts. The model is scored using v-measure (Rosenberg and Hirschberg, 2007). V- measure does not depend on the cluster label, thus the permutation of labels does not affect the score.\n",
      "Pair Classication A pair of text inputs is pro- vided and a label needs to be assigned. Labels are typically binary variables denoting duplicate or paraphrase pairs. The two texts are embedded and their distance is computed with various metrics (cosine similarity, dot product, euclidean distance, manhattan distance). Using the best binary thresh- old accuracy, average precision, f1, precision and recall are computed. The average precision score based on cosine similarity is the main metric.\n",
      "Classication A train and test set are embedded with the provided model. The train set embeddings are used to train a logistic regression classier with 100 maximum iterations, which is scored on the test set. The main metric is accuracy with average precision and f1 additionally provided.\n",
      "3https://huggingface.co/datasets/mteb\n",
      "Reranking Inputs are a query and a list of rele- vant and irrelevant reference texts. The aim is to rank the results according to their relevance to the query. The model is used to embed the references which are then compared to the query using cosine similarity. The resulting ranking is scored for each query and averaged across all queries. Metrics are mean MRR@k and MAP with the latter being the main metric.\n",
      "/results\n",
      "ArguAna\n",
      "TwitterURLCorpus\n",
      "MSMARCO\n",
      "STS13\n",
      "STS13\n",
      "Banking77Classification\n",
      "Banking77Classification\n",
      "CQADupstackWebmastersRetrieval\n",
      "SICK-R\n",
      "STS16\n",
      "AskUbuntuDupQuestions\n",
      "CQADupstackGisRetrieval\n",
      "CQADupstackGisRetrieval\n",
      "BiorxivClusteringP2P\n",
      "BiorxivClusteringP2P\n",
      "TwitterURLCorpus\n",
      "CQADupstackWebmastersRetrieval\n",
      "MSMARCO\n",
      "Touche2020\n",
      "AmazonCounterfactualClassification\n",
      "AmazonReviewsClassification\n",
      "AmazonReviewsClassification\n",
      "80\n",
      "BIOSSES\n",
      "AskUbuntuDupQuestions\n",
      "STS16\n",
      "BIOSSES\n",
      "STS14\n",
      "STS14\n",
      "CQADupstackPhysicsRetrieval\n",
      "CQADupstackPhysicsRetrieval\n",
      "STS22\n",
      "BiorxivClusteringS2S\n",
      "SICK-R\n",
      "STS22\n",
      "MedrxivClusteringS2S\n",
      "FEVER\n",
      "DBPedia\n",
      "DBPedia\n",
      "CQADupstackUnixRetrieval\n",
      "CQADupstackUnixRetrieval\n",
      "ArxivClusteringS2S\n",
      "ArguAna\n",
      "STS15\n",
      "STS15\n",
      "MTOPIntentClassification\n",
      "MTOPIntentClassification\n",
      "MassiveScenarioClassification\n",
      "MassiveScenarioClassification\n",
      "90\n",
      "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\n",
      "ArxivClusteringP2P\n",
      "HotpotQA\n",
      "ArxivClusteringP2P\n",
      "SciFact\n",
      "FEVER\n",
      "EmotionClassification\n",
      "EmotionClassification\n",
      "NFCorpus\n",
      "NFCorpus\n",
      "MedrxivClusteringS2S\n",
      "ArxivClusteringS2S\n",
      "CQADupstackAndroidRetrieval\n",
      "CQADupstackAndroidRetrieval\n",
      "MTOPDomainClassification\n",
      "MTOPDomainClassification\n",
      "ImdbClassification\n",
      "ImdbClassification\n",
      "SciFact\n",
      "HotpotQA\n",
      "BiorxivClusteringS2S\n",
      "CQADupstackGamingRetrieval\n",
      "CQADupstackStatsRetrieval\n",
      "100\n",
      "95\n",
      "NQ\n",
      "StackOverflowDupQuestions\n",
      "RedditClusteringP2P\n",
      "TwitterSemEval2015\n",
      "ToxicConversationsClassification\n",
      "TwitterSemEval2015\n",
      "SCIDOCS\n",
      "SCIDOCS\n",
      "RedditClusteringP2P\n",
      "RedditClustering\n",
      "AmazonCounterfactualClassification\n",
      "CQADupstackStatsRetrieval\n",
      "StackOverflowDupQuestions\n",
      "NQ\n",
      "ToxicConversationsClassification\n",
      "ClimateFEVER\n",
      "ClimateFEVER\n",
      "TweetSentimentExtractionClassification\n",
      "MindSmallReranking\n",
      "MindSmallReranking\n",
      "CQADupstackEnglishRetrieval\n",
      "TweetSentimentExtractionClassification\n",
      "CQADupstackProgrammersRetrieval\n",
      "STS17\n",
      "STS17\n",
      "QuoraRetrieval\n",
      "CQADupstackEnglishRetrieval\n",
      "Touche2020\n",
      "CQADupstackProgrammersRetrieval\n",
      "QuoraRetrieval\n",
      "STS12\n",
      "STS12\n",
      "RedditClustering\n",
      "FiQA2018\n",
      "SprintDuplicateQuestions\n",
      "StackExchangeClusteringP2P\n",
      "SciDocsRR\n",
      "SciDocsRR\n",
      "CQADupstackMathematicaRetrieval\n",
      "CQADupstackMathematicaRetrieval\n",
      "STSBenchmark\n",
      "StackExchangeClusteringP2P\n",
      "STSBenchmark\n",
      "StackExchangeClustering\n",
      "MassiveIntentClassification\n",
      "MassiveIntentClassification\n",
      "CQADupstackGamingRetrieval\n",
      "CQADupstackTexRetrieval\n",
      "MedrxivClusteringP2P\n",
      "SummEval\n",
      "FiQA2018\n",
      "CQADupstackWordpressRetrieval\n",
      "CQADupstackWordpressRetrieval\n",
      "SprintDuplicateQuestions\n",
      "75\n",
      "AmazonPolarityClassification\n",
      "MedrxivClusteringP2P\n",
      "StackExchangeClustering\n",
      "CQADupstackTexRetrieval\n",
      "TRECCOVID\n",
      "70\n",
      "TwentyNewsgroupsClustering\n",
      "TwentyNewsgroupsClustering\n",
      "TRECCOVID\n",
      "AmazonPolarityClassification\n",
      "85\n",
      "Figure 2: Similarity of MTEB datasets. We use the best model on MTEB STS (ST5-XXL, see Table 1) to embed 100 samples for each dataset. Cosine similarities between the averaged embeddings are computed and visualized.\n",
      "Retrieval Each dataset consists of a corpus, queries and a mapping for each query to relevant documents from the corpus. The aim is to nd these relevant documents. The provided model is used to embed all queries and all corpus documents and similarity scores are computed using cosine simi- larity. After ranking the corpus documents for each query based on the scores, nDCG@k, MRR@k, MAP@k, precision@k and recall@k are computed for several values of k. nDCG@10 serves as the main metric. MTEB reuses datasets and evaluation from BEIR (Thakur et al., 2021).\n",
      "Summarization A set of human-written and machine-generated summaries are provided. The aim is to score the machine summaries. The pro- vided model is rst used to embed all summaries. For each machine summary embedding, distances to all human summary embeddings are computed. The closest score (e.g. highest cosine similarity) is kept and used as the models score of a single machine-generated summary. Pearson and Spear- man correlations with ground truth human assess- ments of the machine-generated summaries are computed. Like for STS, Spearman correlation based on cosine similarity serves as the main met- ric (Reimers et al., 2016).\n",
      "Semantic Textual Similarity (STS) Given a sentence pair the aim is to determine their simi- larity. Labels are continuous scores with higher numbers indicating more similar sentences. The provided model is used to embed the sentences and their similarity is computed using various distance metrics. Distances are benchmarked with ground truth similarities using Pearson and Spearman cor- relations. Spearman correlation based on cosine similarity serves as the main metric (Reimers et al., 2016).\n",
      "3.3 Datasets\n",
      "To further the diversity of MTEB, datasets of vary- ing text lengths are included. All datasets are grouped into three categories:\n",
      "Sentence to sentence (S2S) A sentence is com- pared with another sentence. An example of S2S are all current STS tasks in MTEB, where the simi- larity between two sentences is assessed.\n",
      "Num. Datasets ()\n",
      "Class. Clust. PairClass. Rerank. Retr. 15 3\n",
      "12\n",
      "11\n",
      "4\n",
      "STS 10\n",
      "Summ. Avg. 56\n",
      "1\n",
      "Self-supervised methods\n",
      "Glove Komninos BERT SimCSE-BERT-unsup\n",
      "57.29 57.65 61.66 62.50\n",
      "27.73 26.57 30.12 29.04\n",
      "70.92 72.94 56.33 70.33\n",
      "43.29 44.75 43.44 46.47\n",
      "21.62 21.22 10.59 20.29\n",
      "61.85 62.47 54.36 74.33\n",
      "28.87 30.49 29.82 31.15\n",
      "41.97 42.06 38.33 45.45\n",
      "Supervised methods\n",
      "SimCSE-BERT-sup coCondenser-msmarco Contriever SPECTER LaBSE LASER2 MiniLM-L6 MiniLM-L12 MiniLM-L12-multilingual MPNet MPNet-multilingual OpenAI Ada Similarity SGPT-125M-nli SGPT-5.8B-nli SGPT-125M-msmarco SGPT-1.3B-msmarco SGPT-2.7B-msmarco SGPT-5.8B-msmarco SGPT-BLOOM-7.1B-msmarco GTR-Base GTR-Large GTR-XL GTR-XXL ST5-Base ST5-Large ST5-XL ST5-XXL\n",
      "67.32 64.71 66.68 52.37 62.71 53.65 63.06 63.21 64.30 65.07 67.91 70.44 61.46 70.14 60.72 66.52 67.13 68.13 66.19 65.25 67.14 67.11 67.41 69.81 72.31 72.84 73.42\n",
      "33.43 37.64 41.10 34.06 29.55 15.28 42.35 41.81 37.14 43.69 38.40 37.52 30.95 36.98 35.79 39.92 39.83 40.35 38.93 38.63 41.60 41.51 42.42 40.21 41.65 42.34 43.71\n",
      "73.68 81.74 82.53 61.37 78.87 68.86 82.37 82.41 78.45 83.04 80.81 76.86 71.78 77.03 75.23 79.58 80.65 82.00 81.90 83.85 85.33 86.13 86.12 85.17 84.97 86.06 85.06\n",
      "47.54 51.84 53.14 48.10 48.42 41.44 58.04 58.44 53.62 59.36 53.80 49.02 47.56 52.33 50.58 54.00 54.67 56.56 55.65 54.23 55.36 55.96 56.65 53.09 54.00 54.71 56.43\n",
      "21.82 32.96 41.88 15.88 18.99 7.93 41.95 42.69 32.45 43.81 35.34 18.36 20.90 32.34 37.04 44.49 46.54 50.25 48.21 44.67 47.42 47.96 48.48 33.63 36.71 38.47 42.24\n",
      "79.12 76.47 76.51 61.02 70.80 55.32 78.90 79.80 78.92 80.28 80.73 78.60 74.71 80.53 73.41 75.74 76.83 78.10 77.74 77.07 78.19 77.80 78.38 81.14 81.83 81.66 82.63\n",
      "23.31 29.50 30.36 27.66 31.05 26.80 30.81 27.90 30.67 27.49 31.57 26.94 30.26 30.38 28.90 25.44 27.87 24.75 24.99 29.67 29.50 30.21 30.64 31.39 29.64 29.91 30.08\n",
      "48.72 52.35 56.00 40.28 45.21 33.63 56.26 56.53 52.44 57.78 54.71 49.52 45.97 53.74 51.23 56.11 57.12 58.81 57.44 56.19 58.28 58.42 58.97 55.27 57.06 57.87 59.51\n",
      "Table 1: Average of the main metric (see Section 3.2) per task per model on MTEB English subsets.\n",
      "Paragraph to paragraph (P2P) A paragraph is compared with another paragraph. MTEB imposes no limit on the input length, leaving it up to the models to truncate if necessary. Several clustering tasks are framed as both S2S and P2P tasks. The former only compare titles, while the latter include both title and content. For ArxivClustering, for example, abstracts are concatenated to the title in the P2P setting.\n",
      "the same corpora, such as ClimateFEVER and FEVER, resulting in a score of 1. Clusters of simi- lar datasets can be seen among CQADupstack vari- ations and STS datasets. S2S and P2P variations of the same dataset tend to also be similar. Scientic datasets, such as SciDocsRR, SciFact, ArxivClus- tering, show high similarities among each other even when coming from different tasks (Reranking, Retrieval and Clustering in this case).\n",
      "Sentence to paragraph (S2P) A few retrieval datasets are mixed in a S2P setting. Here a query is a single sentence, while documents are long paragraphs consisting of multiple sentences.\n",
      "Similarities across 56 MTEB datasets are vi- sualized in Figure 2. Several datasets rely on\n",
      "4 Results\n",
      "4.1 Models\n",
      "We evaluate on the test splits of all datasets except for MSMARCO, where the dev split is used follow- ing Thakur et al. (2021). We benchmark models claiming state-of-the-art results on various embed-\n",
      "0.64\n",
      "0.66\n",
      "0.425\n",
      "Retrieval\n",
      "0.475\n",
      "0.80\n",
      "0.80\n",
      "0.53\n",
      "0.74\n",
      "0.82Average Performance (cos. sim. spearman corr.)\n",
      "0.76\n",
      "0.76\n",
      "0.37\n",
      "0.44Average Performance (v_measure)\n",
      "0.1B\n",
      "0.1B\n",
      "0.42\n",
      "0.72\n",
      "0.1B\n",
      "0.1B\n",
      "SGPT\n",
      "4BModel Parameters (Billions)\n",
      "4BModel Parameters (Billions)\n",
      "4BModel Parameters (Billions)\n",
      "4BModel Parameters (Billions)\n",
      "4BModel Parameters (Billions)\n",
      "2B\n",
      "Clustering\n",
      "Reranking\n",
      "0.1B\n",
      "2B\n",
      "2B\n",
      "2B\n",
      "2B\n",
      "2B\n",
      "0.1B\n",
      "0.82\n",
      "0.38\n",
      "0.78\n",
      "1B\n",
      "0.450\n",
      "0.74Average Performance (accuracy)\n",
      "0.62\n",
      "0.70\n",
      "PairClassification\n",
      "0.43\n",
      "0.86Average Performance (ap)\n",
      "0.52\n",
      "0.56Average Performance (map)\n",
      "0.84\n",
      "Classification\n",
      "0.54\n",
      "0.375\n",
      "0.36\n",
      "0.40\n",
      "0.55\n",
      "0.350\n",
      "0.39\n",
      "0.41\n",
      "ST5\n",
      "STS\n",
      "0.500Average Performance (nDCG@10)\n",
      "0.51\n",
      "0.68\n",
      "1B\n",
      "1B\n",
      "0.78\n",
      "1B\n",
      "1B\n",
      "1B\n",
      "GTR\n",
      "0.400\n",
      "4BModel Parameters (Billions)\n",
      "Figure 3: MTEB performance scales with model size. The smallest SGPT variant underperforms similar- sized GTR and ST5 variants. This may be due to the bias-only ne-tuning SGPT employs, which catches up with full ne-tuning only as model size and thus the number of bias parameters increases (Muennighoff, 2022).\n",
      "ding tasks leading to a high representation of trans- formers (Vaswani et al., 2017). We group models into self-supervised and supervised methods.\n",
      "(a) Transformer- Self-supervised methods based BERT (Devlin et al., 2018) is trained using self-supervised mask and sentence prediction tasks. By taking the mean across the sequence length (mean-pooling) the model can directly be used to produce text embeddings. SimCSE-Unsup (Gao et al., 2021b) uses BERT as a foundation and performs additional self-supervised training. (b) Non-transformer: Komninos (Komninos and Manandhar, 2016) and Glove (Pennington et al., 2014) are two word embedding models that directly map words to vectors. Hence, their embeddings lack context awareness, but provide signicant speed-ups.\n",
      "Supervised methods The original transformer model (Vaswani et al., 2017) consists of an encoder and decoder network. Subsequent transformers often train only encoders like BERT (Devlin et al.,\n",
      "2018) or decoders like GPT (Radford et al., 2019). (a) Transformer encoder methods coCon- denser (Gao and Callan, 2021), Contriever (Izac- ard et al., 2021), LaBSE (Feng et al., 2020) and SimCSE-BERT-sup (Gao et al., 2021b) are based on the pre-trained BERT model (Devlin et al., 2018). coCondenser and Contriever add a self- supervised stage prior to supervised ne-tuning for a total of three training stages. LaBSE uses BERT to perform additional pre-training on par- allel data to produce a competitive bitext mining model. SPECTER (Cohan et al., 2020a) relies on the pre-trained SciBERT (Beltagy et al., 2019) vari- ant instead and ne-tunes on citation graphs. GTR (Ni et al., 2021b) and ST5 (Ni et al., 2021a) are based on the encoder part of the T5 model (Raf- fel et al., 2020) and only differ in their ne-tuning datasets. After additional self-supervised training, ST5 does contrastive ne-tuning on NLI (Ni et al., 2021a; Gao et al., 2021b) being geared towards STS tasks. Meanwhile, GTR ne-tunes on MS- MARCO and focuses on retrieval tasks. MPNet and MiniLM correspond to ne-tuned embedding models (Reimers and Gurevych, 2019) of the pre- trained MPNet (Song et al., 2020) and MiniLM (Wang et al., 2020) models using diverse datasets to target any embedding use case.\n",
      "(b) Transformer decoder methods SGPT Bi- Encoders (Muennighoff, 2022) perform contrastive ne-tuning of <0.1% of pre-trained parameters us- ing weighted-mean pooling. Similar to ST5 and GTR, SGPT-nli models are geared towards STS, while SGPT-msmarco models towards retrieval. SGPT-msmarco models embed queries and doc- uments for retrieval with different special tokens to help the model distinguish their role. For non- retrieval tasks, we use its query representations. We benchmark publicly available SGPT models based on GPT-NeoX (Andonian et al., 2021), GPT- J (Wang and Komatsuzaki, 2021) and BLOOM (Scao et al., 2022). Alternatively, cpt-text (Nee- lakantan et al., 2022) passes pre-trained GPT de- coders through a two-stage process using last token pooling to provide embeddings from decoders. We benchmark their models via the OpenAI Embed- dings API4.\n",
      "(c) Non-transformer LASER (Heffernan et al., 2022) is the only context aware non-transformer relying on an LSTM model we benchmark,\n",
      "4https://beta.openai.com/docs/guides/\n",
      "embeddings\n",
      "T5\n",
      "55\n",
      "50\n",
      "MPNet\n",
      "GPT\n",
      "Speed (examples per sec)\n",
      "102\n",
      "60MTEB Score\n",
      "SciBERT\n",
      "103\n",
      "Base Architecture\n",
      "45\n",
      "LASER\n",
      "104\n",
      "MiniLM\n",
      "LASER2KomninosGloveSGPT-125M-nliSGPT-125M-msmarcoSGPT-5.8B-nliSGPT-5.8B-msmarcoMiniLM-L6MPNetST5-BaseST5-XXLGTR-BaseGTR-XXLContrievercoCondenser-msmarcoBERTSimCSE-BERT-supSimCSE-BERT-unsupLaBSEMiniLM-L12SPECTER\n",
      "WordEmbeddings\n",
      "40\n",
      "BERT\n",
      "35\n",
      "Figure 4: Performance, speed, and size of produced embeddings (size of the circles) of different embedding models. Embedding sizes range from 1.2 kB (Glove / Komninos) to 16.4 kB (SGPT-5.8B) per example. Speed was benchmarked on STS15 using 1x Nvidia A100 80GB with CUDA 11.6.\n",
      "(Hochreiter and Schmidhuber, 1997) instead. Simi- lar to LaBSE, the model trains on parallel data and focuses on bitext mining applications.\n",
      "4.2 Analysis\n",
      "Based on the results in Table 1, we observe that there is considerable variability between tasks. No model claims the state-of-the-art in all seven En- glish tasks. There is even more variability in the results per dataset present in the appendix. Further, there remains a large gap between self-supervised and supervised methods. Self-supervised large lan- guage models have been able to close this gap in many natural language generation tasks (Chowd- hery et al., 2022). However, they appear to still require supervised ne-tuning for competitive em- bedding performance.\n",
      "We nd that performance strongly correlates with model size, see Figure 3. A majority of MTEB tasks are dominated by multi-billion param- eter models. However, these come at a signicant cost as we investigate in Section 4.3.\n",
      "the best non-ST5 model, OpenAI Ada Similarity.\n",
      "Clustering Despite being almost 50x smaller, the MPNet embedding model is on par with the ST5- XXL state-of-the-art on Clustering. This may be due to the large variety of datasets MPNet (and MiniLM) has been ne-tuned on. Clustering re- quires coherent distances between a large number of embeddings. Models like SimCSE-sup or SGPT- nli, which are only ne-tuned on a single dataset, NLI, may produce incoherent embeddings when encountering topics unseen during ne-tuning. Re- latedly, we nd that the query embeddings of SGPT- msmarco and the Ada Search endpoint are competi- tive with SGPT-nli and the Ada Similarity endpoint, respectively. We refer to the public leaderboard5 for Ada Search results. This could be due to the MSMARCO dataset being signicantly larger than NLI. Thus, while the OpenAI docs recommend us- ing the similarity embeddings for clustering use cases6, the retrieval query embeddings may be the better choice in some cases.\n",
      "Classication ST5 models dominate the classi- cation task across most datasets, as can be seen in detail in the full results in the appendix. ST5-XXL has the highest average performance, 3% ahead of\n",
      "5https://huggingface.co/spaces/mteb/l\n",
      "eaderboard\n",
      "6https://beta.openai.com/docs/guides/\n",
      "embeddings/similarity-embeddings\n",
      "cbk-eng\n",
      "nds-eng\n",
      "hsb-eng\n",
      "kur-eng\n",
      "bul-eng\n",
      "dsb-eng\n",
      "kor-eng\n",
      "arq-eng\n",
      "ces-eng\n",
      "tuk-eng\n",
      "hin-eng\n",
      "ron-eng\n",
      "lvs-eng\n",
      "fra-eng\n",
      "0.8\n",
      "dan-eng\n",
      "nov-eng\n",
      "mon-eng\n",
      "mkd-eng\n",
      "pes-eng\n",
      "swh-eng\n",
      "SGPT-BLOOM-7.1B-msmarco\n",
      "est-eng\n",
      "fin-eng\n",
      "swg-eng\n",
      "slv-eng\n",
      "nno-eng\n",
      "0.2\n",
      "tha-eng\n",
      "0.0\n",
      "tat-eng\n",
      "tzl-eng\n",
      "dtp-eng\n",
      "MPNet-multilingual\n",
      "ita-eng\n",
      "ast-eng\n",
      "ang-eng\n",
      "cor-eng\n",
      "kat-eng\n",
      "fao-eng\n",
      "bre-eng\n",
      "tel-eng\n",
      "gle-eng\n",
      "max-eng\n",
      "khm-eng\n",
      "kaz-eng\n",
      "afr-eng\n",
      "eus-eng\n",
      "aze-eng\n",
      "ina-eng\n",
      "epo-eng\n",
      "uig-eng\n",
      "1.0F1 score\n",
      "srp-eng\n",
      "jav-eng\n",
      "lfn-eng\n",
      "rus-eng\n",
      "hun-eng\n",
      "deu-eng\n",
      "tam-eng\n",
      "cmn-eng\n",
      "zsm-eng\n",
      "ell-eng\n",
      "ben-eng\n",
      "bos-eng\n",
      "por-eng\n",
      "cat-eng\n",
      "amh-eng\n",
      "kzj-eng\n",
      "wuu-eng\n",
      "ceb-eng\n",
      "vie-eng\n",
      "oci-eng\n",
      "urd-eng\n",
      "kab-eng\n",
      "LASER2\n",
      "gla-eng\n",
      "ido-eng\n",
      "awa-eng\n",
      "hrv-eng\n",
      "tgl-eng\n",
      "lit-eng\n",
      "uzb-eng\n",
      "mhr-eng\n",
      "heb-eng\n",
      "0.6\n",
      "MiniLM-L12-multilingual\n",
      "csb-eng\n",
      "arz-eng\n",
      "sqi-eng\n",
      "slk-eng\n",
      "isl-eng\n",
      "LaBSE\n",
      "ber-eng\n",
      "war-eng\n",
      "cha-eng\n",
      "yid-eng\n",
      "ukr-eng\n",
      "cym-eng\n",
      "nob-eng\n",
      "jpn-eng\n",
      "mal-eng\n",
      "ile-eng\n",
      "bel-eng\n",
      "pam-eng\n",
      "pol-eng\n",
      "ind-eng\n",
      "hye-eng\n",
      "tur-eng\n",
      "spa-eng\n",
      "glg-eng\n",
      "gsw-eng\n",
      "mar-eng\n",
      "fry-eng\n",
      "orv-eng\n",
      "ara-eng\n",
      "0.4\n",
      "lat-eng\n",
      "xho-eng\n",
      "pms-eng\n",
      "nld-eng\n",
      "swe-eng\n",
      "yue-eng\n",
      "(a) Bitext Mining on Tatoeba\n",
      "he\n",
      "sq\n",
      "hy\n",
      "my\n",
      "ja\n",
      "0.1\n",
      "nl\n",
      "ko\n",
      "0.3\n",
      "te\n",
      "fa\n",
      "zh-CN\n",
      "pl\n",
      "cy\n",
      "km\n",
      "th\n",
      "ka\n",
      "0.2\n",
      "af\n",
      "ml\n",
      "zh\n",
      "lv\n",
      "nb\n",
      "fr\n",
      "ta\n",
      "ur\n",
      "tr\n",
      "it\n",
      "kn\n",
      "zh-TW\n",
      "0.5\n",
      "de\n",
      "az\n",
      "sl\n",
      "sw\n",
      "id\n",
      "ro\n",
      "ar\n",
      "da\n",
      "sv\n",
      "0.6\n",
      "fi\n",
      "es\n",
      "hi\n",
      "jv\n",
      "am\n",
      "el\n",
      "vi\n",
      "pt\n",
      "mn\n",
      "ru\n",
      "0.4\n",
      "ms\n",
      "is\n",
      "hu\n",
      "tl\n",
      "en\n",
      "bn\n",
      "0.7Accuracy\n",
      "ar\n",
      "0.4\n",
      "en-ar\n",
      "0.3\n",
      "ru\n",
      "zh-en\n",
      "en-tr\n",
      "de-fr\n",
      "es\n",
      "0.8Cos. Sim. Spearman Corr.\n",
      "fr-pl\n",
      "0.2\n",
      "pl\n",
      "it\n",
      "de-en\n",
      "fr\n",
      "ko\n",
      "0.1\n",
      "0.6\n",
      "nl-en\n",
      "it-en\n",
      "de\n",
      "0.7\n",
      "en-de\n",
      "zh\n",
      "fr-en\n",
      "0.5\n",
      "es-it\n",
      "pl-en\n",
      "de-pl\n",
      "en\n",
      "es-en\n",
      "tr\n",
      "(b) Multilingual Classication\n",
      "(c) Multi- and Crosslingual STS\n",
      "Figure 5: MTEB multilingual performance. Bitext mining is dominated by LaBSE, while classication and STS results are mixed. SGPT-BLOOM-7B1-msmarco tends to perform well on the languages BLOOM has been pre- trained on, such as Chinese, French and Portuguese.\n",
      "Pair Classication GTR-XL and GTR-XXL have the strongest performance. Pair classica- tion is closest to STS in its framing, yet models rank signicantly differently on the two tasks. This highlights the importance of benchmarking on a diverse set of tasks to avoid blindly reusing a model for a different task.\n",
      "Reranking MPNet and MiniLM models perform strongly on reranking tasks. On SciDocsRR (Co- han et al., 2020a) they perform far better than big- ger models, which is likely due to parts of Sci- DocsRR being included in their training data. Our scale of experiments and that of model pre-training make controlling for data contamination challeng- ing. Thus, we ignore overlap of MTEB datasets with model training datasets in MTEB scores. As long as enough datasets are averaged, we believe these effects to be insignicant.\n",
      "Retrieval SGPT-5.8B-msmarco is the best em- bedding model on the BEIR subset in MTEB as well as on the full BEIR benchmark (Thakur et al., 2021; Muennighoff, 2022). The even larger 7.1B SGPT model making use of BLOOM (Scao et al., 2022) performs signicantly weaker, which is likely due to the multilinguality of BLOOM. Models geared towards STS (SimCSE, ST5, SGPT- nli) perform badly on retrieval tasks. Retrieval tasks are unique in that there are two distinct types of texts: Queries and documents (asymmetric), while other tasks only have a single type of text (symmetric). On the QuoraRetrieval dataset, which has been shown to be largely symmetric\n",
      "(Muennighoff, 2022), the playing eld is more even with SGPT-5.8B-nli outperforming SGPT- 5.8B-msmarco, see Table 11.\n",
      "STS & Summarization Retrieval models (GTR, SGPT-msmarco) perform badly on STS, while ST5- XXL has the highest performance. This highlights the bifurcation of the eld into separate embedding models for retrieval (asymmetric) and similarity (symmetric) use cases (Muennighoff, 2022).\n",
      "4.3 Efciency\n",
      "We investigate the latency-performance trade-off of models in Figure 4. The graph allows for signi- cant elimination of model candidates in the model selection process. It brings model selection down to three clusters:\n",
      "Maximum speed Word Embedding models offer maximum speed with Glove taking the lead on both performance and speed, thus making the choice simple in this case.\n",
      "Maximum performance If latency is less impor- tant than performance, the left-hand side of the graph offers a cluster of highly performant, but slow models. Depending on the task at hand, GTR- XXL, ST5-XXL or SGPT-5.8B may be the right choice, see Section 4.2. SGPT-5.8B comes with the additional caveat of its high-dimensional em- beddings requiring more storage.\n",
      "Speed and performance The ne-tuned MPNet and MiniLM models lead the middle cluster mak- ing the choice easy.\n",
      "4.4 Multilinguality\n",
      "MTEB comes with 10 multilingual datasets across bitext mining, classication and STS tasks. We in- vestigate performance on these in Figure 5. Tabular results can be found in Tables 12, 13 and 14.\n",
      "Bitext Mining LaBSE (Feng et al., 2020) per- forms strongly across a wide array of languages in bitext mining. Meanwhile, LASER2 shows high variance across different languages. While there are additional language-specic LASER2 models available for some of the languages we benchmark, we use the default multilingual LASER2 model for all languages. This is to provide a fair one-to- one comparison of models. In practice, however, the high variance of LASER2s performance may be resolved by mixing its model variants. MP- Net, MiniLM and SGPT-BLOOM-7B1-msmarco perform poorly on languages they have not been pre-trained on, such as German for the latter.\n",
      "Classication & STS On multilingual classi- cation and STS, the multilingual MPNet provides the overall strongest performance. It outperforms the slightly faster multilingual MiniLM on almost all languages. Both models have been trained on the same languages, thus bringing decision- making down to performance vs speed. SGPT- BLOOM-7B1-msmarco provides state-of-the-art performance on languages like Hindi, Portuguese, Chinese or French, which the model has seen ex- tensively during pre-training. It also performs com- petitively on languages like Russian or Japanese that unintentionally leaked into its pre-training data (Muennighoff et al., 2022). However, it is not much ahead of the much cheaper MPNet. LASER2 performs consistently worse than other models.\n",
      "5 Conclusion\n",
      "In this work, we presented the Massive Text Em- bedding Benchmark (MTEB). Consisting of 8 text embedding tasks with up to 15 datasets each and covering 112 languages, MTEB aims to provide re- liable embedding performance estimates. By open- sourcing MTEB alongside a leaderboard, we pro- vide a foundation for further pushing the state-of- the-art of available text embeddings.\n",
      "To introduce MTEB, we have conducted the most comprehensive benchmarking of text embed- dings to date. Through the course of close to 5,000 experiments on over 30 different models, we have set up solid baselines for future research to build\n",
      "on. We found model performance on different tasks to vary strongly with no model claiming state-of- the-art on all tasks. Our studies on scaling behav- ior, model efciency and multilinguality revealed various intricacies of models that should ease the decision-making process for future research or in- dustry applications of text embeddings.\n",
      "We welcome task, dataset or metric contributions to the MTEB codebase7 as well as additions to the leaderboard via our automatic submission format8.\n",
      "7https://github.com/embeddings-benchm\n",
      "ark/mteb\n",
      "8https://huggingface.co/spaces/mteb/l\n",
      "eaderboard\n",
      "Acknowledgments\n",
      "This work was granted access to the HPC resources of Institut du dveloppement et des ressources en informatique scientique (IDRIS) du Centre na- tional de la recherche scientique (CNRS) under the allocation 2021-A0101012475 made by Grand quipement national de calcul intensif (GENCI). In particular, all the evaluations and data processing ran on the Jean Zay cluster of IDRIS, and we want to thank the IDRIS team for responsive support throughout the project, in particular Rmi Lacroix. We thank Douwe Kiela, Teven Le Scao and Nan-\n",
      "dan Thakur for feedback and suggestions.\n",
      "References\n",
      "Charu C Aggarwal and ChengXiang Zhai. 2012. A survey of text clustering algorithms. In Mining text data, pages 77128. Springer.\n",
      "Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, et al. 2015. Semeval-2015 task 2: Seman- tic textual similarity, english, spanish and pilot on In Proceedings of the 9th interna- interpretability. tional workshop on semantic evaluation (SemEval 2015), pages 252263.\n",
      "Eneko Agirre, Carmen Banea, Claire Cardie, Daniel M Cer, Mona T Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. Semeval-2014 task 10: Multilingual semantic textual similarity. In SemEval@ COLING, pages 8191.\n",
      "Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez Agirre, Rada Mihalcea, Ger- man Rigau Claramunt, and Janyce Wiebe. 2016. Semeval-2016 task 1: Semantic textual similar- ity, monolingual and cross-lingual evaluation. In SemEval-2016. 10th International Workshop on Se- mantic Evaluation; 2016 Jun 16-17; San Diego, CA. Stroudsburg (PA): ACL; 2016. p. 497-511. ACL (As- sociation for Computational Linguistics).\n",
      "Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi- lot on semantic textual similarity. In * SEM 2012: The First Joint Conference on Lexical and Compu- tational SemanticsVolume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 385 393.\n",
      "Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez- Agirre, and Weiwei Guo. 2013. * sem 2013 shared In Second joint task: Semantic textual similarity. conference on lexical and computational semantics\n",
      "(* SEM), volume 1: proceedings of the Main confer- ence and the shared task: semantic textual similar- ity, pages 3243.\n",
      "Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. 2023. Santa- arXiv preprint coder: dont reach for the stars! arXiv:2301.03988.\n",
      "Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Shivanshu Purohit, Tri Songz, Phil Wang, and Samuel Weinbach. 2021. GPT-NeoX: Large scale autoregressive language modeling in pytorch.\n",
      "Dimo Angelov. 2020. Top2vec: Distributed representa- tions of topics. arXiv preprint arXiv:2008.09470.\n",
      "Akari Asai, Jungo Kasai, Jonathan H Clark, Kenton Lee, Eunsol Choi, and Hannaneh Hajishirzi. 2020. Xor qa: Cross-lingual open-retrieval question an- swering. arXiv preprint arXiv:2010.11856.\n",
      "Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scib- ert: A pretrained language model for scientic text. arXiv preprint arXiv:1903.10676.\n",
      "Sebastian Borgeaud, Arthur Mensch, Jordan Hoff- mann, Trevor Cai, Eliza Rutherford, Katie Milli- can, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022. Improving language models by retrieving from tril- lions of tokens. In International Conference on Ma- chine Learning, pages 22062240. PMLR.\n",
      "Micael Carvalho, Rmi Cadne, David Picard, Laure Soulier, Nicolas Thome, and Matthieu Cord. 2018. Cross-modal retrieval in the cooking context: Learn- In The 41st ing semantic text-image embeddings. International ACM SIGIR Conference on Research & Development in Information Retrieval, pages 35 44.\n",
      "Iigo Casanueva, Tadas Temcinas, Daniela Gerz, Matthew Henderson, and Ivan Vulic. 2020. Efcient intent detection with dual sentence encoders.\n",
      "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.\n",
      "Jonathan H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020. Tydi qa: A benchmark for information-seeking question answering in typo- logically diverse languages. Transactions of the As- sociation for Computational Linguistics, 8:454470.\n",
      "Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Specter: learning using arXiv preprint\n",
      "Downey, and Daniel S Weld. 2020a. Document-level representation citation-informed transformers. arXiv:2004.07180.\n",
      "Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Specter: using\n",
      "Downey, and Daniel S. Weld. 2020b. Document-level citation-informed transformers.\n",
      "representation\n",
      "learning\n",
      "Alexis Conneau and Douwe Kiela. 2018. Senteval: An evaluation toolkit for universal sentence representa- tions. arXiv preprint arXiv:1803.05449.\n",
      "Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from arXiv preprint natural language inference data. arXiv:1705.02364.\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805.\n",
      "Alexander R. Fabbri, Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2020. Summeval: Re-evaluating summarization evaluation.\n",
      "Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2020. Language- agnostic bert sentence embedding. arXiv preprint arXiv:2007.01852.\n",
      "Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann, Ana Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, Swetha Ranganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tur, and Prem Natara- jan. 2022. Massive: A 1m-example multilin- gual natural language understanding dataset with 51 typologically-diverse languages.\n",
      "Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPo, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. 2021a. A framework for few-shot language model evaluation. Version v0. 0.1. Sept.\n",
      "Luyu Gao and Jamie Callan. 2021. Unsupervised cor- pus aware language model pre-training for dense passage retrieval. arXiv preprint arXiv:2108.05540.\n",
      "Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021b. Simcse: Simple contrastive learning of sentence em- beddings. arXiv preprint arXiv:2104.08821.\n",
      "Gregor Geigle, Nils Reimers, Andreas Rckl, and Iryna Gurevych. 2021. Tweac: Transformer with ex- tendable qa agent classiers.\n",
      "Kevin Heffernan, Onur elebi, and Holger Schwenk. 2022. Bitext mining using distilled sentence rep- arXiv resentations for low-resource languages. preprint arXiv:2205.12654.\n",
      "Sepp Hochreiter and Jrgen Schmidhuber. 1997. Neural computation,\n",
      "Long short-term memory. 9(8):17351780.\n",
      "Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanab- han, Giuseppe Ottaviano, and Linjun Yang. 2020. Embedding-based retrieval in facebook search. In Proceedings of the 26th ACM SIGKDD Interna- tional Conference on Knowledge Discovery & Data Mining, pages 25532561.\n",
      "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Code- searchnet challenge: Evaluating the state of seman- tic code search. arXiv preprint arXiv:1909.09436.\n",
      "Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se- bastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Towards unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118.\n",
      "Alexandros Komninos and Suresh Manandhar. 2016. Dependency based embeddings for sentence classi- In Proceedings of the 2016 confer- cation tasks. ence of the North American chapter of the associa- tion for computational linguistics: human language technologies, pages 14901500.\n",
      "Wuwei Lan, Siyu Qiu, Hua He, and Wei Xu. 2017. A continuously growing dataset of sentential para- phrases. In Proceedings of The 2017 Conference on Empirical Methods on Natural Language Process- ing (EMNLP), pages 12351245. Association for Computational Linguistics.\n",
      "Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, et al. 2021. Datasets: A commu- nity library for natural language processing. arXiv preprint arXiv:2109.02846.\n",
      "Haoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar Mehdad. 2020. Mtop: A comprehensive multilingual task-oriented semantic parsing benchmark.\n",
      "Xueqing Liu, Chi Wang, Yue Leng, and ChengXiang Zhai. 2018. Linkso: a dataset for learning to retrieve similar question answer pairs on software develop- ment forums. In Proceedings of the 4th ACM SIG- SOFT International Workshop on NLP for Software Engineering, pages 25.\n",
      "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analy- sis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu- man Language Technologies, pages 142150, Port- land, Oregon, USA. Association for Computational Linguistics.\n",
      "Julian McAuley and Jure Leskovec. 2013. Hidden fac- tors and hidden topics: Understanding rating dimen- sions with review text. RecSys 13, New York, NY, USA. Association for Computing Machinery.\n",
      "Niklas Muennighoff. 2020. Vilio: State-of-the-art visio-linguistic models applied to hateful memes. arXiv preprint arXiv:2012.07788.\n",
      "Niklas Muennighoff. 2022.\n",
      "Sgpt: Gpt sentence arXiv preprint\n",
      "embeddings for semantic search. arXiv:2202.08904.\n",
      "Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hai- ley Schoelkopf, et al. 2022. Crosslingual general- ization through multitask netuning. arXiv preprint arXiv:2211.01786.\n",
      "Pandu Nayak. 2019. Understanding searches better\n",
      "than ever before.\n",
      "Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. 2022. Text and code embeddings by contrastive pre- training. arXiv preprint arXiv:2201.10005.\n",
      "Jianmo Ni, Gustavo Hernndez brego, Noah Con- stant, Ji Ma, Keith B Hall, Daniel Cer, and Yinfei Yang. 2021a. Sentence-t5: Scalable sentence en- coders from pre-trained text-to-text models. arXiv preprint arXiv:2108.08877.\n",
      "Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus- tavo Hernndez brego, Ji Ma, Vincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. 2021b. Large dual encoders are generalizable re- trievers. arXiv preprint arXiv:2112.07899.\n",
      "Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2021. Glide: To- wards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741.\n",
      "James ONeill, Polina Rozenshtein, Ryuichi Kiryo, Motoko Kubota, and Danushka Bollegala. 2021. I wish i would have loved this one, but i didnt  a multilingual dataset for counterfactual detection in product reviews.\n",
      "F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch- Scikit-learn: Machine learning in esnay. 2011. Journal of Machine Learning Research, Python. 12:28252830.\n",
      "Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word rep- resentation. In Proceedings of the 2014 conference on empirical methods in natural language process- ing (EMNLP), pages 15321543.\n",
      "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Lan- guage models are unsupervised multitask learners. OpenAI blog, 1(8):9.\n",
      "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unied text-to-text trans- former. J. Mach. Learn. Res., 21(140):167.\n",
      "Nils Reimers, Philip Beyer, and Iryna Gurevych. 2016. Task-oriented intrinsic evaluation of semantic tex- In Proceedings of COLING 2016, tual similarity. the 26th International Conference on Computational Linguistics: Technical Papers, pages 8796.\n",
      "Nils Reimers and Iryna Gurevych. 2019. Sentence- bert: Sentence embeddings using siamese bert- networks. arXiv preprint arXiv:1908.10084.\n",
      "Facebook Research. Tatoeba multilingual test set.\n",
      "Andrew Rosenberg and Julia Hirschberg. 2007. V- measure: A conditional entropy-based external clus- ter evaluation measure. pages 410420.\n",
      "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. 2022. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487.\n",
      "Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. 2018. CARER: Con- textualized affect representations for emotion recog- In Proceedings of the 2018 Conference on nition. Empirical Methods in Natural Language Processing, pages 36873697, Brussels, Belgium. Association for Computational Linguistics.\n",
      "Teven Le Scao, Angela Fan, Christopher Akiki, El- lie Pavlick, Suzana Ilic, Daniel Hesslow, Ro- man Castagn, Alexandra Sasha Luccioni, Franois Yvon, Matthias Gall, et al. 2022. Bloom: A 176b- parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.\n",
      "Darsh Shah, Tao Lei, Alessandro Moschitti, Salva- tore Romeo, and Preslav Nakov. 2018. Adversar- ial domain adaptation for duplicate question detec- In Proceedings of the 2018 Conference on tion. Empirical Methods in Natural Language Processing, pages 10561063, Brussels, Belgium. Association for Computational Linguistics.\n",
      "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie- Yan Liu. 2020. Mpnet: Masked and permuted pre- training for language understanding. Advances in Neural Information Processing Systems, 33:16857 16867.\n",
      "Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta,\n",
      "Adri Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the arXiv preprint capabilities of language models. arXiv:2206.04615.\n",
      "Hao Tan and Mohit Bansal. 2019. Lxmert: Learning cross-modality encoder representations from trans- formers. arXiv preprint arXiv:1908.07490.\n",
      "Nandan Thakur, Nils Reimers, Andreas Rckl, Ab- hishek Srivastava, and Iryna Gurevych. 2021. Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information process- ing systems, 30.\n",
      "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language un- derstanding systems. Advances in neural informa- tion processing systems, 32.\n",
      "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461.\n",
      "Ben Wang and Aran Komatsuzaki. 2021. GPT-J- 6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflol z/mesh-transformer-jax.\n",
      "Kexin Wang, Nils Reimers, and Iryna Gurevych. 2021. Tsdae: Using transformer-based sequential denois- ing auto-encoder for unsupervised sentence embed- ding learning. arXiv preprint arXiv:2104.06979.\n",
      "Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. Minilm: Deep self- attention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural In- formation Processing Systems, 33:57765788.\n",
      "Samuel Weinbach, Marco Bellagente, Constantin Eichenberg, Andrew Dai, Robert Baldock, Souradeep Nanda, Bjrn Deiseroth, Koen Oost- ermeijer, Hannah Teufel, and Andres Felipe Cruz-Salinas. 2022. M-vader: A model for dif- arXiv preprint fusion with multimodal context. arXiv:2212.02936.\n",
      "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Rmi Louf, Morgan Funtow- icz, et al. 2020. Transformers: State-of-the-art nat- In Proceedings of the ural language processing. 2020 conference on empirical methods in natural language processing: system demonstrations, pages 3845.\n",
      "Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, et al. 2020. Mind: A large-scale dataset for news recommendation. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 3597 3606.\n",
      "Wei Xu, Chris Callison-Burch, and William B Dolan. 2015. Semeval-2015 task 1: Paraphrase and seman- tic similarity in twitter (pit). In Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015), pages 111.\n",
      "Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xi- aoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. 2022. Making a miracl: Multilingual in- formation retrieval across a continuum of languages. arXiv preprint arXiv:2210.09984.\n",
      "Jeffrey Zhu, Mingqin Li, Jason Li, and Cassandra Oduola. 2021. Bing delivers more contextualized search using quantized transformer inference on nvidia gpus in azure.\n",
      "Pierre Zweigenbaum, Serge Sharoff, and Reinhard Rapp. 2016. Towards preparation of the second bucc shared task: Detecting parallel sentences in compa- rable corpora. In Proceedings of the Ninth Workshop on Building and Using Comparable Corpora. Euro- pean Language Resources Association (ELRA), Por- toroz, Slovenia, pages 3843.\n",
      "Pierre Zweigenbaum, Serge Sharoff, and Reinhard Rapp. 2017. Overview of the second bucc shared task: Spotting parallel sentences in comparable cor- pora. In Proceedings of the 10th Workshop on Build- ing and Using Comparable Corpora, pages 6067.\n",
      "Pierre Zweigenbaum, Serge Sharoff, and Reinhard Rapp. 2018. Overview of the third bucc shared task: Spotting parallel sentences in comparable corpora. In Proceedings of 11th workshop on building and using comparable corpora, pages 3942.\n",
      "A Datasets\n",
      "Table 2 provides a summary along with statistics of all MTEB tasks. In the following, we give a brief description of each dataset included in MTEB.\n",
      "A.1 Clustering\n",
      "ArxivClusteringS2S, ArxivClusteringP2P, BiorxivClusteringS2S, BiorxivClusteringP2P, MedrxivClusteringP2P, MedrxivCluster- ingS2S These datasets are custom-made for MTEB using the public APIs from arXiv9 and bioRxiv/medRxiv10. For S2S datasets, the input text is simply the title of the paper, while for P2P the input text is the concatenation of the title and the abstract. The cluster labels are generated using categories given to the papers by humans. For bioRxiv and medRxiv this category is unique, but for arXiv multiple categories can be given to a single paper so we only use the rst one. For bioRxiv and medRxiv there is only one level of category (e.g. biochemistry, genetics, microbiology, etc.) hence we only perform clustering based on that label. For arXiv there is a main category and secondary category: for example \"cs.AI\" means the main category is Computer Science and the sub-category is AI, math.AG means the main category is Mathematics and the sub-category is Algrebraic Geometry etc. Hence, we create three types of splits:\n",
      "(a) Main category clustering Articles are only clustered based on the main category (Math, Physics, Computer Science etc.). This split evalu- ates coarse clustering capacity of a model.\n",
      "(b) Secondary category clustering within the same main category Articles are clustered based on their secondary category, but within a given main category, for example only Math papers that need to be clustered into Algebraic Geometry, Functional Analysis, Numerical Analysis etc. This split evaluates ne-grained clustering capacity of a model, as differentiating some sub-categories can be very difcult.\n",
      "(c) Secondary category clustering Articles are clustered based on their secondary category for all main categories, so the labels can be Number The- ory, Computational Complexity, Astrophysics of Galaxies etc. These splits evaluate ne-grained\n",
      "9https://arxiv.org/help/api/ 10https://api.biorxiv.org/\n",
      "clustering capacity, as well as multi-scale capac- ities i.e. is a model able to both separate Maths from Physics as well as Probability from Algebraic Topology at the same time.\n",
      "For every dataset, split and strategy, we select subsets of all labels and then sample articles from those labels. This yields splits with a varying amount and size of clusters.\n",
      "RedditClustering (Geigle et al., 2021): Cluster- ing of titles from 199 subreddits. Clustering of 25 splits, each with 10-50 classes, and each class with 100 - 1000 sentences\n",
      "RedditClusteringP2P Dataset for MTEB using available data from Reddit posts11. The task consists of clustering the concatenation of title+post according to their subreddit. It contains 10 splits, with 10 and 100 clusters per split and 1,000 to 100,000 posts.\n",
      "created\n",
      "StackExchangeClustering (Geigle et al., 2021) Clustering of titles from 121 stackexchanges. Clus- tering of 25 splits, each with 10-50 classes, and each class with 100-1000 sentences.\n",
      "StackExchangeClusteringP2P Dataset created for MTEB using available data from StackEx- change posts12. The task consists of clustering the concatenation of title and post according to their subreddit. It contains 10 splits, with 10 to 100 clusters and 5,000 to 10,000 posts per split.\n",
      "TwentyNewsgroupsClustering13 Clustering of the 20 Newsgroups dataset, given titles of article the goal is to nd the newsgroup (20 in total). Con- tains 10 splits, each with 20 classes, with each split containing between 1,000 and 10,000 titles.\n",
      "A.2 Classication\n",
      "AmazonCounterfactual (ONeill et al., 2021) A collection of Amazon customer reviews annotated for counterfactual detection pair classication. For each review the label is either \"counterfactual\" or \"not-counterfactual\". This is a multilingual dataset with 4 available languages.\n",
      "11https://huggingface.co/datasets/sent\n",
      "ence-transformers/reddit-title-body\n",
      "12https://huggingface.co/datasets/flax -sentence-embeddings/stackexchange_title _body_jsonl\n",
      "13https://scikit-learn.org/0.19/datase\n",
      "ts/twenty_newsgroups.html\n",
      "Name\n",
      "Type\n",
      "Categ.\n",
      "#Lang.\n",
      "Train Samples\n",
      "Dev Samples\n",
      "Test Samples\n",
      "Train avg. chars\n",
      "Dev avg. chars\n",
      "Test avg. chars\n",
      "BUCC Tatoeba\n",
      "BitextMining BitextMining\n",
      "s2s s2s\n",
      "4 112\n",
      "0 0\n",
      "0 0\n",
      "641684 2000\n",
      "0 0\n",
      "0 0\n",
      "101.3 39.4\n",
      "AmazonCounterfactualClassication AmazonPolarityClassication AmazonReviewsClassication Banking77Classication EmotionClassication ImdbClassication MassiveIntentClassication MassiveScenarioClassication MTOPDomainClassication MTOPIntentClassication ToxicConversationsClassication TweetSentimentExtractionClassication\n",
      "Classication Classication Classication Classication Classication Classication Classication Classication Classication Classication Classication Classication\n",
      "s2s p2p s2s s2s s2s p2p s2s s2s s2s s2s s2s s2s\n",
      "4 1 6 1 1 1 51 51 6 6 1 1\n",
      "4018 3600000 1200000 10003 16000 25000 11514 11514 15667 15667 50000 27481\n",
      "335 0 30000 0 2000 0 2033 2033 2235 2235 0 0\n",
      "670 400000 30000 3080 2000 25000 2974 2974 4386 4386 50000 3534\n",
      "107.3 431.6 160.5 59.5 96.8 1325.1 35.0 35.0 36.6 36.6 298.8 68.3\n",
      "109.2 0 159.2 0 95.3 0 34.8 34.8 36.5 36.5 0 0\n",
      "106.1 431.4 160.4 54.2 96.6 1293.8 34.6 34.6 36.8 36.8 296.6 67.8\n",
      "ArxivClusteringP2P ArxivClusteringS2S BiorxivClusteringP2P BiorxivClusteringS2S MedrxivClusteringP2P MedrxivClusteringS2S RedditClustering RedditClusteringP2P StackExchangeClustering StackExchangeClusteringP2P TwentyNewsgroupsClustering\n",
      "Clustering Clustering Clustering Clustering Clustering Clustering Clustering Clustering Clustering Clustering Clustering\n",
      "p2p s2s p2p s2s p2p s2s s2s p2p s2s p2p s2s\n",
      "1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 420464 0 417060 0 0\n",
      "732723 732723 75000 75000 37500 37500 420464 459399 373850 75000 59545\n",
      "0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 64.7 0 56.8 0 0\n",
      "1009.9 74.0 1666.2 101.6 1981.2 114.7 64.7 727.7 57.0 1090.7 32.0\n",
      "SprintDuplicateQuestions TwitterSemEval2015 TwitterURLCorpus\n",
      "PairClassication PairClassication PairClassication\n",
      "s2s s2s s2s\n",
      "1 1 1\n",
      "0 0 0\n",
      "101000 0 0\n",
      "101000 16777 51534\n",
      "0 0 0\n",
      "65.2 0 0\n",
      "67.9 38.3 79.5\n",
      "AskUbuntuDupQuestions MindSmallReranking SciDocsRR StackOverowDupQuestions\n",
      "Reranking Reranking Reranking Reranking\n",
      "s2s s2s s2s s2s\n",
      "1 1 1 1\n",
      "0 231530 0 23018\n",
      "0 0 19594 3467\n",
      "2255 107968 19599 3467\n",
      "0 69.0 0 49.6\n",
      "0 0 69.4 49.8\n",
      "52.5 70.9 69.0 49.8\n",
      "ArguAna ClimateFEVER CQADupstackAndroidRetrieval CQADupstackEnglishRetrieval CQADupstackGamingRetrieval CQADupstackGisRetrieval CQADupstackMathematicaRetrieval CQADupstackPhysicsRetrieval CQADupstackProgrammersRetrieval CQADupstackStatsRetrieval CQADupstackTexRetrieval CQADupstackUnixRetrieval CQADupstackWebmastersRetrieval CQADupstackWordpressRetrieval DBPedia FEVER FiQA2018 HotpotQA MSMARCO MSMARCOv2 NFCorpus NQ QuoraRetrieval SCIDOCS SciFact Touche2020 TRECCOVID\n",
      "Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval\n",
      "p2p s2p s2p s2p s2p s2p s2p s2p s2p s2p s2p s2p s2p s2p s2p s2p s2p s2p s2p s2p s2p s2p s2s s2p s2p s2p s2p\n",
      "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 138641342 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 4635989 0 0 0 8848803 138368101 0 0 0 0 0 0 0\n",
      "10080 5418128 23697 41791 46896 38522 17509 39355 33052 42921 71090 48454 17911 49146 4636322 5423234 58286 5240734 8841866 0 3956 2684920 532931 26657 5483 382594 171382\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 341.4 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 310.2 0 0 0 336.6 342.0 0 0 0 0 0 0 0\n",
      "1052.9 539.1 578.7 467.1 474.7 991.1 1103.7 799.4 1030.2 1041.0 1246.9 984.7 689.8 1111.9 310.1 538.6 760.4 288.6 336.8 0 1462.7 492.7 62.9 1161.9 1422.3 1720.1 1117.4\n",
      "BIOSSES SICK-R STS12 STS13 STS14 STS15 STS16 STS17 STS22 STSBenchmark\n",
      "STS STS STS STS STS STS STS STS STS STS\n",
      "s2s s2s s2s s2s s2s s2s s2s s2s p2p s2s\n",
      "1 1 1 1 1 1 1 11 18 1\n",
      "200 19854 4468 0 0 0 0 0 0 11498\n",
      "200 19854 0 0 0 0 0 0 0 3000\n",
      "200 19854 6216 3000 7500 6000 2372 500 8060 2758\n",
      "156.6 46.1 100.7 0 0 0 0 0 0 57.6\n",
      "156.6 46.1 0 0 0 0 0 0 0 64.0\n",
      "156.6 46.1 64.7 54.0 54.3 57.7 65.3 43.3 1992.8 53.6\n",
      "SummEval\n",
      "Summarization\n",
      "p2p\n",
      "1\n",
      "0\n",
      "0\n",
      "2800\n",
      "0\n",
      "0\n",
      "359.8\n",
      "Table 2: Tasks in MTEB\n",
      "AmazonPolarity (McAuley and Leskovec, 2013) A collection of Amazon customer reviews annotated for polarity classication. For each review the label is either \"positive\" or \"negative\".\n",
      "AmazonReviews and Leskovec, (McAuley 2013) A collection of Amazon reviews designed to aid research in multilingual text classication. For each review the label is the score given by the review between 0 and 4 (1-5 stars). This is a\n",
      "multilingual dataset with 6 available languages.\n",
      "Banking77 (Casanueva et al., 2020) Dataset composed of online banking queries annotated with their corresponding intents. For each user query the label is an intent among 77 intents like acti- vate_my_card, apple_pay, bank_transfer, etc.\n",
      "Emotion (Saravia et al., 2018) Dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise.\n",
      "Imdb (Maas et al., 2011) Large movie review dataset with labels being positive or negative.\n",
      "MassiveIntent (FitzGerald et al., 2022) A col- lection of Amazon Alexa virtual assistant utter- ances annotated with the associated intent. For each user utterance the label is one of 60 intents like play_music, alarm_set, etc. This is a multi- lingual dataset with 51 available languages.\n",
      "MassiveScenario (FitzGerald et al., 2022) A col- lection of Amazon Alexa virtual assistant utter- ances annotated with the associated intent. For each user utterance the label is a theme among 60 scenarios like music, weather, etc. This is a multilingual dataset with 51 available languages.\n",
      "MTOPDomain / MTOPIntent Multilingual sentence datasets from the MTOP (Li et al., 2020) benchmark. We refer to their paper for details.\n",
      "ToxicConversations Dataset from Kaggle com- petition14. Collection of comments from the Civil Comments platform together with annotations if the comment is toxic or not.\n",
      "TweetSentimentExtraction Dataset from Kag- gle competition15. Sentiment classication of tweets as neutral, positive or negative.\n",
      "A.3 Pair Classication\n",
      "SprintDuplicateQuestions (Shah et al., 2018): Collection of questions from the Sprint commu- nity. The goal is to classify a pair of sentences as duplicates or not.\n",
      "TwitterSemEval2015 (Xu 2015) Paraphrase-Pairs of Tweets from the SemEval 2015 workshop. The goal is to classify a pair of tweets as paraphrases or not.\n",
      "et\n",
      "al.,\n",
      "14https://www.kaggle.com/competitions/ jigsaw-unintended-bias-in-toxicity-class ification\n",
      "15https://www.kaggle.com/competitions/\n",
      "tweet-sentiment-extraction\n",
      "TwitterURLCorpus Paraphrase-Pairs of Tweets. classify a pair of tweets as paraphrases or not.\n",
      "(Lan\n",
      "et\n",
      "al., The goal\n",
      "2017) is to\n",
      "A.4 Bitext Mining\n",
      "BUCC (Zweigenbaum et al., 2016, 2017, 2018) BUCC provides big set of sentences ( 10-70k each) for English, French, Russian, German and Chinese, along with associated pairs annotation. The annotated pairs here corresponds to a pairs of translated sentences, i.e. a sentence and its transla- tion in the other language.\n",
      "Tatoeba (Research) Tatoeba provides sets of sen- tences (1000 sentences each) for 112 languages with annoated associated pairs. Each pair is one sentence and its translation in another language.\n",
      "A.5 Reranking AskUbuntuDupQuestions16 Questions from AskUbuntu with manual annotations marking pairs of questions as similar or dissimilar.\n",
      "MindSmall (Wu et al., 2020) Large-scale En- glish Dataset for News Recommendation Research. Ranking news article titles given the title of a news article. The idea is to recommend other news from the one you are reading.\n",
      "SciDocsRR (Cohan et al., 2020b) Ranking of re- lated scientic papers based on their title.\n",
      "StackOverowDupQuestions (Liu et al., 2018) Stack Overow Duplicate Questions Task for ques- tions with the tags Java, JavaScript and Python, ranking questions as duplicates or not.\n",
      "A.6 Semantic Textual Similarity (STS)\n",
      "STS12, STS13, STS14, STS15, STS16, STS17, STS22, STSBenchmark (Agirre et al., 2012, 2013)17181920 Original STS benchmark, with scores from 0 to 5. The selection of sentences includes text from image captions, news headlines and user forums. In total they contain between 1,000 and 20,000 sentences. STS12 - STS16 and\n",
      "16https://github.com/taolei87/askubuntu 17https://alt.qcri.org/semeval2014/tas\n",
      "k10/\n",
      "18https://alt.qcri.org/semeval2015/tas\n",
      "k2/\n",
      "19https://alt.qcri.org/semeval2016/tas\n",
      "k1/\n",
      "20https://competitions.codalab.org/com\n",
      "petitions/33835\n",
      "STSBenchmark are monolingual english bench- marks. STS17 and STS22 contain crosslingual pairs of sentences, where the goal is to assess the similarity of two sentences in different languages. STS17 has 11 language pairs (among Korean, Ara- bic, English, French, German, Turkish, Spanish, Italian and Dutch) and STS22 has 18 language pairs (among Arabic, English, French, German, Turkish, Spanish, Polish, Italian, Russian and Chinese).\n",
      "BIOSSES21 Contains 100 sentence pairs from the biomedical eld.\n",
      "SICK-R (Agirre et al., 2014) Sentences Involv- ing Compositional Knowledge (SICK) contains a large number of sentence pairs (10 0000) that are lexically, syntactically and semantically rich.\n",
      "A.7 Summarization\n",
      "SummEval (Fabbri et al., 2020) Summaries gen- erated by recent summarization models trained on CNN or DailyMail alongside human annotations.\n",
      "A.8 Retrieval\n",
      "We refer to the BEIR paper (Thakur et al., 2021), which contains description of each dataset. For MTEB, we include all publicly available datasets: ArguAna, ClimateFEVER, CQADupstack, DB- Pedia, FEVER, FiQA2018, HotpotQA, MS- MARCO, NFCorpus, NQ, Quora, SCIDOCS, SciFact, Touche2020, TRECCOVID.\n",
      "B Limitations of MTEB\n",
      "While MTEB aims to be a diverse benchmark to provide holistic performance reviews, the bench- mark has its limitations. We list them here:\n",
      "1. Long document datasets MTEB covers mul- tiple text lengths (S2S, P2P, S2P), but very long documents are still missing. The longest datasets in MTEB have a few hundred words, and longer text sizes could be relevant for use cases like retrieval.\n",
      "2. Task imbalance Tasks in MTEB have a differ- ent amount of datasets with summarization consist- ing of only a single dataset. This means MTEB av- erage scores, which are computed over all datasets, are biased towards tasks with many datasets, no- tably retrieval, classication and clustering. As MTEB grows, we hope to add more datasets to cur- rently underrepresented tasks like summarization or pair classication.\n",
      "21https://tabilab.cmpe.boun.edu.tr/BIO\n",
      "SSES/DataSet.html\n",
      "3. Multinguality MTEB contains multilingual classication, STS and bitext mining datasets. However, retrieval and clustering are English-only. SGPT-BLOOM-7B1-msmarco is geared towards multilingual retrieval datasets and due to the lack thereof cannot be comprehensively benchmarked in MTEB. Further, MTEB does not contain any code datasets that could be used to benchmark code models (Neelakantan et al., 2022; Allal et al., 2023). It should be easy to extend MTEB with datasets, such as CodeSearchNet (Husain et al., 2019), TyDI QA (Clark et al., 2020), XOR QA (Asai et al., 2020) or MIRACL (Zhang et al., 2022).\n",
      "4. Additional modalities Text embeddings are commonly used as input features for downstream models, such as in our classication task. This can involve other modalities, notably image con- tent (Carvalho et al., 2018; Tan and Bansal, 2019; Muennighoff, 2020; Nichol et al., 2021; Saharia et al., 2022; Weinbach et al., 2022). We have fo- cused solely on natural language applications and leave extensive benchmarking of text embeddings as inputs for other modalities to future work. C Examples\n",
      "Tables 3-9 provide examples for each dataset for each task. For retrieval datasets, we refer to the BEIR paper (Thakur et al., 2021).\n",
      "D Correlations\n",
      "Figure 6 provides correlation heatmaps for model performance and MTEB tasks.\n",
      "E Models\n",
      "Table 10 provides publicly available model check- points used for MTEB evaluation.\n",
      "F Additional results\n",
      "Tables 11 until the end provide results on individ- ual datasets of MTEB. The results are additionally available in json format on the Hugging Face Hub22 and can be inspected on the leaderboard23.\n",
      "22https://huggingface.co/datasets/mteb\n",
      "/results\n",
      "23https://huggingface.co/spaces/mteb/l\n",
      "eaderboard\n",
      "Dataset\n",
      "Text\n",
      "Label\n",
      "AmazonCounterfactualClassication\n",
      "In person it looks as though it would have cost a lot more.\n",
      "counterfactual\n",
      "AmazonPolarityClassication\n",
      "an absolute masterpiece I am quite sure any of you actually taking the time to read this have played the game at least once, and heard at least a few of the tracks here. And whether you were aware of it or not, Mitsudas music contributed greatly to the...\n",
      "positive\n",
      "AmazonReviewsClassication\n",
      "solo llega una unidad cuando te obligan a comprar dos Te obligan a comprar dos unidades y te llega solo una y no hay forma de reclamar, una autentica estafa, no compreis!!\n",
      "0\n",
      "Banking77Classication\n",
      "What currencies is an exchange rate calculated in?\n",
      "exchange_rate\n",
      "EmotionClassication\n",
      "i feel so inhibited in someone elses kitchen like im painting on someone elses picture\n",
      "sadness\n",
      "ImdbClassication\n",
      "When I rst saw a glimpse of this movie, I quickly noticed the actress who was playing the role of Lucille Ball. Rachel Yorks portrayal of Lucy is absolutely awful. Lucille Ball was an astounding comedian with incredible talent. To think about a legend like Lucille Ball being portrayed the way she was in the movie is horrendous. I cannot believe...\n",
      "negative\n",
      "MassiveIntentClassication\n",
      "rveille-moi  neuf heures du matin le vendredi\n",
      "alarm_set\n",
      "MassiveScenarioClassication\n",
      "tell me the artist of this song\n",
      "music\n",
      "MTOPDomainClassication\n",
      "Maricopa County weather forecast for this week\n",
      "weather\n",
      "MTOPIntentClassication\n",
      "what ingredients do is have left\n",
      "GET_INFO_RECIPES\n",
      "ToxicConversationsClassication\n",
      "The guys a damn cop, so what do you expect?\n",
      "toxic\n",
      "TweetSentimentExtractionClassication\n",
      "I really really like the song Love Story by Taylor Swift\n",
      "positive\n",
      "Table 3: Classication examples\n",
      "Dataset\n",
      "Text\n",
      "Cluster\n",
      "ArxivClusteringP2P\n",
      "Finite groups of rank two which do not involve Qd(p). Let p > 3 be a prime. We show that if G is a nite group with p-rank equal to 2, then G involves Qd(p) if and only if G p(cid:48)-involves Qd(p). This allows us to use a version of Glaubermans ZJ-theorem to give a more direct construction of nite group actions on mod-p homotopy spheres. We give an example to illustrate that the above conclusion does not hold for p  3.\n",
      "math\n",
      "ArxivClusteringS2S\n",
      "Vertical shift and simultaneous Diophantine approximation on polynomial curves\n",
      "math\n",
      "BiorxivClusteringP2P\n",
      "Innate Immune sensing of Inuenza A viral RNA through IFI16 promotes pyroptotic cell death Programmed cell death pathways are triggered by various stresses or stimuli, including viral infections. The mechanism underlying the regula- tion of these pathways upon Inuenza A virus IAV infection is not well characterized. We report that a cytosolic DNA sensor IFI16 is...\n",
      "immunology\n",
      "BiorxivClusteringS2S\n",
      "Association of CDH11 with ASD revealed by matched-gene co-expression analysis and mouse behavioral\n",
      "neuroscience\n",
      "MedrxivClusteringP2P\n",
      "Temporal trends in the incidence of haemophagocytic lymphohistiocytosis: a nationwide cohort study from England 2003-2018. Haemophagocytic lymphohistiocytosis (HLH) is rare, results in high mortality and is increasingly being diagnosed. Little is known about what is driving the apparent rise in the incidence of this disease. Using national linked electronic health data from hospital admissions and death certication cases of HLH that were diagnosed in England between 1/1/2003 and 31/12/2018 were identied using a previously validated approach. We calculated incidence...\n",
      "infectious diseases\n",
      "MedrxivClusteringS2S\n",
      "Current and Lifetime Somatic Symptom Burden Among Transition-aged Young Adults on the Autism Spectrum\n",
      "psychiatry and clinical psychology\n",
      "RedditClustering\n",
      "Could anyone tell me what breed my bicolor kitten is?\n",
      "r/cats\n",
      "RedditClusteringP2P\n",
      "Headaches after working out? Hey guys! Ive been diagnosed with adhd since I was seven. I just recently got rediag- nosed (22f) and Ive been out on a different medication, adderall I was normally taking vyvanse but because of cost and no insurance adderall was more affordable. Ive noticed that if I take adderall and workout...\n",
      "r/ADHD\n",
      "StackExchangeClustering\n",
      "Does this property characterize a space as Hausdorff?\n",
      "math.stackexchange.com\n",
      "StackExchangeClusteringP2P\n",
      "Google play services error DEBUG: Application is pausing, which disconnects the RTMP client. I am having this issue from past day with Google Play Services Unity. What happens is, when I install app directly ot device via Unity, the Google Play Services work ne but when I upload it as beta to play store console and install it via that then it starts to give \" DEBUG: Application is pausing, which disconnects the RTMP client\" error. I have a proper SHA1 key.\n",
      "unity\n",
      "TwentyNewsgroupsClustering\n",
      "Commercial mining activities on the moon\n",
      "14\n",
      "Table 4: Clustering examples\n",
      "Dataset\n",
      "Sentence 1\n",
      "Sentence 2\n",
      "Label\n",
      "SprintDuplicateQuestions\n",
      "Franklin U722 USB modem signal strength\n",
      "How do I know if my Franklin U772 USB Modem has a weak signal ?\n",
      "1\n",
      "TwitterSemEval2015\n",
      "All the home alones watching 8 mile\",\"All the home alones watching 8 mile\n",
      "The last rap battle in 8 Mile nevr gets old ahah\n",
      "0\n",
      "TwitterURLCorpus\n",
      "How the metaphors we use to describe discovery affect men and women in the sciences\n",
      "Light Bulbs or Seeds ? How Metaphors for Ideas Inuence Judgments About Genius\n",
      "0\n",
      "Table 5: Pair classication examples. Labels are binary.\n",
      "Dataset\n",
      "Query\n",
      "Positive\n",
      "Negative\n",
      "AskUbuntuDupQuestions\n",
      "change the application icon theme but not changing the panel icons\n",
      "change folder icons in ubuntu-mono-dark theme\n",
      "change steam tray icon back to default\n",
      "MindSmallReranking\n",
      "Man accused in probe of Giuliani associates is freed on bail\n",
      "Studies show these are the best and worst states for your retirement\n",
      "There are 14 cheap days to y left in 2019: When are they and what deals can you score?\n",
      "SciDocsRR\n",
      "Discovering social circles in ego networks\n",
      "Benchmarks for testing community detection algorithms on directed and weighted graphs with overlapping communi- ties.\n",
      "Improving www proxies performance with greedy-dual- size-frequency caching policy\n",
      "StackOverowDupQuestions\n",
      "Java launch error selection does not contain a main type\n",
      "Error: Selection does not contain a main type\n",
      "Selection Sort in Java\n",
      "Table 6: Reranking examples\n",
      "Dataset\n",
      "Sentence 1\n",
      "Sentence 2\n",
      "Score\n",
      "BIOSSES\n",
      "It has recently been shown that Craf is essential for Kras G12D-induced NSCLC.\n",
      "It has recently become evident that Craf is essential for the onset of Kras-driven non-small cell lung cancer.\n",
      "4.0\n",
      "SICK-R\n",
      "A group of children is playing in the house and there is no man standing in the background\n",
      "A group of kids is playing in a yard and an old man is stand- ing in the background\n",
      "3.2\n",
      "STS12\n",
      "Nationally, the federal Centers for Disease Control and Pre- vention recorded 4,156 cases of West Nile, including 284 deaths.\n",
      "There were 293 human cases of West Nile in Indiana in 2002, including 11 deaths statewide.\n",
      "1.7\n",
      "STS13\n",
      "this frame has to do with people ( the residents ) residing in locations , sometimes with a co-resident .\n",
      "inhabit or live in ; be an inhabitant of ;\n",
      "2.8\n",
      "STS14\n",
      "then the captain was gone.\n",
      "then the captain came back.\n",
      "0.8\n",
      "STS15\n",
      "you ll need to check the particular policies of each pub- lisher to see what is allowed and what is not allowed.\n",
      "if you need to publish the book and you have found one publisher that allows it.\n",
      "3.0\n",
      "STS16\n",
      "you do not need to worry.\n",
      "you don t have to worry.\n",
      "5.0\n",
      "STS17\n",
      "La gente muestra su afecto el uno por el otro.\n",
      "A women giving something to other lady.\n",
      "1.4\n",
      "STS22\n",
      "El secretario general de la Asociacin Gremial de los Tra- bajadores del Subte y Premetro de Metrodelegados, Beto Pianelli, dijo que el Gobierno porteo debe convocar in- mediatamente a licitacin para la compra de nuevos trenes y retirar los que quedan en circulacin...\n",
      "En dilogo con el servicio informativo de la Radio Pblica, el ministro de Salud de la Nacin, Gins Gonzlez Garca, habl sobre el avance del coronavirus en la Argentina y se manifest a favor de prorrogar la cuarentena obligatoria dis- puesta por...\n",
      "1\n",
      "STSBenchmark\n",
      "A man is playing the cello.\n",
      "A man seated is playing the cello.\n",
      "4.25\n",
      "Table 7: STS examples. Scores are continuous between 0 and 5 (included).\n",
      "Dataset\n",
      "First set sentence\n",
      "Second set sentence\n",
      "BUCC\n",
      "Morales remporte llection prsidentielle de 2005  la ma- jorit absolue.\n",
      "Morales went on to win the 2005 presidential election with an absolute majority.\n",
      "Tatoeba\n",
      "Chi le ha detto che Tom lha fatto?\n",
      "Who told you that Tom did that?\n",
      "Table 8: Bitext mining examples\n",
      "Dataset\n",
      "Human Summary\n",
      "Machine Summary\n",
      "Relevance\n",
      "SummEval\n",
      "V. Stiviano must pay back $2.6 million in gifts from Donald Sterling. Sterlings wife claimed the ex-Clippers used the couples money for the gifts. The items included a Ferrari, two Bentleys and a Range Rover.\n",
      "donald sterling , nba team last year . sterling s wife sued for $ 2.6 million in gifts . sterling says he is the former female companion who has lost the . sterling has ordered v. stiviano to pay back $ 2.6 m in gifts after his wife sued . sterling also includes a $ 391 easter bunny costume , $ 299 and a $ 299 .\n",
      "1.7\n",
      "Table 9: Summarization example\n",
      "LASER2\n",
      "80\n",
      "SimCSE-BERT-sup\n",
      "SPECTER\n",
      "SGPT-BLOOM-7.1B-msmarco\n",
      "SGPT-BLOOM-7.1B-msmarco\n",
      "GTR-XL\n",
      "GTR-XL\n",
      "SimCSE-BERT-sup\n",
      "MPNet\n",
      "Glove\n",
      "Glove\n",
      "MPNet-multilingual\n",
      "MPNet-multilingual\n",
      "GTR-Base\n",
      "MiniLM-L6\n",
      "MiniLM-L12-multilingual\n",
      "MPNet\n",
      "ST5-Base\n",
      "SGPT-125M-nli\n",
      "ST5-XXL\n",
      "SGPT-125M-nli\n",
      "LaBSE\n",
      "SGPT-125M-msmarco\n",
      "SGPT-125M-msmarco\n",
      "MiniLM-L6\n",
      "LaBSE\n",
      "GTR-XXL\n",
      "GTR-XXL\n",
      "SimCSE-BERT-unsup\n",
      "SimCSE-BERT-unsup\n",
      "ST5-Base\n",
      "90\n",
      "MiniLM-L12\n",
      "SGPT-5.8B-msmarco\n",
      "SGPT-2.7B-msmarco\n",
      "Contriever\n",
      "Contriever\n",
      "95\n",
      "100\n",
      "Komninos\n",
      "ST5-Large\n",
      "SGPT-2.7B-msmarco\n",
      "85\n",
      "MiniLM-L12-multilingual\n",
      "ST5-XXL989088979593959492999593859594908979919098939289929190849694929897959194949491979690859097919078919097978892879088779190979787918610095938596969896919592979789887890909696879086999996949385979698978995939797999597969199989591949796929197919696958998989794919694949497939899929078918996968789859596969495939589877588879697838782969694949589949987867387859597818580959694949488939810084826983819295788177939491929184909799998483698381929478827894959193918490979899100878574888796987987839697959595889296979796958583728786959876848095969395948690949797959410085837286859597768479959693959485909497979595100100848271858594977584799596929593848993969694949910010094928797979693889593949396949797979190898585929191919290859596949386939193939594969597909089868692929292999290849495949385929093939494959496919190878793939392991009088819293949483908793949495959296929392909095959594979999\n",
      "SGPT-5.8B-msmarco\n",
      "SPECTER\n",
      "Komninos\n",
      "GTR-Base\n",
      "MiniLM-L12\n",
      "GTR-Large\n",
      "GTR-Large\n",
      "70\n",
      "coCondenser-msmarco\n",
      "coCondenser-msmarco\n",
      "ST5-XL\n",
      "75\n",
      "BERT\n",
      "ST5-XL\n",
      "BERT\n",
      "SGPT-5.8B-nli\n",
      "SGPT-5.8B-nli\n",
      "SGPT-1.3B-msmarco\n",
      "SGPT-1.3B-msmarco\n",
      "ST5-Large\n",
      "LASER2\n",
      "Class.\n",
      "PairClass.\n",
      "80\n",
      "Summ.6872815895835785879079758578693-48-8-160\n",
      "Class.\n",
      "40\n",
      "Summ.\n",
      "20\n",
      "100\n",
      "STS\n",
      "Retr.\n",
      "Retr.\n",
      "Clust.\n",
      "Clust.\n",
      "0\n",
      "STS\n",
      "Rerank.\n",
      "PairClass.\n",
      "60\n",
      "Rerank.\n",
      "(a) Model correlation based on all results\n",
      "(b) Task correlation based on average task results\n",
      "Figure 6: Pearson correlations across model and task results. Left: Size variants of the same architecture show high correlations. Right: Performance on clustering and reranking correlates strongest, while summarization and classication show weaker correlation with other tasks.\n",
      "Model\n",
      "Public Checkpoint\n",
      "Glove Komninos BERT SimCSE-BERT-unsup SimCSE-BERT-sup coCondenser-msmarco Contriever SPECTER LaBSE LASER2 MiniLM-L6 MiniLM-L12 MiniLM-L12-multilingual MPNet MPNet-multilingual MiniLM-L12-multilingual SGPT-125M-nli SGPT-5.8B-nli SGPT-125M-msmarco SGPT-1.3B-msmarco SGPT-2.7B-msmarco SGPT-5.8B-msmarco SGPT-BLOOM-7.1B-msmarco SGPT-BLOOM-1.7B-nli GTR-Base GTR-Large GTR-XL GTR-XXL ST5-Base ST5-Large ST5-XL ST5-XXL\n",
      "https://huggingface.co/sentence-transformers/average_word_embeddings_glove.6B.300d https://huggingface.co/sentence-transformers/average_word_embeddings_komninos https://huggingface.co/bert-base-uncased https://huggingface.co/princeton-nlp/unsup-simcse-bert-base-uncased https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased https://huggingface.co/sentence-transformers/msmarco-bert-co-condensor https://huggingface.co/nthakur/contriever-base-msmarco https://huggingface.co/sentence-transformers/allenai-specter https://huggingface.co/sentence-transformers/LaBSE https://github.com/facebookresearch/LASER https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2 https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 https://huggingface.co/sentence-transformers/all-mpnet-base-v2 https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2 https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 https://huggingface.co/Muennighoff/SGPT-125M-weightedmean-nli-bitfit https://huggingface.co/Muennighoff/SGPT-5.8B-weightedmean-nli-bitfit https://huggingface.co/Muennighoff/SGPT-125M-weightedmean-msmarco-specb-bitfit https://huggingface.co/Muennighoff/SGPT-1.3B-weightedmean-msmarco-specb-bitfit https://huggingface.co/Muennighoff/SGPT-2.7B-weightedmean-msmarco-specb-bitfit https://huggingface.co/Muennighoff/SGPT-5.8B-weightedmean-msmarco-specb-bitfit https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco https://huggingface.co/bigscience-data/sgpt-bloom-1b7-nli https://huggingface.co/sentence-transformers/gtr-t5-base https://huggingface.co/sentence-transformers/gtr-t5-large https://huggingface.co/sentence-transformers/gtr-t5-xl https://huggingface.co/sentence-transformers/gtr-t5-xxl https://huggingface.co/sentence-transformers/sentence-t5-base https://huggingface.co/sentence-transformers/sentence-t5-large https://huggingface.co/sentence-transformers/sentence-t5-xl https://huggingface.co/sentence-transformers/sentence-t5-xxl\n",
      "Table 10: Publicly available model links used for evaluation\n",
      "Dataset\n",
      "AmazonCounterfactualClassication AmazonPolarityClassication AmazonReviewsClassication Banking77Classication EmotionClassication ImdbClassication MassiveIntentClassication MassiveScenarioClassication MTOPDomainClassication MTOPIntentClassication ToxicConversationsClassication TweetSentimentExtractionClassication\n",
      "ArxivClusteringP2P ArxivClusteringS2S BiorxivClusteringP2P BiorxivClusteringS2S MedrxivClusteringP2P MedrxivClusteringS2S RedditClustering RedditClusteringP2P StackExchangeClustering StackExchangeClusteringP2P TwentyNewsgroupsClustering\n",
      "SprintDuplicateQuestions TwitterSemEval2015 TwitterURLCorpus\n",
      "AskUbuntuDupQuestions MindSmallReranking SciDocsRR StackOverowDupQuestions\n",
      "ArguAna ClimateFEVER CQADupstackRetrieval DBPedia FEVER FiQA2018 HotpotQA MSMARCO NFCorpus NQ QuoraRetrieval SCIDOCS SciFact Touche2020 TRECCOVID\n",
      "BIOSSES SICK-R STS12 STS13 STS14 STS15 STS16 STS17 STS22 STSBenchmark\n",
      "SummEval\n",
      "Average\n",
      "Glove\n",
      "56.91 60.32 29.67 67.69 36.93 62.57 56.19 66.03 79.11 55.85 65.40 50.80\n",
      "32.56 23.14 29.27 19.18 26.12 20.38 28.46 35.82 35.80 28.51 25.83\n",
      "86.96 48.45 77.35\n",
      "49.57 27.01 62.56 34.03\n",
      "36.30 14.44 15.47 18.29 14.99 10.09 19.18 9.60 13.87 12.87 71.32 8.04 29.58 13.99 36.22\n",
      "44.93 55.43 54.64 69.16 60.81 72.31 65.34 77.95 56.35 61.54\n",
      "28.87\n",
      "41.97\n",
      "Komninos\n",
      "60.54 59.59 31.01 67.05 33.18 63.98 57.21 66.11 78.57 57.07 67.76 49.68\n",
      "34.73 26.01 29.76 20.71 26.65 21.50 28.84 7.37 39.04 30.23 27.42\n",
      "85.55 53.85 79.41\n",
      "50.88 28.92 63.55 35.65\n",
      "30.96 14.87 16.79 15.88 15.56 10.49 20.77 9.75 11.79 12.75 71.58 8.47 29.53 13.17 35.92\n",
      "50.25 55.49 53.51 70.80 63.56 74.08 64.60 76.91 53.89 61.55\n",
      "30.49\n",
      "42.06\n",
      "BERT\n",
      "74.25 71.33 33.56 63.41 35.28 65.35 59.88 64.28 82.63 68.14 70.0 51.81\n",
      "35.19 27.51 30.12 24.77 26.09 23.60 27.24 43.32 43.58 26.55 23.35\n",
      "36.81 55.90 76.29\n",
      "45.84 28.37 64.94 34.62\n",
      "28.29 5.41 5.51 4.13 3.30 2.19 8.26 1.91 4.30 2.61 61.03 2.81 13.34 0.97 14.74\n",
      "54.70 58.65 30.87 59.89 47.73 60.29 63.73 64.10 56.37 47.29\n",
      "29.82\n",
      "38.33\n",
      "SimCSE- BERT- unsup\n",
      "67.09 74.48 33.85 73.55 42.22 69.63 59.84 66.25 81.71 59.23 68.82 53.36\n",
      "32.61 24.68 24.90 19.55 23.60 21.97 32.18 45.14 43.07 28.50 23.21\n",
      "69.41 60.21 81.37\n",
      "51.57 28.62 66.33 39.35\n",
      "38.34 11.80 13.22 15.04 21.05 9.84 19.75 9.35 9.88 11.69 78.03 5.50 25.72 8.90 26.2\n",
      "72.31 72.24 66.05 81.49 73.61 79.72 78.12 83.58 59.65 76.52\n",
      "31.15\n",
      "45.45\n",
      "SimCSE- BERT- sup\n",
      "75.75 82.47 39.60 75.76 44.81 73.53 65.95 70.78 84.29 63.14 72.04 59.73\n",
      "35.18 27.54 30.15 24.67 26.25 24.12 40.23 47.74 47.55 29.45 34.86\n",
      "69.39 67.75 83.89\n",
      "51.80 29.30 70.14 38.90\n",
      "38.33 11.98 14.50 19.73 20.41 10.41 22.89 11.00 12.42 16.08 79.62 7.53 29.59 9.89 22.93\n",
      "68.38 80.77 75.30 84.67 80.19 85.40 80.82 89.44 61.96 84.25\n",
      "23.31\n",
      "48.72\n",
      "coCondenser- msmarco\n",
      "64.06 66.88 34.85 82.35 41.91 60.17 70.40 73.73 91.34 71.07 64.01 55.74\n",
      "36.94 29.03 32.35 28.16 30.23 27.01 48.04 53.53 59.54 30.48 38.68\n",
      "96.09 65.95 83.17\n",
      "58.99 27.13 72.78 48.48\n",
      "45.15 16.96 27.72 27.86 45.68 15.62 35.61 29.57 22.29 29.85 86.51 10.13 52.31 8.57 40.54\n",
      "77.32 72.00 68.19 80.40 74.02 82.57 79.78 85.94 67.54 76.97\n",
      "29.50\n",
      "52.35\n",
      "Contr- iever\n",
      "72.19 68.63 37.42 80.02 44.77 67.04 67.78 76.00 93.18 69.31 67.77 56.10\n",
      "42.61 32.32 34.97 29.08 31.19 27.27 54.89 57.58 63.15 32.25 46.82\n",
      "95.55 66.85 85.21\n",
      "56.69 31.58 76.51 47.78\n",
      "48.32 24.79 33.67 38.10 59.29 27.42 56.81 36.77 31.31 41.83 86.72 17.12 65.51 15.79 44.77\n",
      "83.32 70.20 64.34 80.03 74.51 83.30 79.67 86.32 64.64 78.81\n",
      "30.36\n",
      "56.00\n",
      "SPECTER\n",
      "58.70 57.77 26.26 66.66 24.82 56.35 51.73 58.58 74.53 50.05 57.44 45.52\n",
      "44.75 35.27 39.52 34.53 35.04 31.66 24.13 35.06 39.01 31.46 24.22\n",
      "71.63 43.25 69.22\n",
      "50.07 24.80 81.31 36.22\n",
      "32.67 6.86 14.60 4.14 5.45 5.64 5.46 5.58 0.84 5.99 64.65 0.00 47.88 8.46 29.91\n",
      "64.95 56.39 62.49 58.70 54.87 62.54 64.27 69.63 55.06 61.26\n",
      "27.66\n",
      "40.28\n",
      "LaBSE\n",
      "LASER2\n",
      "MiniLM- L6\n",
      "MiniLM- L12-\n",
      "MiniLM- L12- multilingual\n",
      "MPNet\n",
      "MPNet- multilingual\n",
      "OpenAI Ada Similarity\n",
      "SGPT-125M- nli\n",
      "SGPT-5.8B- nli\n",
      "SGPT-125M- msmarco\n",
      "SGPT-1.3B- msmarco\n",
      "SGPT-2.7B- msmarco\n",
      "75.93 68.95 35.80 69.85 37.22 62.04 61.46 66.41 86.06 63.03 66.90 58.82\n",
      "76.84 61.01 28.71 57.76 24.83 57.58 47.91 55.92 75.36 49.47 54.05 48.73\n",
      "64.15 62.58 31.79 79.75 38.43 60.66 67.40 75.76 91.56 62.18 66.99 55.41\n",
      "65.28 62.98 30.79 80.40 41.17 59.76 67.15 74.58 91.90 62.84 67.47 54.25\n",
      "71.57 69.21 35.11 79.77 42.37 60.46 66.84 71.51 87.06 65.52 66.07 56.12\n",
      "65.27 67.13 31.92 81.86 39.73 70.72 69.57 76.01 92.08 70.21 60.86 55.46\n",
      "75.81 76.41 38.51 81.07 45.84 64.57 69.32 75.35 89.24 68.69 71.02 59.03\n",
      "76.40 92.83 47.45 68.04 50.32 89.38 65.17 67.67 89.89 64.80 70.00 63.35\n",
      "65.88 74.94 35.10 74.68 42.23 62.90 58.08 66.34 81.52 58.24 62.79 54.82\n",
      "74.07 82.31 41.58 81.74 49.92 74.33 70.0 75.03 89.64 70.68 69.93 62.44\n",
      "61.24 65.40 31.17 77.70 39.08 58.67 61.41 69.74 86.96 62.25 62.66 52.41\n",
      "65.21 73.21 34.96 82.06 46.39 64.05 68.65 76.04 92.08 71.19 68.73 55.67\n",
      "67.57 71.44 35.75 83.22 49.21 63.53 69.01 75.90 92.56 71.85 68.84 56.69\n",
      "32.13 22.05 29.84 20.57 30.13 24.82 28.79 49.14 35.43 28.83 23.28\n",
      "17.77 12.39 12.40 8.83 17.91 16.63 9.96 26.42 15.79 18.63 11.38\n",
      "46.55 37.86 38.48 33.17 34.41 32.29 50.67 54.15 53.36 38.00 46.86\n",
      "46.07 37.50 36.99 33.21 34.25 32.24 51.18 54.80 53.05 33.13 47.47\n",
      "38.33 31.55 33.49 29.44 31.52 30.87 42.02 50.73 49.60 31.69 39.28\n",
      "48.38 39.72 39.62 35.02 35.58 32.87 54.82 56.77 53.80 34.28 49.74\n",
      "37.78 31.68 33.09 29.60 31.96 31.70 45.24 51.31 52.98 32.94 44.10\n",
      "41.49 28.47 36.86 27.55 31.09 26.50 42.47 58.10 53.52 30.43 36.26\n",
      "34.74 24.68 28.93 23.08 28.30 24.93 33.76 41.01 44.59 28.23 28.24\n",
      "40.55 32.49 33.59 29.13 30.33 28.02 42.17 48.02 54.13 31.12 37.20\n",
      "39.71 28.24 33.63 27.04 31.37 26.87 40.23 49.09 52.74 32.66 32.13\n",
      "43.38 33.71 35.06 30.71 32.08 29.45 48.23 53.18 60.86 32.36 40.06\n",
      "44.72 35.08 34.41 30.53 31.35 28.77 46.47 54.17 59.19 32.57 40.89\n",
      "89.26 62.78 84.58\n",
      "65.54 59.57 81.47\n",
      "94.55 67.86 84.70\n",
      "92.45 70.02 84.77\n",
      "89.46 62.06 83.83\n",
      "90.15 73.85 85.11\n",
      "90.55 66.75 85.14\n",
      "77.85 69.04 83.69\n",
      "77.73 57.09 80.51\n",
      "80.54 66.00 84.54\n",
      "89.89 54.75 81.06\n",
      "92.58 62.37 83.79\n",
      "93.47 63.68 84.80\n",
      "52.75 29.81 68.72 42.42\n",
      "48.99 24.79 54.99 36.98\n",
      "63.48 30.80 87.12 50.76\n",
      "64.06 31.02 87.20 51.47\n",
      "60.49 30.37 77.78 45.85\n",
      "65.85 30.97 88.65 51.98\n",
      "60.16 30.15 78.09 46.79\n",
      "53.49 30.71 71.04 40.85\n",
      "52.63 29.27 68.36 39.97\n",
      "55.90 31.11 77.54 44.77\n",
      "55.84 30.40 71.34 44.74\n",
      "58.13 31.34 77.21 49.32\n",
      "59.63 31.72 77.72 49.61\n",
      "34.18 3.83 18.75 15.57 12.17 7.00 18.75 7.60 16.54 8.42 77.03 5.63 38.20 4.88 16.34\n",
      "12.86 0.36 4.12 1.53 0.77 1.73 5.50 1.09 2.44 0.64 71.14 0.78 4.04 1.06 10.97\n",
      "50.17 20.27 41.32 32.33 51.93 36.87 46.51 36.54 31.59 43.87 87.56 21.64 64.51 16.90 47.25\n",
      "47.13 21.57 42.53 33.36 55.91 37.27 44.59 39.03 32.25 46.47 87.75 21.82 62.64 17.22 50.82\n",
      "44.88 18.49 30.71 22.63 52.66 20.33 30.01 23.72 23.45 29.80 86.55 0.03 48.37 16.06 39.12\n",
      "46.52 21.97 44.96 32.09 50.86 49.96 39.29 39.75 33.29 50.45 87.46 23.77 65.57 19.93 51.33\n",
      "48.91 15.27 31.32 26.22 56.76 22.96 37.03 26.60 25.49 33.60 86.41 13.96 50.30 17.40 37.87\n",
      "39.65 2.83 10.17 3.48 4.45 7.54 12.6 10.53 20.59 2.02 82.18 6.28 45.46 3.1 24.56\n",
      "31.04 11.01 20.29 10.87 18.40 8.94 17.73 6.27 11.80 7.63 78.96 7.13 31.79 12.27 39.31\n",
      "35.07 17.57 29.98 26.10 38.64 18.59 33.99 15.83 28.26 24.63 84.68 13.55 46.66 16.18 55.35\n",
      "45.42 21.86 27.25 22.72 60.45 21.12 40.88 27.98 22.79 29.73 72.98 12.21 56.90 22.97 70.30\n",
      "49.68 26.6 33.33 31.51 68.12 29.99 49.93 36.05 32.08 42.94 85.28 16.18 68.29 24.45 72.98\n",
      "50.49 27.11 36.53 34.70 72.73 33.29 52.84 38.83 33.89 46.70 85.60 16.57 70.17 23.44 75.17\n",
      "78.70 69.99 65.08 67.98 64.03 76.59 72.98 79.45 60.97 72.25\n",
      "62.01 62.86 62.60 59.62 57.03 71.57 70.75 76.73 39.75 69.77\n",
      "81.64 77.58 72.37 80.60 75.59 85.39 78.99 87.59 67.21 82.03\n",
      "83.57 79.32 73.08 82.13 76.73 85.58 80.23 88.63 65.67 83.09\n",
      "74.18 79.61 76.02 80.70 78.85 85.84 81.05 86.87 61.72 84.42\n",
      "80.43 80.59 72.63 83.48 78.00 85.66 80.03 90.60 67.95 83.42\n",
      "76.27 79.62 77.90 85.11 80.81 87.48 83.20 86.99 63.06 86.82\n",
      "78.04 77.48 72.30 81.49 74.74 84.28 82.06 87.08 64.71 83.78\n",
      "70.93 74.57 69.17 77.23 70.99 79.74 77.93 87.33 59.64 79.54\n",
      "79.50 79.59 74.29 85.35 79.21 85.52 82.54 90.44 63.20 85.67\n",
      "75.21 65.93 66.53 76.17 69.05 79.24 76.07 84.95 65.66 75.34\n",
      "83.02 67.23 66.59 77.33 71.83 80.66 78.91 86.99 67.30 77.59\n",
      "84.84 68.20 66.99 77.58 72.78 82.62 80.10 87.25 68.75 79.21\n",
      "31.05\n",
      "26.8\n",
      "30.81\n",
      "27.9\n",
      "30.67\n",
      "27.49\n",
      "31.57\n",
      "26.94\n",
      "30.26\n",
      "30.38\n",
      "28.90\n",
      "25.44\n",
      "27.87\n",
      "45.21\n",
      "34.95\n",
      "56.26\n",
      "56.53\n",
      "52.44\n",
      "57.78\n",
      "54.71\n",
      "49.52\n",
      "45.97\n",
      "53.74\n",
      "51.23\n",
      "56.11\n",
      "57.12\n",
      "Table 11: All English results. The main score for each task is reported as described in Section 3.2.\n",
      "SGPT-5.8B- msmarco\n",
      "69.22 71.26 39.19 84.49 49.66 66.64 70.39 76.28 93.47 72.42 67.71 56.85\n",
      "45.59 38.86 36.55 33.70 31.51 28.76 40.45 55.75 59.21 33.95 39.46\n",
      "93.84 66.87 85.29\n",
      "61.63 32.29 80.79 51.53\n",
      "51.38 30.46 39.40 39.87 78.24 37.20 59.26 39.91 36.21 52.41 84.58 19.87 74.70 25.43 84.88\n",
      "86.25 69.63 67.50 79.16 74.46 84.47 80.96 87.78 69.35 81.39\n",
      "24.75\n",
      "58.81\n",
      "SGPT- BLOOM-7.1B- msmarco\n",
      "68.06 68.97 33.86 84.33 44.87 61.77 69.67 75.34 93.68 71.34 66.55 55.85\n",
      "44.59 38.03 36.03 32.48 31.05 29.26 35.53 54.52 55.13 34.31 37.28\n",
      "94.93 65.31 85.46\n",
      "59.97 31.79 79.77 51.07\n",
      "47.28 29.39 39.62 39.03 73.97 35.84 57.26 41.12 35.78 53.15 74.71 18.62 72.11 23.98 81.37\n",
      "85.31 69.82 69.66 79.67 74.61 83.81 80.40 87.07 66.13 80.90\n",
      "24.99\n",
      "57.44\n",
      "GTR- Base\n",
      "69.33 67.82 38.48 79.26 42.20 65.99 67.05 75.40 92.42 62.44 66.60 56.02\n",
      "35.49 27.18 27.66 23.25 27.57 25.13 56.13 58.53 64.21 33.01 46.72\n",
      "94.55 72.23 84.77\n",
      "60.86 31.33 73.71 51.01\n",
      "50.83 24.88 34.55 35.24 68.93 35.15 54.93 41.16 30.22 50.47 87.98 14.00 59.74 25.89 56.05\n",
      "79.00 71.45 68.59 79.09 74.64 84.85 81.57 85.80 66.17 79.58\n",
      "29.67\n",
      "56.19\n",
      "GTR- Large\n",
      "70.03 73.92 37.21 81.21 46.32 70.86 70.06 75.49 94.01 63.86 68.65 54.09\n",
      "37.50 30.55 29.59 25.72 28.72 27.39 61.69 61.67 69.93 33.21 51.64\n",
      "95.05 76.03 84.89\n",
      "61.64 31.84 76.39 51.58\n",
      "52.09 26.90 36.62 39.55 72.66 42.79 57.85 42.73 32.63 55.09 88.47 15.51 63.42 28.29 56.68\n",
      "84.86 73.39 70.33 82.19 77.16 86.31 81.85 83.93 64.30 77.60\n",
      "29.50\n",
      "58.28\n",
      "GTR- XL\n",
      "68.60 74.58 38.20 82.22 45.55 68.15 70.23 75.94 93.60 65.93 67.56 54.77\n",
      "37.90 30.45 30.52 26.06 28.69 26.69 61.34 61.11 69.95 32.73 51.15\n",
      "95.45 77.81 85.14\n",
      "63.08 31.50 76.49 52.79\n",
      "52.81 27.01 37.35 39.74 72.18 44.19 58.91 43.52 33.34 56.16 88.91 15.71 64.20 25.26 60.09\n",
      "78.94 73.63 69.11 81.82 77.07 86.01 82.23 84.90 66.61 77.65\n",
      "30.21\n",
      "58.42\n",
      "GTR- XXL\n",
      "67.30 75.05 37.30 82.32 43.19 70.8 70.61 77.77 93.84 67.71 68.48 54.54\n",
      "37.90 32.39 30.48 27.50 29.12 27.56 64.13 62.84 71.43 32.85 50.44\n",
      "95.68 77.54 85.13\n",
      "63.23 31.93 77.96 53.50\n",
      "53.77 27.21 38.56 41.28 74.08 46.78 59.67 44.05 34.18 57.24 89.09 15.88 66.77 26.76 51.90\n",
      "81.91 74.29 70.12 82.72 78.24 86.26 81.61 85.18 65.76 77.73\n",
      "30.64\n",
      "58.97\n",
      "ST5- Base\n",
      "75.82 85.12 44.94 76.48 51.36 77.34 69.74 72.32 90.34 63.32 68.20 62.71\n",
      "39.28 27.26 33.99 22.92 33.20 26.13 52.93 59.67 63.13 35.68 48.10\n",
      "91.23 78.25 86.05\n",
      "59.73 30.20 73.96 48.46\n",
      "44.85 10.37 35.23 27.77 26.16 34.83 33.20 20.71 28.64 36.32 85.49 14.16 45.76 20.30 40.70\n",
      "75.89 80.18 78.05 85.85 82.19 87.46 84.03 89.57 62.66 85.52\n",
      "31.39\n",
      "55.27\n",
      "ST5- Large\n",
      "75.51 92.87 47.12 78.46 51.73 87.01 71.78 73.16 90.99 64.98 71.73 62.33\n",
      "41.62 29.44 35.99 24.02 32.40 26.33 54.53 62.50 65.11 36.86 49.33\n",
      "89.01 79.75 86.14\n",
      "61.51 30.27 74.88 49.34\n",
      "39.27 11.36 38.96 31.55 36.21 43.55 33.95 23.96 31.10 42.02 85.73 15.38 49.91 21.63 46.11\n",
      "78.93 80.34 79.11 87.33 83.17 88.28 84.36 88.99 62.39 85.36\n",
      "29.64\n",
      "57.06\n",
      "ST5- XL\n",
      "76.01 93.17 48.18 80.88 51.95 87.54 72.09 73.26 90.73 68.15 70.95 61.21\n",
      "41.62 31.17 36.43 26.47 32.30 26.93 57.03 62.34 67.13 34.79 49.53\n",
      "91.44 80.89 85.86\n",
      "62.86 29.77 75.16 51.05\n",
      "39.40 10.61 40.78 33.65 36.12 44.71 37.17 25.17 33.18 46.29 85.85 15.97 50.91 22.51 54.77\n",
      "73.12 79.98 79.02 88.80 84.33 88.89 85.31 88.91 64.32 83.93\n",
      "29.91\n",
      "57.87\n",
      "ST5- XXL\n",
      "77.07 92.79 48.93 82.31 48.57 90.23 73.44 74.82 92.49 68.33 70.04 62.01\n",
      "42.89 33.47 36.53 28.66 32.09 26.82 58.99 64.46 70.78 35.25 50.93\n",
      "88.89 80.28 86.01\n",
      "66.16 30.60 76.09 52.85\n",
      "39.85 14.63 44.65 39.19 51.20 46.68 42.14 27.68 35.08 52.87 85.96 17.17 55.38 21.65 59.48\n",
      "80.43 80.47 78.85 88.94 84.86 89.32 84.67 89.46 65.33 84.01\n",
      "30.08\n",
      "59.51\n",
      "Dataset\n",
      "Language LASER2 LaBSE MiniLM-L12-multilingual MPNet-multilingual SGPT-BLOOM-7.1B-msmarco\n",
      "de-en BUCC fr-en BUCC ru-en BUCC zh-en BUCC sqi-eng Tatoeba fry-eng Tatoeba kur-eng Tatoeba tur-eng Tatoeba deu-eng Tatoeba nld-eng Tatoeba ron-eng Tatoeba ang-eng Tatoeba ido-eng Tatoeba jav-eng Tatoeba isl-eng Tatoeba slv-eng Tatoeba cym-eng Tatoeba kaz-eng Tatoeba est-eng Tatoeba heb-eng Tatoeba gla-eng Tatoeba mar-eng Tatoeba lat-eng Tatoeba bel-eng Tatoeba pms-eng Tatoeba gle-eng Tatoeba pes-eng Tatoeba nob-eng Tatoeba bul-eng Tatoeba cbk-eng Tatoeba hun-eng Tatoeba uig-eng Tatoeba rus-eng Tatoeba spa-eng Tatoeba hye-eng Tatoeba tel-eng Tatoeba Tatoeba afr-eng Tatoeba mon-eng arz-eng Tatoeba hrv-eng Tatoeba nov-eng Tatoeba gsw-eng Tatoeba nds-eng Tatoeba ukr-eng Tatoeba uzb-eng Tatoeba lit-eng Tatoeba ina-eng Tatoeba lfn-eng Tatoeba zsm-eng Tatoeba ita-eng Tatoeba cmn-eng Tatoeba lvs-eng Tatoeba glg-eng Tatoeba ceb-eng Tatoeba bre-eng Tatoeba ben-eng Tatoeba swg-eng Tatoeba arq-eng Tatoeba kab-eng Tatoeba fra-eng Tatoeba por-eng Tatoeba tat-eng Tatoeba oci-eng Tatoeba pol-eng Tatoeba war-eng Tatoeba aze-eng Tatoeba vie-eng Tatoeba nno-eng Tatoeba Tatoeba cha-eng Tatoeba mhr-eng dan-eng Tatoeba ell-eng Tatoeba amh-eng Tatoeba pam-eng Tatoeba hsb-eng Tatoeba srp-eng Tatoeba epo-eng Tatoeba kzj-eng Tatoeba awa-eng Tatoeba fao-eng Tatoeba mal-eng Tatoeba ile-eng Tatoeba bos-eng Tatoeba cor-eng Tatoeba cat-eng Tatoeba eus-eng Tatoeba yue-eng Tatoeba swe-eng Tatoeba dtp-eng Tatoeba kat-eng Tatoeba jpn-eng Tatoeba csb-eng Tatoeba xho-eng Tatoeba orv-eng Tatoeba ind-eng Tatoeba Tatoeba tuk-eng Tatoeba max-eng swh-eng Tatoeba hin-eng Tatoeba dsb-eng Tatoeba ber-eng Tatoeba tam-eng Tatoeba slk-eng Tatoeba tgl-eng Tatoeba Tatoeba ast-eng Tatoeba mkd-eng khm-eng Tatoeba ces-eng Tatoeba tzl-eng Tatoeba urd-eng Tatoeba ara-eng Tatoeba kor-eng Tatoeba yid-eng Tatoeba n-eng Tatoeba tha-eng Tatoeba wuu-eng Tatoeba\n",
      "99.21 98.39 97.62 97.70 97.22 42.07 19.09 98.03 99.07 95.35 96.52 25.22 80.86 9.95 94.32 95.40 5.85 53.30 96.43 0.00 1.52 92.93 64.81 79.54 36.23 4.20 93.13 95.77 93.57 77.17 95.20 56.49 92.58 97.33 88.72 96.72 92.59 3.42 66.16 96.72 60.02 27.52 77.13 93.52 23.20 96.20 93.93 63.39 95.41 94.32 85.62 95.33 96.14 9.93 31.2 89.43 33.10 26.63 65.88 94.28 94.54 34.74 58.13 97.32 8.25 82.41 96.73 72.75 14.86 6.86 95.22 96.20 80.82 3.24 45.75 93.64 96.61 4.46 33.74 57.04 98.16 87.88 95.86 4.45 95.80 93.32 87.75 95.31 7.39 81.16 93.78 27.03 4.68 23.24 92.98 16.35 36.96 55.66 95.32 42.34 77.63 87.32 95.82 63.19 76.35 93.63 74.19 95.52 36.56 84.23 90.14 87.97 2.49 96.98 96.38 75.09\n",
      "99.35 98.72 97.78 99.16 96.76 89.31 83.59 98.00 99.20 96.07 96.92 59.28 89.42 79.77 94.75 96.03 92.00 87.49 96.55 91.53 85.66 92.65 80.07 95.00 64.57 93.80 94.70 98.40 94.58 79.44 96.55 92.40 93.75 98.40 94.09 97.86 96.18 95.91 76.00 96.95 74.38 46.50 79.42 93.97 84.23 96.47 95.37 67.54 95.62 92.72 95.10 95.88 96.82 64.42 15.07 88.55 59.36 42.69 4.31 94.86 94.14 85.92 65.81 97.22 60.29 94.93 97.20 94.48 31.77 15.74 95.71 95.35 91.47 10.73 67.11 94.43 98.20 11.33 71.70 87.40 98.45 85.58 94.92 10.11 95.38 95.01 89.58 95.63 10.85 95.02 95.38 52.57 91.55 38.93 93.66 75.27 63.26 84.50 96.87 64.81 8.40 89.0 96.5 96.02 90.68 93.6 78.37 96.68 58.88 93.22 88.80 90.95 88.79 96.37 96.14 90.18\n",
      "97.11 94.99 95.06 95.63 98.17 31.13 46.94 95.08 97.02 94.58 95.30 10.24 40.25 17.04 24.07 96.92 13.25 34.89 97.33 86.88 3.61 92.38 19.47 67.73 30.70 11.62 92.59 97.73 92.65 55.37 91.58 24.39 91.87 95.42 93.28 36.40 58.22 95.04 51.26 95.98 47.99 25.74 32.16 92.82 17.14 93.16 79.13 47.02 95.31 93.05 94.93 97.87 94.00 8.05 5.56 36.48 26.31 18.60 1.16 91.72 92.13 10.25 38.57 94.28 7.25 62.10 95.12 76.34 15.98 6.89 94.80 95.43 36.21 5.41 36.10 92.24 41.73 6.24 33.43 27.51 32.20 57.71 93.27 3.42 94.42 23.18 71.45 94.42 5.69 95.44 90.41 21.56 4.52 15.10 92.74 15.16 45.25 14.48 97.62 33.43 4.43 24.64 95.15 13.09 62.17 91.00 32.11 95.12 25.46 94.57 87.93 92.52 14.38 93.10 96.72 76.00\n",
      "98.59 96.89 96.44 97.56 98.57 43.54 61.44 96.17 97.73 95.50 96.43 16.72 43.91 23.39 59.25 97.08 22.31 61.49 98.40 88.26 4.72 93.83 24.25 79.94 34.19 16.85 93.47 98.53 93.52 58.68 94.18 48.35 92.92 97.00 94.38 79.73 72.96 96.14 55.69 97.00 50.23 25.12 38.88 92.67 23.19 95.37 84.32 49.56 95.80 93.76 95.83 97.53 95.32 7.39 6.42 64.90 22.80 19.84 1.41 93.12 93.02 10.89 43.49 96.95 7.42 76.36 97.23 81.41 12.59 7.57 96.17 94.93 53.49 5.39 44.32 94.12 55.12 5.88 42.83 38.24 88.46 60.36 94.02 3.53 96.05 31.33 77.58 95.45 5.03 95.46 92.51 23.73 6.53 23.77 93.50 14.91 48.77 16.02 97.75 36.85 4.88 73.60 96.62 17.67 70.08 93.02 58.80 95.73 34.21 95.12 90.19 93.07 30.73 95.92 95.99 78.25\n",
      "54.00 97.06 45.30 97.96 10.38 24.62 8.26 6.15 70.10 29.74 27.23 28.76 43.91 15.02 6.29 10.14 6.97 3.32 4.76 1.69 2.09 45.53 28.76 8.03 31.94 3.26 12.13 21.07 20.09 64.63 5.07 1.27 59.84 94.48 0.50 64.62 16.62 2.85 70.66 12.79 52.23 21.03 23.92 22.06 4.71 4.49 73.67 44.85 79.95 65.04 91.45 6.55 79.86 6.64 4.67 75.98 16.89 27.75 1.69 91.44 92.62 3.59 40.17 14.09 10.38 6.32 94.20 16.28 23.26 1.56 23.52 5.34 0.03 5.85 9.68 11.69 26.20 5.17 35.01 12.61 83.30 59.59 13.65 2.83 88.31 53.38 77.03 19.53 3.41 0.42 71.36 10.03 5.51 5.79 88.04 5.48 36.14 16.74 85.23 8.78 4.92 72.76 9.98 10.70 71.13 10.47 0.37 9.55 27.82 70.10 85.37 22.39 0.16 3.41 2.22 79.58\n",
      "Average\n",
      "mix\n",
      "67.42\n",
      "81.75\n",
      "57.98\n",
      "63.38\n",
      "31.08\n",
      "Table 12: Multilingual bitext mining results. Scores are f1.\n",
      "Dataset\n",
      "Language LASER2 LaBSE MiniLM-L12-multilingual MPNet-multilingual SGPT-BLOOM-7.1B-msmarco\n",
      "AmazonCounterfactualClassication AmazonCounterfactualClassication AmazonReviewsClassication AmazonReviewsClassication AmazonReviewsClassication AmazonReviewsClassication AmazonReviewsClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveIntentClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MassiveScenarioClassication MTOPDomainClassication MTOPDomainClassication MTOPDomainClassication MTOPDomainClassication MTOPDomainClassication MTOPIntentClassication MTOPIntentClassication MTOPIntentClassication MTOPIntentClassication MTOPIntentClassication\n",
      "de ja de es fr ja zh af am ar az bn cy da de el es fa  fr he hi hu hy id is it ja jv ka km kn ko lv ml mn ms my nb nl pl pt ro ru sl sq sv sw ta te th tl tr ur vi zh-CN zh-TW af am ar az bn cy da de el es fa  fr he hi hu hy id is it ja jv ka km kn ko lv ml mn ms my nb nl pl pt ro ru sl sq sv sw ta te th tl tr ur vi zh-CN zh-TW de es fr hi th de es fr hi th\n",
      "67.82 68.76 31.07 32.72 31.12 28.94 30.89 38.01 12.70 37.16 19.98 42.51 17.33 45.61 44.79 46.71 45.44 45.01 45.94 46.13 42.55 40.20 42.77 28.07 45.81 39.86 48.25 45.30 24.30 22.70 22.48 4.32 44.26 39.75 41.33 16.20 43.23 25.37 37.74 45.00 44.99 48.55 44.30 44.29 44.72 46.12 45.95 31.89 29.63 36.03 43.39 29.73 43.93 26.11 44.33 40.62 32.93 47.10 17.70 45.21 28.21 50.52 22.58 54.87 54.34 55.47 52.77 52.50 52.63 54.32 52.41 47.37 53.43 33.57 54.38 49.78 54.84 54.12 32.71 26.92 27.23 10.06 52.01 44.82 49.10 21.51 53.60 29.72 43.90 53.33 52.92 53.41 50.48 51.84 51.29 55.65 54.64 42.04 36.72 42.08 52.15 37.34 52.56 32.60 50.97 50.22 42.32 74.08 73.47 72.26 72.95 72.68 51.62 52.75 50.12 45.55 50.07\n",
      "73.17 76.42 39.92 39.39 38.52 36.44 36.45 56.12 55.71 50.86 58.97 58.22 50.16 58.25 56.21 57.03 58.32 62.33 60.12 60.47 56.55 59.40 59.52 56.20 61.12 54.90 59.83 63.11 50.98 48.35 48.55 56.24 60.99 57.10 57.91 58.50 58.60 57.35 57.91 59.37 59.71 60.16 57.92 60.67 59.37 58.03 59.66 51.62 55.04 58.32 56.58 55.28 60.91 56.70 56.67 63.86 59.51 63.39 62.02 57.72 63.48 61.84 56.13 65.24 62.39 64.58 63.61 67.46 64.58 65.10 63.53 64.40 65.82 61.25 65.84 61.94 64.09 67.72 58.29 53.38 56.18 61.74 67.26 61.87 62.26 62.60 65.63 62.94 64.29 65.16 64.56 63.28 62.41 65.25 64.25 64.54 66.01 58.36 59.08 64.13 64.34 60.23 65.43 61.52 61.05 70.85 67.08 86.95 84.07 84.14 85.11 81.24 63.42 64.44 62.01 62.58 64.61\n",
      "68.35 63.45 35.91 37.49 35.30 33.24 35.26 45.88 36.75 45.14 47.42 35.34 26.12 57.73 50.71 58.70 59.66 61.02 57.54 60.25 52.51 58.37 60.41 51.60 59.85 30.83 59.61 60.89 32.37 43.03 40.04 40.98 50.30 54.68 42.41 51.77 54.76 52.01 55.50 59.51 59.43 61.27 58.39 59.04 57.36 56.59 59.43 29.57 36.77 40.72 58.97 33.67 59.90 52.80 56.61 61.99 58.77 53.64 41.89 51.74 52.06 41.17 31.72 66.87 57.40 66.14 65.04 65.86 63.75 66.06 59.20 65.21 66.56 56.11 66.16 37.52 65.00 66.50 38.60 50.66 46.96 45.73 55.66 59.80 47.69 57.07 61.71 59.10 64.25 65.52 65.04 65.79 64.17 65.24 64.01 64.31 67.14 34.86 42.62 46.46 67.01 37.37 66.55 60.43 60.72 67.44 65.70 79.20 83.04 78.63 81.36 79.99 54.23 60.28 54.05 59.90 61.96\n",
      "69.95 69.79 39.52 39.99 39.00 36.64 37.74 52.32 41.55 51.43 56.98 48.79 27.87 62.77 59.57 62.62 64.43 65.34 62.28 64.82 58.21 62.77 63.87 57.74 65.43 37.05 64.68 63.74 36.49 49.85 45.47 50.63 61.82 61.29 54.34 56.59 60.70 57.09 62.60 63.57 64.30 64.89 62.80 63.26 63.51 62.49 64.73 31.95 50.17 52.82 61.11 38.83 64.54 56.37 59.68 65.33 62.35 59.67 48.97 57.78 61.53 54.53 35.26 71.00 67.34 68.81 70.42 69.88 67.60 70.69 65.16 67.92 70.30 63.02 70.73 44.16 69.73 69.69 44.20 57.30 53.14 56.08 68.52 66.28 60.13 60.85 65.81 63.03 70.24 70.37 68.99 70.09 67.95 69.92 70.81 69.63 71.60 37.29 55.96 58.81 69.44 43.99 70.4 62.9 65.71 71.23 68.73 85.73 86.96 81.21 84.76 82.51 61.27 66.59 59.76 62.37 64.80\n",
      "61.35 58.23 29.70 35.97 35.92 27.64 32.63 47.85 33.30 59.25 45.24 61.59 44.92 51.23 56.10 46.13 66.35 51.20 45.33 66.95 43.18 63.54 44.73 38.13 64.06 44.35 60.77 61.22 50.94 33.84 37.34 53.54 53.36 46.50 58.27 40.28 59.65 37.42 49.41 52.09 50.48 66.69 50.53 58.32 47.74 48.94 50.79 49.81 56.40 54.71 44.43 50.21 46.56 56.75 64.53 67.07 62.89 51.47 34.87 65.21 45.58 67.30 46.29 53.52 61.74 48.96 73.34 53.17 44.69 72.91 43.10 69.27 45.16 38.73 70.13 44.21 65.57 65.76 54.79 32.99 39.34 60.50 55.69 44.35 65.53 38.72 64.99 36.84 51.80 56.32 49.98 71.46 53.69 61.60 48.04 50.06 51.73 54.22 62.77 62.59 45.18 52.06 47.21 64.26 70.61 73.95 70.30 82.05 93.55 90.98 89.33 60.49 61.92 74.49 69.12 64.85 49.36\n",
      "Average\n",
      "mix\n",
      "42.85\n",
      "60.77\n",
      "54.87\n",
      "60.39\n",
      "54.4\n",
      "Table 13: Multilingual classication results. Scores are accuracy.\n",
      "Dataset\n",
      "Language Komninos LASER2 LaBSE MiniLM-L12-multilingual MPNet-multilingual SGPT-BLOOM-7.1B-msmarco\n",
      "STS17 STS17 STS17 STS17 STS17 STS17 STS17 STS17 STS17 STS17 STS22 STS22 STS22 STS22 STS22 STS22 STS22 STS22 STS22 STS22 STS22 STS22 STS22 STS22 STS22 STS22 STS22\n",
      "ko-ko ar-ar en-ar en-de en-tr es-en es-es fr-en it-en nl-en de es pl tr ar ru zh fr de-en es-en it pl-en zh-en es-it de-fr de-pl fr-pl\n",
      "2.54 13.78 9.08 -3.11 -0.45 -8.18 48.23 5.81 3.64 -0.44 33.04 48.53 12.47 47.38 32.42 19.44 4.78 49.43 28.65 26.97 57.77 45.55 14.05 41.10 14.77 11.21 39.44\n",
      "70.52 67.47 65.05 66.66 70.05 55.30 79.67 70.82 70.98 68.12 25.69 54.92 18.34 36.97 42.57 39.24 49.41 58.61 32.35 54.34 60.31 53.63 46.19 42.21 37.41 15.67 39.44\n",
      "71.32 69.07 74.51 73.85 72.07 65.71 80.83 76.98 76.99 75.22 48.58 63.18 39.30 58.15 57.67 57.49 63.02 77.95 50.14 71.86 72.22 69.41 64.02 69.69 53.28 58.69 61.98\n",
      "77.03 79.16 81.22 84.22 76.74 84.44 85.56 76.59 82.35 81.71 44.64 56.56 33.74 53.39 46.2 57.08 58.75 70.55 52.65 67.33 55.22 69.02 65.71 47.67 51.73 44.22 50.71\n",
      "83.41 79.10 80.85 83.28 74.90 86.11 85.14 81.17 84.24 82.51 46.70 59.91 33.65 56.30 52.19 58.74 61.75 74.30 50.81 70.26 60.65 73.07 67.96 53.70 62.34 40.53 84.52\n",
      "66.89 76.42 78.07 59.10 11.80 78.22 86.00 80.46 51.58 45.85 30.05 65.41 31.13 47.14 58.67 43.36 66.78 80.38 51.16 75.06 65.65 53.31 68.45 65.50 53.28 43.05 28.17\n",
      "Average mix\n",
      "22.14\n",
      "51.55\n",
      "65.67\n",
      "64.23\n",
      "67.71\n",
      "57.81\n",
      "Table 14: Multilingual STS Results. Scores are Spearman correlations of cosine similarities.\n",
      "######## MarginMSELoss.pdf ######## \n",
      "\n",
      "\n",
      "1 2 0 2\n",
      "n a J\n",
      "2 2\n",
      "]\n",
      "R\n",
      "I . s c [\n",
      "2 v 6 6 6 2 0 . 0 1 0 2 : v i X r a\n",
      "Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation\n",
      "Sebastian Hofsttter, Sophia Althammer, Michael Schrder, Mete Sertkan and Allan Hanbury first.last@tuwien.ac.at TU Wien, Vienna, Austria\n",
      "ABSTRACT Retrieval and ranking models are the backbone of many applications such as web search, open domain QA, or text-based recommender systems. The latency of neural ranking models at query time is largely dependent on the architecture and deliberate choices by their designers to trade-off effectiveness for higher efficiency. This focus on low query latency of a rising number of efficient ranking architectures make them feasible for production deployment. In machine learning an increasingly common approach to close the effectiveness gap of more efficient models is to apply knowledge distillation from a large teacher model to a smaller student model. We find that different ranking architectures tend to produce output scores in different magnitudes. Based on this finding, we propose a cross-architecture training procedure with a margin focused loss (Margin-MSE), that adapts knowledge distillation to the varying score output distributions of different BERT and non-BERT pas- sage ranking architectures. We apply the teachable information as additional fine-grained labels to existing training triples of the MSMARCO-Passage collection. We evaluate our procedure of dis- tilling knowledge from state-of-the-art concatenated BERT models to four different efficient architectures (TK, ColBERT, PreTT, and a BERT CLS dot product model). We show that across our evaluated architectures our Margin-MSE knowledge distillation significantly improves re-ranking effectiveness without compromising their effi- ciency. Additionally, we show our general distillation method to improve nearest neighbor based index retrieval with the BERT dot product model, offering competitive results with specialized and much more costly training methods. To benefit the community, we publish the teacher-score training files in a ready-to-use package.\n",
      "1 INTRODUCTION The same principles that applied to traditional IR systems to achieve low query latency also apply to novel neural ranking models: We need to transfer as much computation and data transformation to the indexing phase as possible to require less resources at query time [33, 34]. For the most effective BERT-based [11] neural ranking models, which we refer to as BERTCAT, this transfer is simply not possible, as the concatenation of query and passage require all Transformer layers to be evaluated at query time to receive a ranking score [36].\n",
      "To overcome this architecture restriction the neural-IR com- munity proposed new architectures by deliberately choosing to trade-off effectiveness for higher efficiency. Among these low query latency approaches are: TK [18] with shallow Transformers and separate query and document contextualization; ColBERT [21] with late-interactions of BERT term representations; PreTT [29] with a combination of query-independent and query-dependent Transformer layers; and a BERT-CLS dot product scoring model\n",
      "200\n",
      "200Output Score\n",
      "50\n",
      "100\n",
      "100\n",
      "150\n",
      "300\n",
      "500Training Batch Count (Thousands)\n",
      "0\n",
      "0\n",
      "Neg. Average\n",
      "BERTDOT\n",
      "50\n",
      "ColBERT\n",
      "Pos. Average\n",
      "TK\n",
      "BERTCAT\n",
      "400\n",
      "Figure 1: Raw query-passage pair scores during training of different ranking models. The margin between the positive and negative samples is shaded.\n",
      "which we refer to as BERTDOT, also known in the literature as Tower-BERT [4], BERT-Siamese [44], or TwinBERT [27].1 Each approach has unique characteristics that make them suitable for production-level query latency which we discuss in Section 2.\n",
      "An increasingly common way to improve smaller or more effi- cient models is to train them, as students, to imitate the behavior of larger or ensemble teacher models via Knowledge Distillation (KD) [15]. This is typically applied to the same architecture with fewer layers and dimensions [20, 38] via the output or layer-wise activations [39]. KD has been applied in the ranking task for the same architecture with fewer layers [5, 14, 25] and in constrained sub-tasks, such as keyword-list matching [27].\n",
      "In this work we propose a model-agnostic training procedure using cross-architecture knowledge distillation from BERTCAT with the goal to improve the effectiveness of efficient passage ranking models without compromising their query latency benefits.\n",
      "A unique challenge for knowledge distillation in the ranking task is the possible range of scores, i.e. a ranking model outputs a single unbounded decimal value and the final result solely depends on the relative ordering of the scores for the candidate documents per query. We make the crucial observation, depicted in Figure 1, that different architectures during their training gravitate towards unique range patterns in their output scores. The BERTCAT model exhibits positive relevant-document scores, whereas on average the non-relevant documents are below zero. The TK model solely\n",
      "1Yes, we see the irony: https://xkcd.com/927/\n",
      "produces negative averages, and the BERTDOT and ColBERT mod- els, due to their dot product scoring, show high output scores. This leads us to our main research question:\n",
      "RQ1 How can we apply knowledge distillation in retrieval across\n",
      "architecture types?\n",
      "To optimally support the training of cross-architecture knowl- edge distillation, we allow our models to converge to a free scoring range, as long as the margin is alike with the teacher. We make use of the common triple (q, relevant doc, non-relevant doc) training regime, by distilling knowledge via the margin of the two scoring pairs. We train the students to learn the same margin as their teach- ers, which leaves the models to find the most comfortable or natural range for their architecture. We optimize the student margin to the teacher margin with a Mean Squared Error loss (Margin-MSE). We confirm our strategy with an ablation study of different knowledge distillation losses and show the Margin-MSE loss to be the most effective.\n",
      "Thanks to the rapid advancements and openness of the Natural Language Processing community, we have a number of pre-trained BERT-style language models to choose from to create different vari- ants of the BERTCAT architecture to study, allowing us to answer: RQ2 How effective is the distillation with a single teacher model\n",
      "in comparison to an ensemble of teachers?\n",
      "We train three different BERTCAT versions as teacher models with different initializations: BERT-Base [11], BERT-Large with whole word masking [11], and ALBERT-large [24]. To understand the behavior that the different language models bring to the BERTCAT architecture, we compare their training score margin distributions and find that the models offer variability suited for an ensemble.\n",
      "We created the teacher ensemble by averaging each of the three scores per query-document pair. We conduct the knowledge distil- lation with a single teacher and a teacher ensemble. The knowledge distillation has a general positive effect on all retrieval effectiveness metrics of our student models. In most cases the teacher ensemble further improves the student models effectiveness in the re-ranking scenario above the already improved single teacher training.\n",
      "The dual-encoder BERTDOT model can be used for full collec- tion indexing and retrieval with a nearest neighbor vector search approach, so we study:\n",
      "RQ3 How effective is our distillation for dense nearest neighbor\n",
      "retrieval?\n",
      "We observe similar trends in terms of effectiveness per teacher strategy, with increased effectiveness of BERTDOT models for a single teacher and again a higher increase for the ensemble of teachers. Even though we do not add dense retrieval specific train- ing methods, such as index-based passage sampling [44] or in-batch negatives [26] we observe very competitive results compared to those much more costly training approaches.\n",
      "To put the improved models in the perspective of the efficiency-\n",
      "effectiveness trade-off, we investigated the following question:\n",
      "RQ4 By how much does effective knowledge distillation shift the\n",
      "balance in the efficiency-effectiveness trade-off?\n",
      "We show how the knowledge distilled efficient architectures outperform the BERTCAT baselines on several metrics. There is no longer a compromise in utilizing PreTT or ColBERT and the\n",
      "effectiveness gap, i.e. the difference between the most effective and the other models, of BERTDOT and TK is significantly smaller.\n",
      "The contributions of this work are as follows:\n",
      "We propose a cross-architecture knowledge distillation pro- cedure with a Margin-MSE loss for a range of neural retrieval architectures\n",
      "We conduct a comprehensive study of the effects of cross- architecture knowledge distillation in the ranking scenario  We publish our source code as well as ready-to-use teacher training files for the community at: https://github.com/sebastian-hofstaetter/neural-ranking-kd\n",
      "2 RETRIEVAL MODELS We study the effects of knowledge distillation on a wide range of recently introduced Transformer- & BERT-based ranking models. We describe their architectures in detail below and summarize them in Table 1.\n",
      "2.1 BERTCAT Concatenated Scoring The common way of utilizing the BERT pre-trained Transformer model in a re-ranking scenario [31, 36, 47] is by concatenating query and passage input sequences. We refer to this base architecture as BERTCAT. In the BERTCAT ranking model, the query 1: and passage 1: sequences are concatenated with special tokens (using the ; operator) and the CLS token representation computed by BERT (selected with 1) is scored with single linear layer  :\n",
      "BERTCAT (1:, 1:) = BERT([CLS; 1:; SEP; 1:])1  \n",
      "(1) We utilize BERTCAT as our teacher architecture, as it represents the current state-of-the art in terms of effectiveness, however it requires substantial compute at query time and increases the query latency by seconds [16, 44]. Simply using smaller BERT variants does not change the design flaw of having to compute every repre- sentation at query time.\n",
      "2.2 BERTDOT Dot Product Scoring In contrast to BERTCAT, which requires a full online computation, the BERTDOT model only matches a single CLS vector of the query with a single CLS vector of a passage [27, 28, 44]. The BERTDOT model uses two independent BERT computations as follows:\n",
      " = BERT([CLS; 1:])1    = BERT([CLS; 1:])1   which allows us to pre-compute every contextualized passage rep- resentation . After this, the model computes the final scores as the dot product  of  and :\n",
      "BERTDOT (1:, 1:) =   \n",
      "BERTDOT, with its bottleneck of comparing single vectors, com- presses information much more strongly than BERTCAT, which brings large query time improvements at the cost of lower effec- tiveness, as can be seen in Table 1.\n",
      "2.3 ColBERT The ColBERT model [21] is similar in nature to BERTDOT, by de- laying the interactions between query and document to after the\n",
      "(2)\n",
      "(3)\n",
      "Table 1: Comparison of model characteristics using DistilBERT instances. Effectiveness compares the baseline nDCG@10 of MSMARCO-DEV. NN Index refers to indexing the passage representations in a nearest neighbor index. | | refers to the number of passages; | | to the total number of term occurrences in the collection;  the query length; and  the document length.\n",
      "Model\n",
      "Effectiveness\n",
      "GPU Latency Memory\n",
      "Query\n",
      "Query-Passage Interaction\n",
      "Passage NN Cache\n",
      "Index\n",
      "Storage Req. ( Vector Size)\n",
      "BERTCAT\n",
      "BERTDOT ColBERT PreTT TK\n",
      "1\n",
      " 0.87  0.97  0.97  0.89\n",
      "950 ms\n",
      "23 ms 28 ms 455 ms 14 ms\n",
      "10.4 GB All TF layers\n",
      "  3.6 GB Single dot product  3.4 GB    dot products  10.9 GB Min. 1 TF layer (here 3) 1.8 GB    dot products + Kernel-pooling \n",
      "    \n",
      " | | | | | | | |\n",
      "BERT computation. ColBERT uses every query and document rep- resentation:\n",
      "1: = BERT([CLS; 1:; rep(MASK)])   1: = BERT([CLS; 1:])  \n",
      "where the rep() method repeats the MASK token a num- ber of times, set by a hyperparameter. Khattab and Zaharia [21] introduced this query augmentation method to increase the com- putational capacity of the BERT model for short queries. We inde- pendently confirmed that adding these MASK tokens improves the effectiveness of ColBERT. The interactions in the ColBERT model are aggregated with a max-pooling per query term and sum of query-term scores as follows:\n",
      "(4)\n",
      "We selected PreTT simply as a representative of this group of mod- els. Similar to ColBERT, we omitted the optional compression of representations for better comparability.\n",
      "2.5 Transformer-Kernel The Transformer-Kernel (TK) model [18] is not based on BERT pre- training, but rather uses shallow Transformers. TK independently contextualizes query 1: and passage 1: based on pre-trained word embeddings, where the intensity of the contextualization (Transformers as TF) is set by a gate :\n",
      " =    + TF(1:)  (1  )  =    + TF(1:)  (1  )\n",
      "ColBERT(1:, 1:) =\n",
      " \n",
      "1\n",
      "max 1..\n",
      " 1:  1:\n",
      "The aggregation only requires    dot product computations, making it roughly as efficient as BERTDOT, however the storage cost of pre-computing passage representations is much higher and depends on the total number of terms in the collection. Khattab and Zaharia [21] proposed to compress the dimensions of the represen- tation vectors by reducing the output features of  . We omitted this compression, as storage space is not the focus of our study and to better compare results across different models.\n",
      "2.4 PreTT The PreTT architecture [29] is conceptually between BERTCAT and ColBERT, as it allows to compute  BERT-layers separately for query and passage:\n",
      "(5)\n",
      "The sequences 1: and 1: interact in a match-matrix with a cosine similarity per term pair and each similarity is activated by a set of Gaussian kernels [43]: (cid:32)\n",
      "(cid:33)\n",
      "(cid:0)cos( ,   )   (cid:1)2 22\n",
      " , = exp\n",
      "\n",
      "Kernel-pooling is a soft-histogram, which counts the number of occurrences of similarity ranges. Each kernel  focuses on a fixed range with center  and width of .\n",
      "These kernel activations are then summed, first by the passage term dimension , log-activated, and then the query dimension is summed, resulting in a single score per kernel. The final score is calculated by a weighted sum using  :\n",
      "TK(1:, 1:) =\n",
      "(cid:18)  \n",
      "=1\n",
      "log (cid:169) (cid:173) (cid:171)\n",
      " \n",
      "=1\n",
      " , (cid:170) (cid:174) (cid:172)\n",
      "(cid:19)\n",
      " \n",
      "1: = BERT\n",
      "1:\n",
      "([CLS; 1:])\n",
      "1: = BERT\n",
      "1:\n",
      "([CLS; 1:])\n",
      "Then PreTT concatenates the sequences with a SEP separator token and computes the remaining layers to compute a total of  BERT-layers. Finally, the CLS token output is pooled with single linear layer  :\n",
      "PreTT(1:, 1:) = BERT : \n",
      "([ 1:; SEP; 1:])1  \n",
      "Concurrently to PreTT, DC-BERT [48] and EARL [13] have been proposed with very similar approaches to split Transformer layers.\n",
      "(6)\n",
      "(7)\n",
      "2.6 Comparison In Table 1 we summarize our evaluated models. We compare the ef- ficiency and effectiveness trade-off in the leftmost section, followed by a general overview of the model capabilities in the right most section. We measure the query latency for 1 query and 1000 doc- uments with cached document representations where applicable and report the peak GPU memory requirement for the inference of the validation set. We summarize our observations of the different model characteristics:\n",
      "The query latency of BERTCAT is prohibitive for efficient production use (Except for head queries that can be fully pre-computed).\n",
      "(8)\n",
      "(9)\n",
      "(10)\n",
      "Passage +\n",
      "<q, p+><q, p->\n",
      "Query\n",
      "Student\n",
      "Passage +\n",
      " Teacher Inference\n",
      "ResultStorePassage -\n",
      "BERTCAT Teacher Training\n",
      "BERTCAT\n",
      "BERTCAT\n",
      "BERTCAT\n",
      "Query\n",
      "Margin-MSE\n",
      "Ranknet\n",
      "Passage -\n",
      "Student Student Training\n",
      "Figure 2: Our knowledge distillation process, re-visiting the same training triples in all steps:  Training the BERTCAT model;  Using the trained BERTCAT to create scores for all training triples;  Individually training the student models with Margin- MSE using the teacher scores.\n",
      "BERTDOT is the most efficient BERT-based model with re- gards to storage and query latency, at the cost of lower ef- fectiveness compared to ColBERT and PreTT.\n",
      "PreTT highly depends on the choice of the concatenation- layer hyperparameter, which we set to 3 to be between BERTCAT and ColBERT.\n",
      "the relevant and the non-relevant sample passage per query. We call our proposed approach Margin Mean Squared Error (Margin-MSE). We train ranking models on batches containing triples of queries , relevant passages  +, and non-relevant passages  . We utilize the output margin of the teacher model  as label to optimize the weights of the student model  :\n",
      "ColBERT is especially suited for small collections, as it re- quires a large passage cache.\n",
      "TK is less effective overall, however it is much cheaper to run than the other models.\n",
      "L (,  +,  ) = MSE( (,  +)   (,  ),  (,  +)   (,  ))\n",
      "The most suitable neural ranking model ultimately depends on the exact scenario. To allow people to make the choice, we evaluated all presented models. we use BERTCAT as our teacher architecture and the other presented architectures as students.\n",
      "MSE is the Mean Squared Error loss function, calculating the mean of the squared differences between the scores  and the targets  over the batch size:\n",
      "3 CROSS-ARCHITECTURE\n",
      "MSE(, ) =\n",
      "1 | |\n",
      "\n",
      " , \n",
      "(  )2\n",
      "KNOWLEDGE DISTILLATION\n",
      "The established approach to training deep neural ranking models is mainly based on large-scale annotated data. Here, the MSMARCO collection is becoming the de-facto standard. The MSMARCO col- lection only contains binary annotations for fewer than two positive examples per query, and no explicit annotations for non-relevant passages. The approach proposed by Bajaj et al. [1] is to utilize ran- domly selected passages retrieved from the top 1000 candidates of a traditional retrieval system as negative examples. This approach works reasonably well, but accidentally picking relevant passages is possible.\n",
      "Neural retrieval models are commonly trained on triples of bi- nary relevance assignments of one relevant and one non-relevant passage. However, they are used in a setting that requires a much more nuanced view of relevance when they re-rank a thousand possibly relevant passages. The BERTCAT architecture shows the strongest generalization capabilities, which other architectures do not posses.\n",
      "The Margin-MSE loss discards the original binary relevance information, in contrast to other knowledge distillation approaches [25], as the margin of the teacher can potentially be negative, which would indicate a reverse ordering from the original training data. We observe that the teacher models have a very high pairwise ranking accuracy during training of over 98%, therefore we view it as redundant to add the binary information in the ranking loss.2\n",
      "In Figure 2 we show the staged process of our knowledge distilla- tion. For simplicity and ease of re-use, we utilize the same training triples for every step. The process begins with training a BERTCAT teacher model on the collection labels with a RankNet loss [3]. After the teacher training is finished, we use the teacher model again to infer all scores for the training data, without updating its weights. This allows us to store the teacher scores once, for an efficient ex- perimentation and sharing workflow. Finally, we train our student model of a different architecture, by using the teacher scores as labels with our proposed Margin-MSE loss.\n",
      "Following our observation of distinct scoring ranges of different model architectures in Figure 1, we propose to utilize a knowledge distillation loss by only optimizing the margin between the scores of\n",
      "2We do not analyze this statistic further in this paper, as we did not see a correlation or interesting difference between models on this pairwise training accuracy metric.\n",
      "(11)\n",
      "(12)\n",
      "4 EXPERIMENT DESIGN For our neural re-ranking training and inference we use PyTorch [37] and the HuggingFace Transformer library [42]. For the first stage indexing and retrieval we use Anserini [46].\n",
      "4.1 Collection & Query Sets We use the MSMARCO-Passage [1] collection with sparsely-judged MSMARCO-DEV query set of 49,000 queries as well as the densely- judged query set of 43 queries derived from TREC-DL19 [7]. For TREC graded relevance labels we use a binarization point of 2 for MRR and MAP. MSMARCO is based on sampled Bing queries and contains 8.8 million passages with a proposed training set of 40 million triples sampled. We evaluate our teachers on the full training set, so to not limit future work in terms of the number of triples available. We cap the query length at 30 tokens and the passage length at 200 tokens.\n",
      "4.2 Training Configuration We use the Adam [22] optimizer with a learning rate of 7  106 for all BERT layers, regardless of the number of layers trained. TK is the only model trained on a higher rate of 105. We employ early stopping, based on the best nDCG@10 value of the validation set. We use a training batch size of 32.\n",
      "4.3 Model Parameters All student language models use a 6-layer DistilBERT [38] as their initialization standpoint. We chose DistilBERT over BERT-Base, as it has been shown to provide a close lower bound on the re- sults at half the runtime [29, 38]. For our ColBERT implementation we repeat the query MASK augmentation 8 times, regardless of the amount of padding in a batch in contrast to Khattab and Za- haria [21]. For PreTT we decided to concatenate sequences after 3 layers of the 6 layer DistilBERT, as we want to evaluate it as a mid-choice between ColBERT and BERTCAT. For TK we use the standard 2 layer configuration with 300 dimensional embeddings. For the traditional BM25 we use the tuned parameters from the Anserini documentation.\n",
      "5 RESULTS We now discuss our research questions, starting with the study of our proposed Margin-MSE loss function; followed by an analysis of different teacher model results and their impact on the knowledge distillation; and finally examining what the knowledge distillation improvement means for the efficiency-effectiveness trade-off.\n",
      "5.1 Optimization Study We validate our approach presented in Section 3 and our research question RQ1 How can we apply knowledge distillation in retrieval across architecture types? by comparing Margin-MSE with different knowledge distillation losses using the same training data. We com- pare our approach with a pointwise MSE loss, defined as follows:\n",
      "L (,  +,  ) = MSE( (,  +),  (,  +)) +\n",
      "MSE( (,  ),  (,  ))\n",
      "(13)\n",
      "Table 2: Loss function ablation results on MSMARCO-DEV, using a single teacher (T1 in Table 3). The original training baseline is indicated by .\n",
      "Model\n",
      "KD Loss\n",
      "nDCG@10 MRR@10 MAP@100\n",
      "ColBERT\n",
      " Weighted RankNet Pointwise MSE Margin-MSE\n",
      ".417 .417 .428 .431\n",
      ".357 .356 .365 .370\n",
      ".361 .360 .369 .374\n",
      "BERTDOT\n",
      " Weighted RankNet Pointwise MSE Margin-MSE\n",
      ".373 .384 .387 .388\n",
      ".316 .326 .328 .330\n",
      ".321 .332 .332 .335\n",
      "TK\n",
      " Weighted RankNet Pointwise MSE Margin-MSE\n",
      ".384 .387 .394 .398\n",
      ".326 .328 .335 .339\n",
      ".331 .333 .340 .344\n",
      "This is a standard approach already used by Vakili Tahami et al. [41] and Li et al. [25]. Additionally, we utilize a weighted RankNet loss, where we weight the samples in a batch according to the teacher margin:\n",
      "L (,  +,  ) = RankNet( (,  +)   (,  ))  || (,  +)   (,  )||\n",
      "(14)\n",
      "We show the results of our ablation study in Table 2 for three distinct ranking architectures that significantly differ from the BERTCAT teacher model. We use a single (BERT-Base ) teacher model for this study. For each of the three architectures the Margin- MSE loss outperforms the pointwise MSE and weighted RankNet losses on all metrics. However, we also note that applying knowl- edge distillation in general improves each models result over the respective original baseline. Our aim in proposing to use the Margin- MSE loss was to create a simple yet effective solution that does not require changes to the model architectures or major adaptions to the training procedure.\n",
      "5.2 Knowledge Distillation Results Utilizing our proposed Margin-MSE loss in connection with our trained teacher models, we follow the procedure laid out in Section 3 to train our knowledge-distilled student models. Table 3 first shows our baselines, then in the second section the results of our teacher models, and in the third section our student architectures. Each student has a baseline result without teacher training (depicted by ) and a single teacher T1 as well as the teacher ensemble denoted with T2. With these results we can now answer:\n",
      "RQ2 How effective is the distillation with a single teacher model\n",
      "in comparison to an ensemble of teachers?\n",
      "We selected BERT-BaseCAT as our single teacher model, as it is a commonly used instance in neural ranking models. The ensemble of different larger BERTCAT models shows strong and consistent improvements on all MSMARCO DEV metrics and MAP@1000 of TREC-DL19. When we compare our teacher model results with the best re-ranking entry [45] of TREC-DL19, we see that our teachers,\n",
      "Table 3: Effectiveness results for both query sets of our baselines (results copied from cited models), teacher model results (with the teacher signs left of the model name), and using those teachers for our student models.\n",
      "Model\n",
      "Teacher\n",
      "TREC DL Passages 2019\n",
      "MSMARCO DEV\n",
      "nDCG@10 MRR@10 MAP@1000 nDCG@10 MRR@10 MAP@1000\n",
      "Baselines  BM25 TREC Best Re-rank [45]  BERTCAT (6-Layer Distilled Best) [14]   BERT-BaseDOT ANCE [44]\n",
      ".501 .738 .719 .677\n",
      ".689 .882  \n",
      ".295 .457  \n",
      ".241   \n",
      ".194  .356 .330\n",
      ".202   \n",
      "Teacher Models  1 BERT-BaseCAT\n",
      "BERT-Large-WMCAT ALBERT-LargeCAT\n",
      " 2 Top-3 Ensemble\n",
      "   \n",
      ".730 .742 .738 .743\n",
      ".866 .860 .903 .889\n",
      ".455 .484 .477 .495\n",
      ".437 .442 .446 .460\n",
      ".376 .381 .385 .399\n",
      ".381 .385 .388 .402\n",
      "Student Models\n",
      "DistilBERTCAT\n",
      " T1 T2\n",
      ".723 .739 .747\n",
      ".851 .889 .891\n",
      ".454 .473 .480\n",
      ".431 .440 .451\n",
      ".372 .380 .391\n",
      ".375 .383 .394\n",
      "PreTT\n",
      " T1 T2\n",
      ".717 .748 .737\n",
      ".862 .890 .859\n",
      ".438 .475 .472\n",
      ".418 .439 .447\n",
      ".358 .378 .386\n",
      ".362 .382 .389\n",
      "ColBERT\n",
      " T1 T2\n",
      ".722 .738 .744\n",
      ".874 .862 .878\n",
      ".445 .472 .478\n",
      ".417 .431 .436\n",
      ".357 .370 .375\n",
      ".361 .374 .379\n",
      "BERT-BaseDOT\n",
      " T1 T2\n",
      ".675 .677 .724\n",
      ".825 .809 .876\n",
      ".396 .427 .448\n",
      ".376 .378 .390\n",
      ".320 .321 .333\n",
      ".325 .327 .338\n",
      "DistilBERTDOT\n",
      " T1 T2\n",
      ".670 .704 .712\n",
      ".841 .821 .862\n",
      ".406 .441 .453\n",
      ".373 .388 .391\n",
      ".316 .330 .332\n",
      ".321 .335 .337\n",
      "TK\n",
      " T1 T2\n",
      ".652 .669 .666\n",
      ".751 .813 .797\n",
      ".403 .414 .415\n",
      ".384 .398 .399\n",
      ".326 .339 .341\n",
      ".331 .344 .345\n",
      "especially the ensemble outperform the TREC results to represent state-of-the-art results in terms of effectiveness.\n",
      "Overall, we observe that either a single teacher or an ensemble of teachers improves the model results over their respective original baselines. The ensemble T2 improves over T1 for all models on the sparse MSMARCO-DEV labels with many queries. Only on the TREC-DL19 query set does T2 fail to improve over T1 for TK and PreTT. The only outlier in our results is BERT-BaseDOT trained on T1, where there is no improvement over the baseline, T2 however does show a substantial improvement. This leads us to the conclusion that utilizing an ensemble of teachers is overall preferred to a single teacher model.\n",
      "a slight advantage trained on T2. However, its T1 results are in- consistent, where almost no improvement is observable, whereas DistilBERTDOT exhibits consistent gains first for T1 and then an- other step for T2.\n",
      "Our T2 training improves both instances of the BERTDOT archi- tecture in comparison to the ANCE [44] trained BERTDOT model and evaluated in the re-ranking setting.\n",
      "To also compare the BERTDOT model in the full collection vector\n",
      "retrieval setting we set out to answer:\n",
      "RQ3 How effective is our distillation for dense nearest neighbor\n",
      "retrieval?\n",
      "Furthermore, when we compare the BERT type for the BERTCAT architecture, we see that DistilBERTCAT-T2 outperforms any single teacher model with twice and four times the layers on almost all metrics. For the BERTDOT architecture we also compared BERT- Base and DistilBERT, both as students, and here BERT-Base has\n",
      "The difference to previous results in Table 3 is that now we only use the score of a nearest neighbor search of all indexed passages, without re-ranking BM25. Because we no longer re-rank first-stage results, the pipeline overall becomes more efficient and less com- plex, however the chance of false positives becomes greater and\n",
      "Table 4: Dense retrieval results for both query sets, using a flat Faiss index without compression.\n",
      "Model\n",
      "Index Size\n",
      "Teacher\n",
      "TREC DL Passages 2019\n",
      "MSMARCO DEV\n",
      "nDCG@10 MRR@10 Recall@1K nDCG@10 MRR@10 Recall@1K\n",
      "Baselines BM25 BERT-BaseDOT ANCE [44] TCT-ColBERT [26] RocketQA [12]\n",
      "2 GB\n",
      "   \n",
      ".501 .648 .670 \n",
      ".689   \n",
      ".739  .720 \n",
      ".241   \n",
      ".194 .330 .335 .370\n",
      ".868 .959 .964 .979\n",
      "Our Dense Retrieval Student Models\n",
      "BERT-BaseDOT\n",
      "12.7 GB\n",
      " T1 T2\n",
      ".593 .631 .668\n",
      ".757 .771 .826\n",
      ".664 .702 .737\n",
      ".347 .358 .371\n",
      ".294 .304 .315\n",
      ".913 .931 .947\n",
      "DistilBERTDOT\n",
      "12.7 GB\n",
      " T1 T2\n",
      ".626 .687 .697\n",
      ".836 .818 .868\n",
      ".713 .749 .769\n",
      ".354 .379 .381\n",
      ".299 .321 .323\n",
      ".930 .954 .957\n",
      "less interpretable in a dense vector space retrieval. The ColBERT ar- chitecture also includes the possibility to conduct a dense retrieval, however at the expense of increasing the storage requirements of 2GB plain text to a 2TB index, which stopped us from conducting extensive experiments with ColBERT.\n",
      "We show nearest neighbor retrieval results of our BERTDOT mod- els (using both BERT-Base and DistilBERT encoders) and baselines for dense retrieval in Table 4. Training with a teacher ensemble is again more effective than training with a single teacher, which is still more effective than training the BERTDOT alone without teachers. Interestingly, DistilBERT outperforms BERT-Base across the board with half the Transformer layers. As we let the models train as long as they improved the early stopping set, it suggests, for the retrieval task we may not need more model capacity, which is a sure bet to improve results on the BERTCAT architecture.\n",
      "Our dense retrieval results are competitive with related meth- ods, even though they specifically train for the dense retrieval task. Our approach, while not specific to dense retrieval training is com- petitive with the more costly and complex approaches ANCE and TCT-ColBERT. On MSMARCO DEV MRR@10 we are at a slight disadvantage, however we outperform the models that also pub- lished TREC-DL19 results. RocketQA, the current state-of-the-art dense retrieval result on MSMARCO DEV requires a batch size of 4,000 and enormous computational resources, which are hardly comparable to our technique that only requires a batch size of 32 and can be trained on a single GPU.\n",
      "5.3 Closing the Efficiency-Effectiveness Gap We round off our results with a thorough look at the effects of knowledge distillation on the relation between effectiveness and efficiency in the re-ranking scenario. We measure the median query latency under the conditions that we have our cached document representation in memory, contextualize a single query, and com- puted the respective models interaction pattern for 1 query and 1000 documents in a single batch on a TITAN RTX GPU with 24GB of memory. The large GPU memory allows us to also compute the same batch size for BERTCAT, which for inference requires 16GB of total reserved GPU memory in the BERT-Base case. We measure the latency of the neural model in PyTorch inference mode (without\n",
      "0.64\n",
      "T2\n",
      "T2\n",
      "T2\n",
      "TK\n",
      "T1\n",
      "BERT-BaseCAT\n",
      "T2\n",
      "T1\n",
      "0.74\n",
      "0.66\n",
      "T1\n",
      "Query Latency (ms)\n",
      "101\n",
      "DistilBERTDOT\n",
      "0.70\n",
      "0.68\n",
      "103\n",
      "102\n",
      "0.72\n",
      "BERT-BaseDOT\n",
      "T2\n",
      "0.76nDCG@10\n",
      "T1\n",
      "T2\n",
      "T1\n",
      "T1\n",
      "ColBERT\n",
      "DistilBERTCAT\n",
      "PreTT\n",
      "Figure 3: Query latency vs. nDCG@10 on TREC19\n",
      "T1\n",
      "BERT-BaseDOT\n",
      "T1\n",
      "101\n",
      "ColBERT\n",
      "T2\n",
      "T2\n",
      "T2\n",
      "T1\n",
      "T2\n",
      "T2\n",
      "0.40MRR@10\n",
      "T2\n",
      "0.34\n",
      "102\n",
      "T1\n",
      "0.38\n",
      "PreTT\n",
      "T1\n",
      "DistilBERTCAT\n",
      "0.32\n",
      "0.36\n",
      "BERT-BaseCAT\n",
      "TK\n",
      "DistilBERTDOT\n",
      "T1\n",
      "Query Latency (ms)\n",
      "0.30\n",
      "103\n",
      "Figure 4: Query latency vs. MRR@10 on MSMARCO DEV\n",
      "accounting for pre-processing or disk access times, as those are highly dependent on the use of optimized inference libraries) to answer:\n",
      "RQ4 By how much does effective knowledge distillation shift the\n",
      "balance in the efficiency-effectiveness trade-off?\n",
      "In Figures 3 and 4, we plot the median query latency on the log-scaled x-axis versus the effectiveness on the y-axis. The teacher trained models are indicated with T1 and T2. The latency for differ- ent teachers does not change, as we do not change the architecture, only the weights of the models. The T1 teacher model BERTCAT is indicated with the red square. The TREC-DL19 results in Fig- ure 3 show how DistilBERTCAT, PreTT, and ColBERT not only close the gap to BERT-BaseCAT, but improve on the single instance BERT-BaseCAT results. The BERTDOT and TK models, while not reaching the effectiveness of the other models, are also improved over their baselines and are more efficient in terms of total runtime (TK) and index space (BERTDOT). The MSMARCO DEV results in Figure 4 differ from Figure 3 in DistilBERTCAT and PreTT outper- forming BERT-BaseCAT as well as the evaluated BERTDOT variants under-performing overall in comparison to TK and ColBERT.\n",
      "Even though in this work we measure the inference time on a GPU, we believe that the most efficient models  namely TK, ColBERT, and BERTDOT  allow for production CPU inference, as- suming the document collection has been pre-computed on GPUs. Furthermore, in a cascading search pipeline, one can hide most of the remaining computation complexity of the query contextualiza- tion during earlier stages.\n",
      "6 TEACHER ANALYSIS Finally, we analyse the distribution of our teacher score margins, to validate the intuition of using a teacher ensemble and we look at per-query nDCG changes for two models between teacher-trained instances and the baseline.\n",
      "6.1 Teacher Score Distribution Analysis To validate the use of an ensemble of teachers for RQ2, we analyze the output score margin distribution of our teacher models in Figure 5, to see if they bring diversity to the ensemble mix. This is the margin used in the Margin-MSE loss. We observe that the same BERTCAT architecture, differing only in the BERT language model used, shows three distinct score patterns. We view this as a good sign for the applicability of an ensemble of teachers, indicating that the different teachers have different viewpoints to offer. To ensemble our teacher models we computed a mean of their scores per example used for the knowledge distillation, to not introduce more complexity in the process.\n",
      "An interesting quirk of our Margin-MSE definition is the possi- bility to reverse orderings if the margin between a pair is negative. In Figure 5 we can see the reversal of the ordering of pairs in the distribution for the < 0 margin. It happens rarely and if a swap occurs the score difference is small. We investigated this issue by qualitatively analyzing a few dozen cases and found that the teacher models are most of the time correct in their determination to re- verse or equalize the margin. Because it only affects a few percent of the training data we retained those samples as well to not change the training data.\n",
      "5 BERT-LargeCAT\n",
      "Figure 5: Distribution of the margins between relevant and non-relevant documents of the three teacher models on MS MARCO-Passage training data\n",
      "DistilBERTDOT-T2 to DistilBERTDOT\n",
      "DistilBERTDOT-T1 to DistilBERTDOT\n",
      "0.2\n",
      "0.2\n",
      "0.2\n",
      "0.0\n",
      "0.4\n",
      "0.6\n",
      "0.6\n",
      "ColBERT-T2 to ColBERTnDCG@10 Change\n",
      "Sorted Queries\n",
      "ColBERT-T1 to ColBERT\n",
      "0.4\n",
      "0.0\n",
      "0.2\n",
      "Figure 6: A detailed comparison between T1 and T2 training ndcg@10 changes per query of the TREC-DL19 query set\n",
      "6.2 Per-Query Teacher Impact Analysis In addition to the aggregated results presented in Table 3, we now take a closer look at the impact of T1 and T2 teachers in a per-query analysis for ColBERT and DistilBERTDOT in Figure 6. We plot the differences in nDCG@10 per query on the TREC-DL19 set between the original training results and the T1 and T2 training respectively. A positive change means the T1/T2 trained model does better on this particular query. We sorted the queries by the T2 changes for both plots, and plotted the corresponding query results for T1 at\n",
      "the same position. Overall, the T1 & T2 training for both models roughly improves 60 % of queries and decreases results on 33 % with the rest of queries unchanged. Interestingly, the average change in each direction between the T1 and T2 training shows that T2 results become more extreme, as they improve more on average (DistilBERTDOT from T1 +10% to T2 +13%; ColBERT from T1 +6% to T2 +9%), but also decrease stronger on average (DistilBERTDOT from T1 6.8% to T2 7.2%; ColBERT from T1 4.3% to T2 7.8%). As we saw in Table 3 the aggregated results, still put T2 in front of T1 overall. However, we caution, that these stronger decreases show a small limitation of our knowledge distillation approach.\n",
      "7 RELATED WORK\n",
      "Efficient relevance models. Recent studies have investigated different approaches for improving the efficiency of relevance mod- els. Ji et al. [19] demonstrate that approximations of interaction- based neural ranking algorithms using kernels with locality-sensitive hashing accelerate the query-document interaction computation. In order to reduce the query processing latency, Mackenzie et al. [33] propose a static index pruning method when augmenting the inverted index with precomputed re-weighted terms [8]. Several approaches aim to improve the efficiency of transformer models with windowed self-attention [17], using locality-sensitive hashing [23], replacing the self-attention with a local windowed and global attention [2] or by combining an efficient transformer-kernel model with a conformer layer [35].\n",
      "Adapted training procedures. In order to tackle the challenge of a small annotated training set, Dehghani et al. [10] propose weak supervision controlled by full supervision to train a confident model. Subsequently they demonstrate the success of a semi-supervised student-teacher approach for an information retrieval task using weakly labelled data where the teacher has access to the high quality labels [9]. Examining different weak supervision sources, MacA- vaney et al. [32] show the beneficial use of headline - content pairs as pseudo-relevance judgements for weak supervision. Considering the success of weak supervision strategies for IR, Khattab and Za- haria [21] train ColBERT [21] for OpenQA with guided supervision by iteratively using ColBERT to extract positive and negative sam- ples as training data. Similarly Xiong et al. [44] construct negative samples from the approximate nearest neighbours to the positive sample during training and apply this adapted training procedure for dense retrieval training. Cohen et al. [6] demonstrate that the sampling policy for negative samples plays an important role in the stability of the training and the overall performance with respect to IR metrics. MacAvaney et al. [30] adapt the training procedure for answer ranking by reordering the training samples and shifting samples to the beginning which are estimated to be easy.\n",
      "Knowledge distillation. Large pretrained language models ad- vanced the state-of-the-art in natural language processing and in- formation retrieval, but the performance gains come with high com- putational cost. There are numerous advances in distilling these models to smaller models aiming for little effectiveness loss.\n",
      "Creating smaller variants of the general-purpose BERT mode, Jiao et al. [20] distill TinyBert and Sanh et al. [38] create DistilBERT\n",
      "and demonstrate how to distill BERT while maintaining the models accuracy for a variety of natural language understanding tasks.\n",
      "In the IR setting, Tang and Wang [40] distill sequential recom- mendation models for recommender systems with one teacher model. Vakili Tahami et al. [41] study the impact of knowledge distillation on BERT-based retrieval chatbots. Gao et al. [14] and Chen et al. [5] distilled different sizes of the same BERTCAT ar- chitecture and the TinyBert library [20]. As part of the PARADE document ranking model Li et al. [25] showed a similar BERTCAT to BERTCAT same-architecture knowledge distillation for different layer and dimension hyperparameters. A shortcoming of these dis- tillation approaches is that they are only applicable to the same architecture which restricts the retrieval model to full online in- ference of the BERTCAT model. Lu et al. [27] utilized knowledge distillation from BERTCAT to BERTDOT in the setting of keyword matching to select ads for sponsored search. They first showed, that a knowledge transfer from BERTCAT to BERTDOT is possible, albeit in a more restricted setting of keyword list matching in comparison to our fulltext ranking setting.\n",
      "8 CONCLUSION We proposed to use cross-architecture knowledge distillation to improve the effectiveness of query latency efficient neural pas- sage ranking models taught by the state-of-the-art full interaction BERTCAT model. Following our observation that different architec- tures converge to different scoring ranges, we proposed to optimize not the raw scores, but rather the margin between a pair of relevant and non-relevant passages with a Margin-MSE loss. We showed that this method outperforms a simple pointwise MSE loss. Further- more, we compared the performance of a single teacher model with an ensemble of large BERTCAT models and find that in most cases using an ensemble of teachers is beneficial in the passage retrieval task. Trained with a teacher ensemble, single instances of efficient models even outperform their single instance teacher models with much more parameters and interaction capacity. We observed a drastic shift in the effectiveness-efficiency trade-off of our evaluated models towards more effectiveness for efficient models. In addition to re-ranking models, we show our general distillation method to produce competitive effectiveness compared to specialized training techniques for the dual-encoder BERTDOT model in the nearest neighbor retrieval setting. We published our teacher training files, so the community can use them without significant changes to their setups. For future work we plan to combine our knowledge distilla- tion approach with other neural ranking training adaptations, such as curriculum learning or dynamic index sampling for end-to-end neural retrieval.\n",
      "REFERENCES [1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew Mcnamara, Bhaskar Mitra, and Tri Nguyen. 2016. MS MARCO : A Human Generated MAchine Reading COmprehension Dataset. In Proc. of NIPS.\n",
      "[2] Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-\n",
      "document transformer. arXiv preprint arXiv:2004.05150 (2020).\n",
      "[3] Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An\n",
      "overview. MSR-Tech Report (2010).\n",
      "[4] Wei-Cheng Chang, Felix X Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. 2020. Pre-training tasks for embedding-based large-scale retrieval. In Proc. of ICLR.\n",
      "[5] Xuanang Chen, Ben He, Kai Hui, Le Sun, and Yingfei Sun. 2020. Simplified Tiny- BERT: Knowledge Distillation for Document Retrieval. arXiv:cs.IR/2009.07531 [6] Daniel Cohen, Scott M. Jordan, and W. Bruce Croft. 2019. Learning a Better Negative Sampling Policy with Deep Neural Networks for Search. In Proc. of ICTIR.\n",
      "[7] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2019. Overview\n",
      "of the TREC 2019 deep learning track. In TREC.\n",
      "[8] Zhuyun Dai and Jamie Callan. 2020. Context-Aware Document Term Weighting\n",
      "for Ad-Hoc Search. In Proc. of WWW.\n",
      "[9] Mostafa Dehghani, Arash Mehrjou, Stephan Gouws, Jaap Kamps, and Bernhard\n",
      "Schlkopf. 2018. Fidelity-weighted learning. Proc. of ICLR (2018).\n",
      "[10] Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, and Jaap Kamps. 2017. Learn- ing to learn from weak supervision by full supervision. Proc. of NIPS Workshop on Meta-Learning (2017).\n",
      "[11] J. Devlin, M. Chang, K. Lee, and K. Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proc. of NAACL. [12] Yingqi Qu Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2020. RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering. arXiv preprint arXiv:2010.08191 (2020).\n",
      "[13] Luyu Gao, Zhuyun Dai, and Jamie Callan. 2020. EARL: Speedup Transformer- based Rankers with Pre-computed Representation. arXiv preprint arXiv:2004.13313 (2020).\n",
      "[14] Luyu Gao, Zhuyun Dai, and Jamie Callan. 2020. Understanding BERT Rankers\n",
      "Under Distillation. arXiv preprint arXiv:2007.11088 (2020).\n",
      "[15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in\n",
      "a neural network. arXiv preprint arXiv:1503.02531 (2015).\n",
      "[16] Sebastian Hofsttter and Allan Hanbury. 2019. Lets measure run time! Extending the IR replicability infrastructure to include performance aspects. In Proc. of OSIRRC.\n",
      "[17] Sebastian Hofsttter, Hamed Zamani, Bhaskar Mitra, Nick Craswell, and Allan Hanbury. 2020. Local Self-Attention over Long Text for Efficient Document Retrieval. In Proc. of SIGIR.\n",
      "[18] Sebastian Hofsttter, Markus Zlabinger, and Allan Hanbury. 2020. Interpretable & Time-Budget-Constrained Contextualization for Re-Ranking. In Proc. of ECAI. [19] Shiyu Ji, Jinjin Shao, and Tao Yang. 2019. Efficient Interaction-based Neural\n",
      "Ranking with Locality Sensitive Hashing. In Proc of. WWW.\n",
      "[20] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351 (2019).\n",
      "[21] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. In Proc. of SIGIR.\n",
      "[22] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\n",
      "mization. arXiv preprint arXiv:1412.6980 (2014).\n",
      "[23] Nikita Kitaev, ukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The Efficient\n",
      "Transformer. In Proc. of ICLR.\n",
      "[24] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942 (2019).\n",
      "[25] Canjia Li, Andrew Yates, Sean MacAvaney, Ben He, and Yingfei Sun. 2020. PA- RADE: Passage Representation Aggregation for Document Reranking. arXiv preprint arXiv:2008.09093 (2020).\n",
      "[26] Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2020. Distilling Dense Representations for Ranking using Tightly-Coupled Teachers. arXiv preprint arXiv:2010.11386 (2020).\n",
      "[27] Wenhao Lu, Jian Jiao, and Ruofei Zhang. 2020. TwinBERT: Distilling knowl- edge to twin-structured BERT models for efficient retrieval. arXiv preprint\n",
      "arXiv:2002.06275 (2020).\n",
      "[28] Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2020. Sparse, arXiv preprint\n",
      "Dense, and Attentional Representations for Text Retrieval. arXiv:2005.00181 (2020).\n",
      "[29] Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli Goharian, and Ophir Frieder. 2020. Efficient Document Re-Ranking for Trans- formers by Precomputing Term Representations. arXiv preprint arXiv:2004.14255 (2020).\n",
      "[30] Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli Goharian, and Ophir Frieder. 2020. Training Curricula for Open Domain Answer Re-Ranking. In Proc. of SIGIR.\n",
      "[31] Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019. CEDR:\n",
      "Contextualized Embeddings for Document Ranking. In Proc. of SIGIR.\n",
      "[32] Sean MacAvaney, Andrew Yates, Kai Hui, and Ophir Frieder. 2019. Content-Based\n",
      "Weak Supervision for Ad-Hoc Re-Ranking. In Proc. of SIGIR.\n",
      "[33] Joel Mackenzie, Zhuyun Dai, Luke Gallagher, and Jamie Callan. 2020. Efficiency\n",
      "implications of term weighting for passage retrieval. In Proc. of SIGIR.\n",
      "[34] Christopher D Manning, Hinrich Schtze, and Prabhakar Raghavan. 2008. Intro-\n",
      "duction to information retrieval. Cambridge university press.\n",
      "[35] Bhaskar Mitra, Sebastian Hofstatter, Hamed Zamani, and Nick Craswell. 2020. Conformer-Kernel with Query Term Independence for Document Retrieval. arXiv preprint arXiv:2007.10434 (2020).\n",
      "[36] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.\n",
      "arXiv preprint arXiv:1901.04085 (2019).\n",
      "[37] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in PyTorch. In Proc. of NIPS-W.\n",
      "[38] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis- tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 (2019).\n",
      "[39] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient knowledge distilla- tion for bert model compression. arXiv preprint arXiv:1908.09355 (2019). [40] Jiaxi Tang and Ke Wang. 2018. Ranking distillation: Learning compact ranking\n",
      "models with high performance for recommender system. In Proc. of SIGKDD.\n",
      "[41] Amir Vakili Tahami, Kamyar Ghajar, and Azadeh Shakery. 2020. Distilling\n",
      "Knowledge for Fast Retrieval-based Chat-bots. In Proc. of SIGIR.\n",
      "[42] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz, et al. 2019. HuggingFaces Transformers: State-of-the-art Natural Language Processing. ArXiv (2019), arXiv1910.\n",
      "[43] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-End Neural Ad-hoc Ranking with Kernel Pooling. In Proc. of SIGIR. [44] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate Nearest Neigh- bor Negative Contrastive Learning for Dense Text Retrieval. arXiv preprint arXiv:2007.00808 (2020).\n",
      "[45] Ming Yan, Chenliang Li, et al. 2020. IDST at TREC 2019 Deep Learning Track: Deep Cascade Ranking with Generation-based Document Expansion and Pre- trained Language Modeling. In TREC.\n",
      "[46] Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini: Enabling the use of Lucene\n",
      "for information retrieval research. In Proc. of SIGIR.\n",
      "[47] Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang, and Jimmy Lin. 2019. Cross-domain modeling of sentence-level evidence for document retrieval. In Proc. of EMNLP-IJCNLP.\n",
      "[48] Yuyu Zhang, Ping Nie, Xiubo Geng, Arun Ramamurthy, Le Song, and Daxin Jiang. 2020. DC-BERT: Decoupling Question and Document for Efficient Contextual Encoding. arXiv preprint arXiv:2002.12591 (2020).\n",
      "######## Attention over pre-trained Sentence Embeddings for Long Document Classification.pdf ######## \n",
      "\n",
      "\n",
      "3 2 0 2\n",
      "l u J\n",
      "8 1\n",
      "] L C . s c [\n",
      "1 v 4 8 0 9 0 . 7 0 3 2 : v i X r a\n",
      "Attention over pre-trained Sentence Embeddings for Long Document Classification\n",
      "Amine Abdaoui1,2,*, Sourav Dutta1\n",
      "1Huawei Ireland Research Center, Dublin, Ireland 2Oracle, Paris, France\n",
      "Abstract Despite being the current de-facto models in most NLP tasks, transformers are often limited to short sequences due to their quadratic attention complexity on the number of tokens. Several attempts to address this issue were studied, either by reducing the cost of the self-attention computation or by modeling smaller sequences and combining them through a recurrence mechanism or using a new transformer model. In this paper, we suggest to take advantage of pre-trained sentence transformers to start from semantically meaningful embeddings of the individual sentences, and then combine them through a small attention layer that scales linearly with the document length. We report the results obtained by this simple architecture on three standard document classification datasets. When compared with the current state-of-the-art models using standard fine-tuning, the studied method obtains competitive results (even if there is no clear best model in this configuration). We also showcase that the studied architecture obtains better results when freezing the underlying transformers. A configuration that is useful when we need to avoid complete fine-tuning (e.g. when the same frozen transformer is shared by different applications). Finally, two additional experiments are provided to further evaluate the relevancy of the studied architecture over simpler baselines.\n",
      "Keywords Transformers, Sentence Embedddings, Attention, Long Document Classification.\n",
      "1. Introduction\n",
      "The Transformer model [1] is now established as the standard architecture in Natural Language Processing (NLP). Several variants of the original model achieved state-of-the-art results in many tasks [2, 3, 4] including document classification. In addition to their accurate results, transformers are also efficient when compared to recurrent neural network encoders. However, this efficiency drops significantly on long sequences. Indeed, transformers compute  *  self-attention matrices to get the contextualized representations. Therefore, the memory and computational requirements grow-up quadratically with the number of tokens . For this reason, most transformer-based models are limited to a fixed number of tokens (usually 512 tokens).\n",
      "ReNeuIR23: Workshop on Reaching Efficiency in Neural Information Retrieval *This co-author is currently employed by Oracle but this work was conducted when he was at the Huawei Ireland Research Center. $ amin.abdaoui@oracle.com (A. Abdaoui); surav.dutta2@huawei.com (S. Dutta) (cid:26) 0000-0002-6160-8461 (A. Abdaoui); 0000-0002-8934-9166 (S. Dutta)\n",
      "CEURWorkshopProceedingshttp://ceur-ws.orgISSN 1613-0073\n",
      " 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CEUR Workshop Proceedings (CEUR-WS.org)\n",
      "To address this limitation, several attempts were made to improve the transformer efficiency on longer sequences. A first family of methods tries to simplify the self-attention complexity by reducing the number of computed weights. Concretely, instead of letting each token attend to every other token in the sequence, these methods restrict the computation of the attention weights to a small number of locations [5, 6]. Another popular approach consists in splitting the long input into smaller chunks that can be modeled more efficiently with a transformer. Then, the obtained representations can be combined using a recurrent neural network or another document-level transformer [7, 8]. Finally, instead of combining the outputs of different chunks using a new model, [9] and [4] implemented a caching mechanism that allows the first tokens of chunk  to have access to the hidden states of the last tokens of chunk   1.\n",
      "In this paper, we suggest to take advantage of pre-trained sentence transformers to get meaning- ful sentence representations without the need of any further pre-training [10]. The availability and variety of these models allow to easily adapt our framework to different domains and languages1. Based on these sentence representations, we evaluate the use of a small attention layer to form a document representation by giving higher weights to more important sentences. Note that we do not compute full self-attention matrices between all sentence pairs but only attention weights between the unique document representation and the different sentence embeddings. Indeed, we believe that sentence representations are less sensitive to external context than token embeddings. Similar architectures that also use linear weighted aggregations were evaluated on other tasks [11, 12].\n",
      "To evaluate these assumptions in the case of long document classification, we compare our proposed architecture with the current state-of-the-art models on three standard datasets. To our knowledge, this is the first detailed evaluation of these models on the same datasets. In addition to complete fine-tuning, we include a setting where the underlying transformers are frozen. Such scenario might be useful when the same transformer is shared by different applications (each application trains only its own task-specific layers).\n",
      "2. Related Work\n",
      "Most of the work that tried to adapt transformers to long documents were evaluated on language generation [13, 9, 6]. In this section, we will mainly focus on methods than can be used in language understanding tasks such as classification.\n",
      "The easiest way to deal with long sequences is to truncate them at the maximum sequence length supported by the model. Usually, the first 512 tokens are used and the following ones are just thrown away. Therefore, the first baseline in this paper will be simple truncation using the Roberta model [3], which is a widely used transformer for Natural Language Understanding. Furthermore, Roberta was used to initialize two other models that are also included in our experiments.\n",
      "A more sophisticated approach uses sparse self-attention matrices to reduce the transformer complexity. Instead of computing all the  *  weights in each matrix, the idea is to compute only the ones that convey important relationships. For example, Longformer [5] combines a windowed local attention for all tokens with a global attention for few important tokens. On\n",
      "1https://github.com/UKPLab/sentence-transformers\n",
      "the one hand, the authors proposed to compute attention weights between each token and all its neighbors that are included in a fixed window. On the other hand, they allow important tokens (e.g. [CLS]) to attend to the whole sequence of  tokens. Thanks to these optimisations, the authors were able to pre-train the Longformer model, which is able to handle 4096 tokens, starting from Roberta weights. Another work suggested to choose the computed attention weights dynamically based on the content [6]. However, the model has been designed for character-level language generation as most of the other sparse attention methods. Therefore, we will only include Longformer in our evaluations to represent this family of methods.\n",
      "Another popular research direction is to use a hierarchical architecture in order to reduce the cost of the self-attention computation. Instead of applying one transformer to the whole sequence, the idea is to stack multiple models that handle a smaller number of inputs. Since transformers complexity is (2), applying multiple transformers to smaller sequences is better than applying one transformer to the whole sequence. Several studies that used multiple levels of transformers or a recurrent neural network on top of transformers have been proposed [7, 14]. However, most of them have not been shared publicly with the community. SMITH, which is able to handle 2048 tokens, is the one of the rare pre-trained hierarchical models that is available online [8]. The proposed architecture is composed of two levels of abstraction: a sentence-level and a document-level. Each level uses a small transformer that has 4 and 3 layers respectively, 4 attention heads and 256 hidden dimensions. Therefore, the resulting model has less parameters than all the other models studied here which follow the common base architectures (12 layers, 12 attention-heads and 768 hidden dimensions). It was pre-trained using the usual masked word prediction and a novel masked sentence prediction task. Then, it was fine-tuned for document matching using a siamese architecture. In this paper, SMITH will be considered as a baseline in our experiments despite of its small size. To our knowledge, this is the first evaluation of SMITH on the document classification datasets considered.\n",
      "Finally, [9] proposed TransformerXL which is able to model an unlimited number of tokens. The proposed auto-regressive model is also applied to smaller chunks extracted from the original long documents. However, the modeling is not conducted independently on each chunk. At each time step, the previous hidden states are reused to compute the current ones introducing a sort of memory that propagates across the different segments. Moreover, the usual absolute position embeddings were replaced by relative positional encoding in order to avoid confusion on token positions when handling different segments. The same authors also pre-trained XLNet [9] in order to improve auto-regressive models in NLU tasks. In addition to handling an unlimited sequence length (thanks to the caching mechanism and relative positional encoding proposed in TransformerXL), the authors used permutation language modeling to capture bidirectional context when pre-training the model. For all these reasons, we will include XLNet as a baseline in our experiments.\n",
      "In this paper, we will compare the above mentioned baselines with an attention-based architecture that relies on pre-trained sentence transformers. These models are usually trained using a siamese architecture on sentence pair datasets to derive semantically meaningful sentence representations [10]. To our knowledge, this is the first attempt to use pre-trained sentence transformers for handling long documents.\n",
      "3. Methods\n",
      "In this section, we will detail the studied Attention over Sentence Embeddings (AoSE) architecture and compare its complexity and size with existing baselines.\n",
      "3.1. AoSE Architecture\n",
      "First, long documents are segmented into sentences using common sentence separators (full stop, line break, etc.). We define a minimum and a maximum number of tokens to avoid generating very small and very long segments that do not correspond to real sentences. Then, a sentence transformer will be used to map each sentence to a fixed dense representation . Relying on such pre-trained models that are already geared towards producing meaningful sentence embeddings is certainly an important advantage. After that, we use an attention layer to combine the normalized sentence embeddings  while giving higher weights to important sentences [15]. To calculate these weights , we rely on a small neural network  and a trainable context vector  that is equivalent to the query in the Transformers self-attention definition [1].\n",
      " = (   + )\n",
      "exp( =1 exp( The document representation  is then computed using a weighted sum of the different\n",
      "  )\n",
      " =\n",
      "\n",
      "  )\n",
      "sentence embeddings .\n",
      " =\n",
      " \n",
      "  \n",
      "=1\n",
      "Finally, a dense layer can be added on top of the document embedding  in order to perform\n",
      "classification. All these steps are presented in Figure 1 below.\n",
      "3.2. Model Complexity\n",
      "Lets define the sequence length  as the product of the number of sentences  and the length of one sentence . The complexity of a vanilla transformer (e.g. Roberta) is therefore (2  2). The sentence transformer of SMITH computes full self-attention between all tokens in each sentence, while the document transformer applies a second full self-attention computation between all sentences. Thus, the complexity of the whole SMITH encoder is (  2 + 2).\n",
      "Longformer computes global attention for  important tokens and local attention for the remaining ones. Let  be the window size for local attention tokens. The Longformers complexity is therefore (     + (    )  ).\n",
      "XLNet computes full self-attention for each chunk. Let  be the maximum length of one XLNet segment (chunk). Even if all the previous hidden states are cached and reused, a given\n",
      "(1)\n",
      "(2)\n",
      "(3)\n",
      "Sentence Transformer\n",
      "1\n",
      "2\n",
      "x\n",
      "x\n",
      "x\n",
      "Class\n",
      "\n",
      "Sentence Transformer\n",
      "Sentence Transformer\n",
      "Sentence 1\n",
      "+\n",
      "\n",
      "\n",
      "2\n",
      "\n",
      "Context (query)Document embedding\n",
      "Sentence n\n",
      "1\n",
      "Document\n",
      "Sentence 2\n",
      "MLP\n",
      "3\n",
      "Figure 1: The proposed Attention over Sentence Embeddings (AoSE) architecture.\n",
      "Table 1 Complexity of the different models ( is the number of sentences,  is the average number of tokens per sentence,  is the number of global attention tokens,  is the window size for local attention tokens, and c is the length of one XLNet segment).\n",
      "Model Roberta SMITH Longformer XLNet AoSE\n",
      "Complexity (2  2) (  2 + 2) (     + (    )  ) (    ) (  2 + )\n",
      "token cant attend to more than  locations. Therefore, the complexity of one chunk is (2) and the number of chunks is   /. Consequently, the complexity of XLNet is (    ). The sentence transformer of our proposed architecture is equivalent to the one used in SMITH, but since our document-level attention is linear with the number of sentences, the complexity of our architecture is (  2 + ).\n",
      "Table 2 Different size comparisons of the evaluated models.\n",
      "Model\n",
      "Roberta SMITH Longformer XLNet AoSE\n",
      "Disk size 478 MB 47 MB 570 MB 445 MB 480 MB\n",
      "#Params (million) 125 m 12 m 149 m 117 m 126 m\n",
      "Vocab. size 50.265 30.522 50.265 32.000 50.265\n",
      "#Layers / #Heads 12 / 12 4+3 / 4 12 / 12 12 / 12 12 / 12\n",
      "Hidden Max input\n",
      "dim. 768 256 768 768 768\n",
      "#tokens 512 2.048 4.096 unlimited unlimited\n",
      "3.3. Model Size\n",
      "Due to hardware limitations, we decided to work with the base versions of Roberta, Longformer and XLNet even if large versions were also available. Similarly, we have chosen a base sentence transformer2 in our AoSE architecture. The chosen sentence transformer was initialized with Roberta base. Therefore, these two models share the exact same number of parameters. SMITH is the only exception as its only available version is much smaller in size. Table 2 presents several size-related measurements for all the models evaluated in this work.\n",
      "We can also notice that our AoSE architecture has slightly more parameters than Roberta. As mentioned before, our architecture is composed of a sentence transformer (that has the same size as Roberta) and a small attention layer that has less than 1 million parameters. Most of them are located in the  matrix that has a shape of 768  768. Therefore, our proposed architecture does not add a lot of parameters when compared to a standard transformer.\n",
      "4. Experiments\n",
      "In this section, we will assess the performance of our architecture along with the selected baselines for long document classification.\n",
      "4.1. Datasets\n",
      "Three classification datasets (of several thousands of documents each) were chosen to conduct our experiments. The first one is the widely used IMDB dataset [16]. We used the binary version3 that distinguishes positive and negative movie reviews. The second one is MIND [17], a large-scale dataset for news recommendation. We used the topic classification task from this dataset4 and discarded a couple of topics that have less than 3 documents. The third one is the 20 News Groups dataset [18]. We used the cleaned version5 that do not contain headers, signatures, and quotations. Table 3 below shows several statistics related to the number of documents, the length of these documents and the number of classes for each dataset. The only\n",
      "2https://huggingface.co/sentence-transformers/nli-roberta-base-v2 3https://huggingface.co/datasets/imdb 4https://msnews.github.io 5https://huggingface.co/datasets/SetFit/20_newsgroups\n",
      "Table 3 Statistics of the different datasets: (i) the total number of documents, (ii) the number of long documents (having more than 512 tokens), (iii) the average number of tokens per document, (iv) the maximum number of tokens per document, and (v) the number of labels. The number of tokens were computed using the roberta tokenizer.\n",
      "Dataset\n",
      "IMDB\n",
      "MIND\n",
      "20 News Groups\n",
      "Split\n",
      "train test train test train test\n",
      "#Docs all 25.000 25.000 101.523 28.275 11.314 7.532\n",
      "#Docs >512 7.729 3.537 48.093 13.160 1.245 784\n",
      "Avg #Tokens 323 314 696 651 398 372\n",
      "Max #Tokens 3.240 3.257 51.662 28.287 49.561 132.115\n",
      "#Labels\n",
      "2 2 15 15 20 20\n",
      "additional preprocessing step that was applied to these datasets consisted in removing HTML tags.\n",
      "4.2. Experimental Settings Even if XLNet and AoSE can theoretically handle sequences of unlimited length6, we had to set a maximum sequence length to each one due to practical hardware restrictions. Indeed, we were not able to fine-tune XLNet on very long sequences using our 2  16 GB GPUs (even with a batch size of 1). Therefore, we decided to set the maximum sequence length of XLNet to 4096 tokens in our experiments (which covers all IMDB and more than 99% of MIND and 20 News Groups). In the case our AoSE model, we were able to input up to 8192 tokens which covers almost all the documents included in the three datasets.\n",
      "We also decided to perform our experiments in two different settings. In the first one, we train all the parameters of every architecture which correspond to standard fine-tuning. In the second setting, we decided to freeze the weights of the underlying transformers. In this case, the different transformers are applied once to produce frozen representations. Then, the training will only happen on the top level parameters of the different architectures. This means that we will only train classifiers for all the baselines, and the classifier along with the attention layer of our AoSE architecture. Training our linear attention layer do not take more time than training the classifier itself.\n",
      "Regarding the other experimental settings, we set the minimum number of tokens per sentence for our AoSE system to 5 and the maximum value to 250. The document representation in the frozen setting is average pooling as it allowed us to obtain better results for all models. When fine-tuning, we use the default pooling strategy implemented by each model. For all models and all datasets, we set the learning rate to 25 and the batch size to 16. When the memory of our GPUs is exceeded, we reduce the batch size but use gradient accumulation to simulate the same parameters update as with a batch size of 16. When freezing the transformers, we train all models for 50 epochs on each dataset. When fine-tuning, we train all models for 20 epochs on\n",
      "6There is no structural limitation caused by the model definition (for example, the size of the position embeddings matrix).\n",
      "Table 4 Results on IMDB.\n",
      "Model\n",
      "Roberta SMITH Longformer XLNet AoSE\n",
      "Max seq. length 512 2048 4096 4096 8192\n",
      "Time (hh:mm) 05:40 04:28 22:50 22:20 15:50\n",
      "Fine-tuning\n",
      "Acc. all 95.2 91.0 95.6 95.6 95.7\n",
      "Acc. <=512 >512 93.0 90.8 95.4 95.7 95.8\n",
      "Acc.\n",
      "95.6 91.0 95.6 95.5 95.7\n",
      "Time (hh:mm) 00:12 00:29 00:30 00:29 00:35\n",
      "Freezing Acc. all 91.8 74.0 92.0 92.2 93.2\n",
      "Acc. <=512 >512 88.4 73.0 91.2 91.8 93.1\n",
      "Acc.\n",
      "92.2 74.2 92.0 92.2 93.2\n",
      "Table 5 Results on MIND.\n",
      "Model\n",
      "Roberta SMITH Longformer XLNet AoSE\n",
      "Max seq. length 512 2048 4096 4096 8192\n",
      "Time (hh:mm) 09:20 13:30 66:45 81:58 83:10\n",
      "Fine-tuning\n",
      "Acc. all 83.1 80.8 84.1 83.4 83.5\n",
      "Acc. <=512 >512 85.8 84.8 88.0 87.3 87.5\n",
      "Acc.\n",
      "80.7 77.4 80.7 79.8 79.8\n",
      "Time (hh:mm) 00:45 00:52 02:05 04:12 04:58\n",
      "Freezing Acc. all 77.4 76.0 77.7 78.3 79.1\n",
      "Acc. <=512 >512 81.6 81.1 82.3 82.9 83.9\n",
      "Acc.\n",
      "73.8 71.6 73.7 74.2 75.0\n",
      "IMDB and News Groups, and for 10 epochs on MIND.\n",
      "4.3. Evaluations Tables 4, 5 and 6 present the results obtained by the different models on each dataset7.Overall, we can observe that Longformer, XLNet and AoSE obtain better accuracies on long documents when compared with the remaining baselines. When fine-tuning the transformers, there is no clear best model between them across the three datasets, which joins the conclusions drawn in [19]. However, when freezing the transformers, our AoSE model obtains systematically the best results. We believe that this setting is useful for applications that use the same underlying transformer as encoder for multiple tasks or s imply for applications that cannot afford expensive training. Finally, being much smaller than the other models, SMITH obtains the worst accuracies in both settings across all the datasets.\n",
      "Regarding the training speed, we can observe that the frozen setting reduces drastically the training time. Since the Max seq. length differs from one model to the other, comparing their training times is not straightforward. However, we can use the IMDB dataset for a fair comparison between Longformer, XLNet and AoSE as all IMDB documents can be modeled entirely without any truncation by these three models8. In this case, AoSE is faster than the two other models in the fine-tuning setting, but slightly slower in the frozen setting.\n",
      "7For more information about the Max seq. length mentioned in these tables, see subsection 4.2. 8the longest IMDB document has less than 4096 tokens.\n",
      "Table 6 Results on 20 News Groups.\n",
      "Model\n",
      "Roberta SMITH Longformer XLNet AoSE\n",
      "Max seq. length 512 2048 4096 4096 8192\n",
      "Time (hh:mm) 02:15 02:30 10:20 13:36 14:30\n",
      "Fine-tuning\n",
      "Acc. all 72.5 60.0 72.7 72.8 72.7\n",
      "Acc. <=512 >512 83.2 74.5 84.3 84.2 83.9\n",
      "Acc.\n",
      "71.5 58.6 71.4 71.7 71.5\n",
      "Time (hh:mm) 00:05 00:07 00:10 00:17 00:25\n",
      "Freezing Acc. all 63.7 56.4 64.1 65.3 66.0\n",
      "Acc. <=512 >512 75.0 71.8 77.7 79.5 79.9\n",
      "Acc.\n",
      "62.6 55.0 62.8 63.8 64.4\n",
      "Table 7 Ablation study conducted on IMDB: (i) S-Roberta refers to the chosen sentence transformer used alone and applied to the whole sequence (truncated after 512 tokens); (ii) AoSE-xxx refers to the application of the proposed architecture that also uses S-Roberta (input sequences are truncated after xxx tokens).\n",
      "Model\n",
      "S-Roberta AoSE-512 AoSE-1024 AoSE-2048 AoSE-4096\n",
      "Max sequence length 512 512 1024 2048 4096\n",
      "Acc. all 95.3 95.4 95.7 95.7 95.7\n",
      "Fine-tuning Acc. Acc. <=512 >512 92.8 93.0 95.7 95.8 95.8\n",
      "95.6 95.7 95.7 95.7 95.7\n",
      "Acc. all 92.2 92.6 93.0 93.1 93.2\n",
      "Freezing Acc. Acc. <=512 >512 89.1 89.9 92.5 92.9 93.1\n",
      "92.7 93.0 93.1 93.1 93.2\n",
      "4.4. Ablation Study\n",
      "We conduct an ablation study to further investigate the relevancy of the studied architecture over simpler baselines. We compare the results obtained by the selected sentence transformer alone (S-Roberta) with different versions of our AoSE architecture that use the same sentence transformer as their first component. The new AoSE versions are fed with sequences that are truncated after 512, 1024, 2048 and 4096 tokens respectively.Table 7 shows the obtained results on the IMDB dataset. It appears that the AoSE architecture is relevant and benefits from increasing the sequence length especially on long documents (that have more than 512 tokens). We also observe a slight improvement on short documents (having less than 512 tokens) in the frozen setting, which may be explained by a better attention layer after training on the additional sentences that appear at the end of long documents.\n",
      "4.5. Impact of the chosen sentence transformers\n",
      "Finally, we evaluate the impact of the chosen sentence transformer on the final results. Table 8 below shows the results obtained with three different sentence transformers in the same two settings presented earlier (fine-tuning and frozen). In addition to the already evaluated S-Roberta, two other sentence transformers have been included here: S-BERT9 and S-MPNet10.\n",
      "9https://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens 10https://huggingface.co/sentence-transformers/paraphrase-mpnet-base-v2\n",
      "Table 8 Results of different sentence transformers used either alone and inside our architecture on the IMDB dataset. Ao(S-Roberta) refers to attention over S-Roberta, the same model used in our previous experi- ments. Ao(S-BERT) refers to attention over S-BERT. Ao(S-MPNet) refers to attention over S-MPNet.\n",
      "Model\n",
      "S-Roberta Ao(S-Roberta) S-BERT Ao(S-BERT) S-MPNet Ao(S-MPNet)\n",
      "Max sequence length 512 8192 512 8192 512 8192\n",
      "Acc. all 95.3 95.7 94.0 94.8 95.2 95.6\n",
      "Fine-tuning Acc. Acc. <=512 >512 92.8 95.8 86.2 94.6 93.3 96.0\n",
      "95.6 95.7 94.5 94.8 95.5 95.5\n",
      "Acc. all 92.2 93.2 87.0 90.7 91.7 93.3\n",
      "Freezing Acc. Acc. <=512 >512 89.1 93.1 82.1 90.4 88.3 92.8\n",
      "92.7 93.2 87.9 90.7 92.2 93.3\n",
      "Again, the IMDB dataset is used to conduct these experiments.\n",
      "Each model is first applied directly to the whole sequence (truncated after 512 tokens)11. Then, it is used inside the studied architecture that is able to handle up to 8192 tokens. Overall, it appears that S-BERT obtains lower results than S-Roberta and S-MPNet either when used alone or inside our architecture. Therefore, we can say that the choice of the sentence transformer has an important impact on the final results. But more importantly, we observe that using the same models inside our AoSE architecture allow to improve the results of all models. This improvement is observable in both settings (fine-tuning and freezing), regardless of the underlying sentence transformer.\n",
      "5. Conclusion\n",
      "In this paper, we investigated the relevancy of pre-trained sentence transformers for long document classification. To our knowledge this is the first time these pre-trained models are used to handle long documents. To do so, we combine the sentence representations using a trainable attention layer to give high weights to important sentences. We have shown that this simple method is competitive when compared with current state-of-the-art models in the standard fine-tuning mode. We also considered another mode where the underlying transformers are frozen, which allows to speed-up the training and to share the same underlying transformer between different applications. In this case, the AoSE architecture obtains better results. Additional experiments have shown an improvement over the direct application of the same sentence transformers. Finally, relying on pre-trained sentence transformers allows to easily extend our architecture to different domains and languages. For example, we can simply replace the English sentence transformer used in this paper with a multilingual one12 to handle multilingual texts, whereas XLNet and Longformer need to be pre-trained again on multilingual datasets.\n",
      "11Initial evaluations have shown that applying sentence transformers directly to the whole document gives better\n",
      "results than applying them to each sentence and then averaging the different embeddings.\n",
      "12For example: https://huggingface.co/sentence-transformers/stsb-xlm-r-multilingual\n",
      "References\n",
      "[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, I. Polosukhin, Attention is all you need, in: I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, R. Garnett (Eds.), Advances in Neural Information Processing Systems, volume 30, Curran Associates, Inc., 2017.\n",
      "[2] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-training of deep bidirectional in: Proceedings of the 2019 Conference of transformers for language understanding, the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Association for Computational Linguistics, 2019, pp. 41714186. doi:10.18653/v1/N19-1423.\n",
      "[3] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, V. Stoyanov, Roberta: A robustly optimized bert pretraining approach, arXiv preprint arXiv:1907.11692 (2019).\n",
      "[4] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, Q. V. Le, Xlnet: Generalized autoregressive pretraining for language understanding, Advances in neural information processing systems 32 (2019).\n",
      "[5] I. Beltagy, M. E. Peters, A. Cohan, Longformer: The long-document transformer, arXiv\n",
      "preprint arXiv:2004.05150 (2020).\n",
      "[6] A. Roy, M. Saffar, A. Vaswani, D. Grangier, Efficient content-based sparse attention with routing transformers, Transactions of the Association for Computational Linguistics 9 (2021) 5368.\n",
      "[7] R. Pappagari, P. Zelasko, J. Villalba, Y. Carmiel, N. Dehak, Hierarchical transformers for long document classification, in: IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), volume https://ieeexplore.ieee.org/abstract/document/9003958, 2019, pp. 838844. doi:10.1109/ASRU46091.2019.9003958.\n",
      "[8] L. Yang, M. Zhang, C. Li, M. Bendersky, M. Najork, Beyond 512 tokens: Siamese multi-depth transformer-based hierarchical encoder for long-form document matching, in: Proceedings of the 29th ACM International Conference on Information & Knowledge Management, 2020, pp. 17251734.\n",
      "[9] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le, R. Salakhutdinov, Transformer-XL: Attentive language models beyond a fixed-length context, in: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics, Florence, Italy, 2019, pp. 29782988. doi:10.18653/v1/P19-1285.\n",
      "[10] N. Reimers, I. Gurevych, Sentence-bert: Sentence embeddings using siamese bert-networks, in: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro- cessing, Association for Computational Linguistics, 2019.\n",
      "[11] C. Li, A. Yates, S. MacAvaney, B. He, Y. Sun, Parade: Passage representation aggregation\n",
      "for document reranking, arXiv preprint arXiv:2008.09093 (2020).\n",
      "[12] S. Althammer, S. Hofsttter, M. Sertkan, S. Verberne, A. Hanbury, Parm: A paragraph aggregation retrieval model for dense document-to-document retrieval, in: Advances in Information Retrieval: 44th European Conference on IR Research, ECIR 2022, Stavanger, Norway, April 1014, 2022, Proceedings, Part I, Springer, 2022, pp. 1934.\n",
      "[13] N. Kitaev, L. Kaiser, A. Levskaya, Reformer: The efficient transformer, in: International\n",
      "Conference on Learning Representations, 2019.\n",
      "[14] C. Wu, F. Wu, T. Qi, Y. Huang, Hi-transformer: Hierarchical interactive transformer for efficient and effective long document modeling, in: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), 2021, pp. 848853. [15] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, E. Hovy, Hierarchical attention networks for document classification, in: Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies, 2016, pp. 14801489.\n",
      "[16] A. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, C. Potts, Learning word vectors for in: Proceedings of the 49th annual meeting of the association for\n",
      "sentiment analysis, computational linguistics: Human language technologies, 2011, pp. 142150.\n",
      "[17] F. Wu, Y. Qiao, J.-H. Chen, C. Wu, T. Qi, J. Lian, D. Liu, X. Xie, J. Gao, W. Wu, et al., Mind: A large-scale dataset for news recommendation, in: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 35973606.\n",
      "[18] K. Lang, Newsweeder: Learning to filter netnews, in: Machine Learning Proceedings 1995,\n",
      "Elsevier, 1995, pp. 331339.\n",
      "[19] H. Park, Y. Vyas, K. Shah, Efficient classification of long documents using transformers, in: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Association for Computational Linguistics, Dublin, Ireland, 2022, pp. 702709. doi:10.18653/v1/2022.acl-short.79.\n"
     ]
    }
   ],
   "source": [
    "for filename, file_content in files.items():\n",
    "    print(f\"######## {filename} ######## \\n\")\n",
    "    print(file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abde92e",
   "metadata": {},
   "source": [
    "# Reading pdf with fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f225adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c9e739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = dict()\n",
    "\n",
    "for file in files_path.glob(\"*.pdf\"):\n",
    "    text = \"\"\n",
    "    doc = fitz.open(file)\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    \n",
    "    files[file.name] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "697c54a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## MTEB - Massive Text Embedding Benchmark.pdf ######## \n",
      "\n",
      "MTEB: Massive Text Embedding Benchmark\n",
      "Niklas Muennighoff1, Nouamane Tazi1, Loïc Magne1, Nils Reimers2*\n",
      "1Hugging Face\n",
      "2cohere.ai\n",
      "1firstname@hf.co\n",
      "2info@nils-reimers.de\n",
      "Abstract\n",
      "Text embeddings are commonly evaluated on\n",
      "a small set of datasets from a single task not\n",
      "covering their possible applications to other\n",
      "tasks. It is unclear whether state-of-the-art em-\n",
      "beddings on semantic textual similarity (STS)\n",
      "can be equally well applied to other tasks like\n",
      "clustering or reranking. This makes progress\n",
      "in the ﬁeld difﬁcult to track, as various models\n",
      "are constantly being proposed without proper\n",
      "evaluation. To solve this problem, we intro-\n",
      "duce the Massive Text Embedding Benchmark\n",
      "(MTEB). MTEB spans 8 embedding tasks cov-\n",
      "ering a total of 58 datasets and 112 languages.\n",
      "Through the benchmarking of 33 models on\n",
      "MTEB, we establish the most comprehensive\n",
      "benchmark of text embeddings to date.\n",
      "We\n",
      "ﬁnd that no particular text embedding method\n",
      "dominates across all tasks. This suggests that\n",
      "the ﬁeld has yet to converge on a universal text\n",
      "embedding method and scale it up sufﬁciently\n",
      "to provide state-of-the-art results on all embed-\n",
      "ding tasks.\n",
      "MTEB comes with open-source\n",
      "code and a public leaderboard at https:\n",
      "//github.com/embeddings-benchm\n",
      "ark/mteb.\n",
      "1\n",
      "Introduction\n",
      "Natural language embeddings power a variety of\n",
      "use cases from clustering and topic representa-\n",
      "tion (Aggarwal and Zhai, 2012; Angelov, 2020)\n",
      "to search systems and text mining (Huang et al.,\n",
      "2020; Zhu et al., 2021; Nayak, 2019) to feature\n",
      "representations for downstream models (Saharia\n",
      "et al., 2022; Borgeaud et al., 2022). Using gener-\n",
      "ative language models or cross-encoders for these\n",
      "applications is often intractable, as they may re-\n",
      "quire exponentially more computations (Reimers\n",
      "and Gurevych, 2019).\n",
      "However, the evaluation regime of current text\n",
      "embedding models rarely covers the breadth of\n",
      "*Most of the work done while at Hugging Face. Corre-\n",
      "spondence to n.muennighoff@gmail.com.\n",
      "their possible use cases.\n",
      "For example, Sim-\n",
      "CSE (Gao et al., 2021b) or SBERT (Reimers and\n",
      "Gurevych, 2019) solely evaluate on STS and clas-\n",
      "siﬁcation tasks, leaving open questions about the\n",
      "transferability of the embedding models to search\n",
      "or clustering tasks. STS is known to poorly corre-\n",
      "late with other real-world use cases (Neelakantan\n",
      "et al., 2022; Wang et al., 2021). Further, evaluating\n",
      "embedding methods on many tasks requires imple-\n",
      "menting multiple evaluation pipelines. Implemen-\n",
      "tation details like pre-processing or hyperparam-\n",
      "eters may inﬂuence the results making it unclear\n",
      "whether performance improvements simply come\n",
      "from a favorable evaluation pipeline. This leads to\n",
      "the “blind” application of these models to new use\n",
      "cases in industry or requires incremental work to\n",
      "reevaluate them on different tasks.\n",
      "The Massive Text Embedding Benchmark\n",
      "(MTEB) aims to provide clarity on how models\n",
      "perform on a variety of embedding tasks and thus\n",
      "serves as the gateway to ﬁnding universal text em-\n",
      "beddings applicable to a variety of tasks. MTEB\n",
      "consists of 58 datasets covering 112 languages\n",
      "from 8 embedding tasks: Bitext mining, classi-\n",
      "ﬁcation, clustering, pair classiﬁcation, reranking,\n",
      "retrieval, STS and summarization. MTEB software\n",
      "is available open-source1 enabling evaluation of\n",
      "any embedding model by adding less than 10 lines\n",
      "of code. Datasets and the MTEB leaderboard are\n",
      "available on the Hugging Face Hub2.\n",
      "We evaluate over 30 models on MTEB with addi-\n",
      "tional speed and memory benchmarking to provide\n",
      "a holistic view of the state of text embedding mod-\n",
      "els. We cover both models available open-source\n",
      "as well as models accessible via APIs, such as the\n",
      "OpenAI Embeddings endpoint. We ﬁnd there to be\n",
      "no single best solution, with different models dom-\n",
      "1https://github.com/embeddings-benchm\n",
      "ark/mteb\n",
      "2https://huggingface.co/spaces/mteb/l\n",
      "eaderboard\n",
      "arXiv:2210.07316v3  [cs.CL]  19 Mar 2023\n",
      "inating different tasks. Our benchmarking sheds\n",
      "light on the weaknesses and strengths of individual\n",
      "models, such as SimCSE’s (Gao et al., 2021b) low\n",
      "performance on clustering and retrieval despite its\n",
      "strong performance on STS. We hope our work\n",
      "makes selecting the right embedding model easier\n",
      "and simpliﬁes future embedding research.\n",
      "2\n",
      "Related Work\n",
      "2.1\n",
      "Benchmarks\n",
      "Benchmarks, such as (Super)GLUE (Wang et al.,\n",
      "2018, 2019) or Big-BENCH (Srivastava et al.,\n",
      "2022), and evaluation frameworks (Gao et al.,\n",
      "2021a) play a key role in driving NLP progress.\n",
      "Yearly released SemEval datasets (Agirre et al.,\n",
      "2012, 2013, 2014, 2015, 2016) are commonly used\n",
      "as the go-to benchmark for text embeddings. Se-\n",
      "mEval datasets correspond to the task of semantic\n",
      "textual similarity (STS) requiring models to embed\n",
      "similar sentences with geometrically close embed-\n",
      "dings. Due to the limited expressivity of a single Se-\n",
      "mEval dataset, SentEval (Conneau and Kiela, 2018)\n",
      "aggregates multiple STS datasets. SentEval focuses\n",
      "on ﬁne-tuning classiﬁers on top of embeddings. It\n",
      "lacks tasks like retrieval or clustering, where em-\n",
      "beddings are directly compared without additional\n",
      "classiﬁers. Further, the toolkit was proposed in\n",
      "2018 and thus does not provide easy support for\n",
      "recent trends like text embeddings from transform-\n",
      "ers (Reimers and Gurevych, 2019). Due to the\n",
      "insufﬁciency of STS benchmarking, USEB (Wang\n",
      "et al., 2021) was introduced consisting mostly of\n",
      "reranking tasks. Consequently, it does not cover\n",
      "tasks like retrieval or classiﬁcation. Meanwhile, the\n",
      "recently released BEIR Benchmark (Thakur et al.,\n",
      "2021) has become the standard for the evaluation\n",
      "of embeddings for zero-shot information retrieval.\n",
      "MTEB uniﬁes datasets from different embed-\n",
      "ding tasks into a common, accessible evaluation\n",
      "framework. MTEB incorporates SemEval datasets\n",
      "(STS11 - STS22) and BEIR alongside a variety of\n",
      "other datasets from various tasks to provide a holis-\n",
      "tic performance review of text embedding models.\n",
      "2.2\n",
      "Embedding Models\n",
      "Text embedding models like Glove (Pennington\n",
      "et al., 2014) lack context awareness and are thus\n",
      "commonly labeled as Word Embedding Models.\n",
      "They consist of a layer mapping each input word\n",
      "to a vector often followed by an averaging layer to\n",
      "provide a ﬁnal embedding invariant of input length.\n",
      "Transformers (Vaswani et al., 2017) inject context\n",
      "awareness into language models via self-attention\n",
      "and form the foundation of most recent embed-\n",
      "ding models. BERT (Devlin et al., 2018) uses the\n",
      "transformer architecture and performs large-scale\n",
      "self-supervised pre-training. The resulting model\n",
      "can directly be used to produce text embeddings\n",
      "via an averaging operation alike Glove. Build-\n",
      "ing on InferSent (Conneau et al., 2017), SBERT\n",
      "(Reimers and Gurevych, 2019) demonstrated it to\n",
      "be beneﬁcial to perform additional ﬁne-tuning of\n",
      "the transformer for competitive embedding perfor-\n",
      "mance. Most recent ﬁne-tuned embedding models\n",
      "use a contrastive loss objective to perform super-\n",
      "vised ﬁne-tuning on positive and negative text pairs\n",
      "(Gao et al., 2021b; Wang et al., 2021; Ni et al.,\n",
      "2021b; Muennighoff, 2022). Due to the large va-\n",
      "riety of available pre-trained transformers (Wolf\n",
      "et al., 2020), there is an at least equally large va-\n",
      "riety of potential text embedding models to be ex-\n",
      "plored. This leads to confusion about which model\n",
      "provides practitioners with the best performance\n",
      "for their embedding use case.\n",
      "We benchmark both word embedding and trans-\n",
      "former models on MTEB quantifying gains pro-\n",
      "vided by often much slower context aware models.\n",
      "3\n",
      "The MTEB Benchmark\n",
      "3.1\n",
      "Desiderata\n",
      "MTEB is built on a set of desiderata: (a) Diversity:\n",
      "MTEB aims to provide an understanding of the\n",
      "usability of embedding models in various use cases.\n",
      "The benchmark comprises 8 different tasks, with\n",
      "up to 15 datasets each. Of the 58 total datasets in\n",
      "MTEB, 10 are multilingual, covering 112 differ-\n",
      "ent languages. Sentence-level and paragraph-level\n",
      "datasets are included to contrast performance on\n",
      "short and long texts. (b) Simplicity: MTEB pro-\n",
      "vides a simple API for plugging in any model that\n",
      "given a list of texts can produce a vector for each\n",
      "list item with a consistent shape. This makes it\n",
      "possible to benchmark a diverse set of models. (c)\n",
      "Extensibility: New datasets for existing tasks can\n",
      "be benchmarked in MTEB via a single ﬁle that\n",
      "speciﬁes the task and a Hugging Face dataset name\n",
      "where the data has been uploaded (Lhoest et al.,\n",
      "2021). New tasks require implementing a task in-\n",
      "terface for loading the data and an evaluator for\n",
      "benchmarking. We welcome dataset, task or metric\n",
      "contributions from the community via pull requests\n",
      "to continue the development of MTEB. (d) Repro-\n",
      "MTEB\n",
      "8 Tasks\n",
      "58 Datasets\n",
      "Massive Text  \n",
      "Embedding Benchmark\n",
      "Classification\n",
      "AmazonCounterfactual\n",
      "Retrieval\n",
      "Pair Classification\n",
      "AmazonPolarity\n",
      "AmazonReviews\n",
      "Banking77\n",
      "Emotion\n",
      "Imdb\n",
      "MassiveIntent\n",
      "MassiveScenario\n",
      "MTOPDomain\n",
      "MTOPIntent\n",
      "ToxicConversations\n",
      "TweetSentimentExtraction\n",
      "SprintDuplicateQuestions\n",
      "TwitterSemEval2015\n",
      "TwitterURLCorpus\n",
      "Clustering\n",
      "ArxivP2P\n",
      "ArxivS2S\n",
      "STS\n",
      "BIOSESS\n",
      "SICK-R\n",
      "STS11\n",
      "STS12\n",
      "STS13\n",
      "STS14\n",
      "Reranking\n",
      "Summarization\n",
      "STS15\n",
      "STS16\n",
      "AskUbuntuDupQuestions\n",
      "MindSmallReranking\n",
      "SciDocsRR\n",
      "StackOverFlowDupQuestions\n",
      "SummEval \n",
      "STSB\n",
      "STS17\n",
      "STS22\n",
      "ArguAna\n",
      "ClimateFEVER\n",
      "CQADupstackRetrieval\n",
      "FEVER\n",
      "DBPedia\n",
      "FiQA2018\n",
      "HotpotQA\n",
      "MSMARCO\n",
      "NFCorpus\n",
      "NQ\n",
      "Quora\n",
      "SCIDOCS\n",
      "SciFact\n",
      "Touche2020\n",
      "TRECCOVID\n",
      "MedrxivP2P\n",
      "MedrxivS2S\n",
      "Reddit\n",
      "StackExchange\n",
      "RedditP2P\n",
      "StackExchangeP2P\n",
      "TwentyNewsgroup\n",
      "BiorxivP2P\n",
      "BiorxivS2S\n",
      "Bitext Mining\n",
      "BUCC\n",
      "Tatoeba\n",
      "Figure 1: An overview of tasks and datasets in MTEB. Multilingual datasets are marked with a purple shade.\n",
      "ducibility: Through versioning at a dataset and\n",
      "software level, we aim to make it easy to repro-\n",
      "duce results in MTEB. JSON ﬁles corresponding\n",
      "to all results available in this paper have been made\n",
      "available together with the MTEB benchmark3.\n",
      "3.2\n",
      "Tasks and Evaluation\n",
      "Figure 1 provides an overview of tasks and datasets\n",
      "available in MTEB. Dataset statistics are available\n",
      "in Table 2. The benchmark consists of the follow-\n",
      "ing 8 task types:\n",
      "Bitext Mining\n",
      "Inputs are two sets of sentences\n",
      "from two different languages. For each sentence\n",
      "in the ﬁrst set, the best match in the second set\n",
      "needs to be found. The matches are commonly\n",
      "translations. The provided model is used to embed\n",
      "each sentence and the closest pairs are found via\n",
      "cosine similarity. F1 serves as the main metric for\n",
      "bitext mining. Accuracy, precision and recall are\n",
      "also computed.\n",
      "Classiﬁcation\n",
      "A train and test set are embedded\n",
      "with the provided model. The train set embeddings\n",
      "are used to train a logistic regression classiﬁer with\n",
      "100 maximum iterations, which is scored on the\n",
      "test set. The main metric is accuracy with average\n",
      "precision and f1 additionally provided.\n",
      "3https://huggingface.co/datasets/mteb\n",
      "/results\n",
      "Clustering\n",
      "Given a set of sentences or para-\n",
      "graphs, the goal is to group them into meaning-\n",
      "ful clusters. A mini-batch k-means model with\n",
      "batch size 32 and k equal to the number of dif-\n",
      "ferent labels (Pedregosa et al., 2011) is trained on\n",
      "the embedded texts. The model is scored using\n",
      "v-measure (Rosenberg and Hirschberg, 2007). V-\n",
      "measure does not depend on the cluster label, thus\n",
      "the permutation of labels does not affect the score.\n",
      "Pair Classiﬁcation\n",
      "A pair of text inputs is pro-\n",
      "vided and a label needs to be assigned. Labels\n",
      "are typically binary variables denoting duplicate\n",
      "or paraphrase pairs. The two texts are embedded\n",
      "and their distance is computed with various metrics\n",
      "(cosine similarity, dot product, euclidean distance,\n",
      "manhattan distance). Using the best binary thresh-\n",
      "old accuracy, average precision, f1, precision and\n",
      "recall are computed. The average precision score\n",
      "based on cosine similarity is the main metric.\n",
      "Reranking\n",
      "Inputs are a query and a list of rele-\n",
      "vant and irrelevant reference texts. The aim is to\n",
      "rank the results according to their relevance to the\n",
      "query. The model is used to embed the references\n",
      "which are then compared to the query using cosine\n",
      "similarity. The resulting ranking is scored for each\n",
      "query and averaged across all queries. Metrics are\n",
      "mean MRR@k and MAP with the latter being the\n",
      "main metric.\n",
      "AmazonCounterfactualClassification\n",
      "AmazonPolarityClassification\n",
      "AmazonReviewsClassification\n",
      "Banking77Classification\n",
      "EmotionClassification\n",
      "ImdbClassification\n",
      "MassiveIntentClassification\n",
      "MassiveScenarioClassification\n",
      "MTOPDomainClassification\n",
      "MTOPIntentClassification\n",
      "ToxicConversationsClassification\n",
      "TweetSentimentExtractionClassification\n",
      "ArxivClusteringP2P\n",
      "ArxivClusteringS2S\n",
      "BiorxivClusteringP2P\n",
      "BiorxivClusteringS2S\n",
      "MedrxivClusteringP2P\n",
      "MedrxivClusteringS2S\n",
      "RedditClustering\n",
      "RedditClusteringP2P\n",
      "StackExchangeClustering\n",
      "StackExchangeClusteringP2P\n",
      "TwentyNewsgroupsClustering\n",
      "SprintDuplicateQuestions\n",
      "TwitterSemEval2015\n",
      "TwitterURLCorpus\n",
      "AskUbuntuDupQuestions\n",
      "MindSmallReranking\n",
      "SciDocsRR\n",
      "StackOverflowDupQuestions\n",
      "ArguAna\n",
      "ClimateFEVER\n",
      "CQADupstackAndroidRetrieval\n",
      "CQADupstackEnglishRetrieval\n",
      "CQADupstackGamingRetrieval\n",
      "CQADupstackGisRetrieval\n",
      "CQADupstackMathematicaRetrieval\n",
      "CQADupstackPhysicsRetrieval\n",
      "CQADupstackProgrammersRetrieval\n",
      "CQADupstackStatsRetrieval\n",
      "CQADupstackTexRetrieval\n",
      "CQADupstackUnixRetrieval\n",
      "CQADupstackWebmastersRetrieval\n",
      "CQADupstackWordpressRetrieval\n",
      "DBPedia\n",
      "FEVER\n",
      "FiQA2018\n",
      "HotpotQA\n",
      "MSMARCO\n",
      "NFCorpus\n",
      "NQ\n",
      "QuoraRetrieval\n",
      "SCIDOCS\n",
      "SciFact\n",
      "Touche2020\n",
      "TRECCOVID\n",
      "BIOSSES\n",
      "SICK-R\n",
      "STS12\n",
      "STS13\n",
      "STS14\n",
      "STS15\n",
      "STS16\n",
      "STS17\n",
      "STS22\n",
      "STSBenchmark\n",
      "SummEval\n",
      "AmazonCounterfactualClassification\n",
      "AmazonPolarityClassification\n",
      "AmazonReviewsClassification\n",
      "Banking77Classification\n",
      "EmotionClassification\n",
      "ImdbClassification\n",
      "MassiveIntentClassification\n",
      "MassiveScenarioClassification\n",
      "MTOPDomainClassification\n",
      "MTOPIntentClassification\n",
      "ToxicConversationsClassification\n",
      "TweetSentimentExtractionClassification\n",
      "ArxivClusteringP2P\n",
      "ArxivClusteringS2S\n",
      "BiorxivClusteringP2P\n",
      "BiorxivClusteringS2S\n",
      "MedrxivClusteringP2P\n",
      "MedrxivClusteringS2S\n",
      "RedditClustering\n",
      "RedditClusteringP2P\n",
      "StackExchangeClustering\n",
      "StackExchangeClusteringP2P\n",
      "TwentyNewsgroupsClustering\n",
      "SprintDuplicateQuestions\n",
      "TwitterSemEval2015\n",
      "TwitterURLCorpus\n",
      "AskUbuntuDupQuestions\n",
      "MindSmallReranking\n",
      "SciDocsRR\n",
      "StackOverflowDupQuestions\n",
      "ArguAna\n",
      "ClimateFEVER\n",
      "CQADupstackAndroidRetrieval\n",
      "CQADupstackEnglishRetrieval\n",
      "CQADupstackGamingRetrieval\n",
      "CQADupstackGisRetrieval\n",
      "CQADupstackMathematicaRetrieval\n",
      "CQADupstackPhysicsRetrieval\n",
      "CQADupstackProgrammersRetrieval\n",
      "CQADupstackStatsRetrieval\n",
      "CQADupstackTexRetrieval\n",
      "CQADupstackUnixRetrieval\n",
      "CQADupstackWebmastersRetrieval\n",
      "CQADupstackWordpressRetrieval\n",
      "DBPedia\n",
      "FEVER\n",
      "FiQA2018\n",
      "HotpotQA\n",
      "MSMARCO\n",
      "NFCorpus\n",
      "NQ\n",
      "QuoraRetrieval\n",
      "SCIDOCS\n",
      "SciFact\n",
      "Touche2020\n",
      "TRECCOVID\n",
      "BIOSSES\n",
      "SICK-R\n",
      "STS12\n",
      "STS13\n",
      "STS14\n",
      "STS15\n",
      "STS16\n",
      "STS17\n",
      "STS22\n",
      "STSBenchmark\n",
      "SummEval\n",
      "97\n",
      "85\n",
      "84\n",
      "90\n",
      "89\n",
      "83\n",
      "90\n",
      "89\n",
      "84\n",
      "87\n",
      "91\n",
      "94\n",
      "81\n",
      "85\n",
      "85\n",
      "92\n",
      "92\n",
      "89\n",
      "91\n",
      "89\n",
      "88\n",
      "92\n",
      "92\n",
      "89\n",
      "91\n",
      "89\n",
      "88\n",
      "100\n",
      "91\n",
      "92\n",
      "87\n",
      "92\n",
      "88\n",
      "88\n",
      "98\n",
      "98\n",
      "91\n",
      "92\n",
      "87\n",
      "92\n",
      "88\n",
      "88\n",
      "98\n",
      "98\n",
      "100\n",
      "93\n",
      "93\n",
      "87\n",
      "90\n",
      "89\n",
      "90\n",
      "96\n",
      "96\n",
      "95\n",
      "95\n",
      "94\n",
      "94\n",
      "89\n",
      "91\n",
      "92\n",
      "90\n",
      "97\n",
      "97\n",
      "96\n",
      "96\n",
      "98\n",
      "91\n",
      "91\n",
      "83\n",
      "87\n",
      "83\n",
      "86\n",
      "90\n",
      "90\n",
      "89\n",
      "89\n",
      "89\n",
      "89\n",
      "92\n",
      "93\n",
      "87\n",
      "90\n",
      "87\n",
      "89\n",
      "97\n",
      "97\n",
      "95\n",
      "95\n",
      "96\n",
      "96\n",
      "93\n",
      "88\n",
      "88\n",
      "81\n",
      "85\n",
      "82\n",
      "85\n",
      "87\n",
      "87\n",
      "87\n",
      "87\n",
      "87\n",
      "87\n",
      "95\n",
      "90\n",
      "91\n",
      "91\n",
      "85\n",
      "87\n",
      "85\n",
      "88\n",
      "93\n",
      "93\n",
      "92\n",
      "92\n",
      "92\n",
      "92\n",
      "95\n",
      "96\n",
      "94\n",
      "87\n",
      "87\n",
      "81\n",
      "84\n",
      "81\n",
      "84\n",
      "87\n",
      "87\n",
      "87\n",
      "87\n",
      "87\n",
      "87\n",
      "92\n",
      "89\n",
      "96\n",
      "93\n",
      "89\n",
      "89\n",
      "83\n",
      "85\n",
      "83\n",
      "85\n",
      "90\n",
      "90\n",
      "89\n",
      "89\n",
      "89\n",
      "90\n",
      "93\n",
      "93\n",
      "93\n",
      "97\n",
      "96\n",
      "94\n",
      "94\n",
      "88\n",
      "92\n",
      "90\n",
      "89\n",
      "95\n",
      "95\n",
      "95\n",
      "95\n",
      "95\n",
      "96\n",
      "90\n",
      "95\n",
      "88\n",
      "93\n",
      "88\n",
      "91\n",
      "94\n",
      "95\n",
      "86\n",
      "93\n",
      "92\n",
      "91\n",
      "95\n",
      "95\n",
      "95\n",
      "95\n",
      "96\n",
      "97\n",
      "92\n",
      "95\n",
      "90\n",
      "93\n",
      "89\n",
      "91\n",
      "96\n",
      "92\n",
      "92\n",
      "89\n",
      "91\n",
      "88\n",
      "88\n",
      "95\n",
      "95\n",
      "94\n",
      "94\n",
      "94\n",
      "94\n",
      "92\n",
      "96\n",
      "89\n",
      "94\n",
      "89\n",
      "92\n",
      "95\n",
      "95\n",
      "87\n",
      "87\n",
      "79\n",
      "86\n",
      "82\n",
      "83\n",
      "90\n",
      "90\n",
      "89\n",
      "89\n",
      "89\n",
      "88\n",
      "89\n",
      "91\n",
      "86\n",
      "90\n",
      "85\n",
      "88\n",
      "89\n",
      "91\n",
      "92\n",
      "93\n",
      "93\n",
      "88\n",
      "91\n",
      "87\n",
      "88\n",
      "96\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "96\n",
      "92\n",
      "98\n",
      "89\n",
      "95\n",
      "90\n",
      "93\n",
      "95\n",
      "95\n",
      "96\n",
      "91\n",
      "74\n",
      "74\n",
      "69\n",
      "78\n",
      "72\n",
      "69\n",
      "77\n",
      "77\n",
      "79\n",
      "79\n",
      "74\n",
      "75\n",
      "73\n",
      "75\n",
      "71\n",
      "75\n",
      "71\n",
      "74\n",
      "77\n",
      "76\n",
      "77\n",
      "74\n",
      "77\n",
      "88\n",
      "89\n",
      "83\n",
      "85\n",
      "85\n",
      "85\n",
      "91\n",
      "91\n",
      "90\n",
      "90\n",
      "92\n",
      "92\n",
      "85\n",
      "91\n",
      "83\n",
      "88\n",
      "83\n",
      "85\n",
      "91\n",
      "92\n",
      "89\n",
      "84\n",
      "90\n",
      "71\n",
      "92\n",
      "92\n",
      "84\n",
      "88\n",
      "87\n",
      "89\n",
      "92\n",
      "92\n",
      "92\n",
      "92\n",
      "93\n",
      "93\n",
      "89\n",
      "92\n",
      "87\n",
      "91\n",
      "87\n",
      "89\n",
      "93\n",
      "93\n",
      "92\n",
      "86\n",
      "93\n",
      "74\n",
      "88\n",
      "88\n",
      "87\n",
      "85\n",
      "89\n",
      "84\n",
      "84\n",
      "92\n",
      "92\n",
      "91\n",
      "91\n",
      "89\n",
      "91\n",
      "89\n",
      "91\n",
      "86\n",
      "90\n",
      "85\n",
      "88\n",
      "90\n",
      "90\n",
      "92\n",
      "88\n",
      "92\n",
      "77\n",
      "85\n",
      "87\n",
      "84\n",
      "86\n",
      "80\n",
      "81\n",
      "80\n",
      "84\n",
      "89\n",
      "89\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "85\n",
      "88\n",
      "82\n",
      "88\n",
      "82\n",
      "86\n",
      "88\n",
      "88\n",
      "87\n",
      "82\n",
      "89\n",
      "67\n",
      "84\n",
      "88\n",
      "83\n",
      "91\n",
      "92\n",
      "86\n",
      "89\n",
      "85\n",
      "88\n",
      "95\n",
      "95\n",
      "93\n",
      "93\n",
      "93\n",
      "93\n",
      "94\n",
      "97\n",
      "91\n",
      "97\n",
      "91\n",
      "95\n",
      "94\n",
      "94\n",
      "95\n",
      "92\n",
      "96\n",
      "76\n",
      "89\n",
      "92\n",
      "91\n",
      "89\n",
      "88\n",
      "88\n",
      "84\n",
      "87\n",
      "83\n",
      "83\n",
      "92\n",
      "92\n",
      "91\n",
      "91\n",
      "89\n",
      "90\n",
      "90\n",
      "92\n",
      "86\n",
      "92\n",
      "86\n",
      "90\n",
      "90\n",
      "90\n",
      "93\n",
      "92\n",
      "92\n",
      "75\n",
      "85\n",
      "87\n",
      "92\n",
      "84\n",
      "93\n",
      "92\n",
      "91\n",
      "84\n",
      "87\n",
      "85\n",
      "89\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "91\n",
      "90\n",
      "92\n",
      "91\n",
      "91\n",
      "91\n",
      "91\n",
      "90\n",
      "91\n",
      "93\n",
      "92\n",
      "87\n",
      "92\n",
      "72\n",
      "86\n",
      "91\n",
      "87\n",
      "86\n",
      "92\n",
      "88\n",
      "87\n",
      "88\n",
      "83\n",
      "86\n",
      "83\n",
      "84\n",
      "91\n",
      "91\n",
      "90\n",
      "90\n",
      "90\n",
      "91\n",
      "88\n",
      "91\n",
      "85\n",
      "90\n",
      "86\n",
      "88\n",
      "91\n",
      "90\n",
      "90\n",
      "85\n",
      "92\n",
      "72\n",
      "85\n",
      "88\n",
      "86\n",
      "84\n",
      "90\n",
      "86\n",
      "87\n",
      "88\n",
      "87\n",
      "80\n",
      "89\n",
      "82\n",
      "84\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "88\n",
      "88\n",
      "87\n",
      "89\n",
      "85\n",
      "88\n",
      "85\n",
      "86\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "90\n",
      "79\n",
      "84\n",
      "86\n",
      "90\n",
      "81\n",
      "90\n",
      "90\n",
      "87\n",
      "85\n",
      "91\n",
      "91\n",
      "83\n",
      "89\n",
      "86\n",
      "87\n",
      "92\n",
      "92\n",
      "92\n",
      "92\n",
      "92\n",
      "92\n",
      "90\n",
      "93\n",
      "88\n",
      "91\n",
      "88\n",
      "89\n",
      "93\n",
      "93\n",
      "96\n",
      "91\n",
      "93\n",
      "74\n",
      "86\n",
      "90\n",
      "87\n",
      "83\n",
      "92\n",
      "89\n",
      "91\n",
      "88\n",
      "91\n",
      "91\n",
      "90\n",
      "82\n",
      "90\n",
      "85\n",
      "87\n",
      "93\n",
      "93\n",
      "92\n",
      "92\n",
      "91\n",
      "91\n",
      "89\n",
      "93\n",
      "87\n",
      "91\n",
      "87\n",
      "89\n",
      "93\n",
      "94\n",
      "94\n",
      "95\n",
      "93\n",
      "75\n",
      "87\n",
      "89\n",
      "90\n",
      "84\n",
      "92\n",
      "91\n",
      "90\n",
      "88\n",
      "94\n",
      "94\n",
      "86\n",
      "85\n",
      "79\n",
      "86\n",
      "80\n",
      "81\n",
      "87\n",
      "87\n",
      "87\n",
      "87\n",
      "86\n",
      "86\n",
      "88\n",
      "89\n",
      "85\n",
      "89\n",
      "86\n",
      "88\n",
      "87\n",
      "88\n",
      "91\n",
      "93\n",
      "89\n",
      "74\n",
      "81\n",
      "84\n",
      "88\n",
      "80\n",
      "90\n",
      "91\n",
      "86\n",
      "84\n",
      "91\n",
      "90\n",
      "92\n",
      "88\n",
      "87\n",
      "80\n",
      "87\n",
      "82\n",
      "83\n",
      "89\n",
      "89\n",
      "89\n",
      "89\n",
      "87\n",
      "87\n",
      "91\n",
      "91\n",
      "88\n",
      "91\n",
      "87\n",
      "89\n",
      "89\n",
      "90\n",
      "93\n",
      "94\n",
      "90\n",
      "77\n",
      "82\n",
      "86\n",
      "89\n",
      "81\n",
      "92\n",
      "92\n",
      "88\n",
      "85\n",
      "92\n",
      "92\n",
      "93\n",
      "94\n",
      "88\n",
      "88\n",
      "80\n",
      "87\n",
      "82\n",
      "83\n",
      "89\n",
      "89\n",
      "88\n",
      "88\n",
      "89\n",
      "88\n",
      "93\n",
      "92\n",
      "88\n",
      "92\n",
      "87\n",
      "89\n",
      "90\n",
      "91\n",
      "93\n",
      "92\n",
      "91\n",
      "73\n",
      "83\n",
      "87\n",
      "87\n",
      "82\n",
      "92\n",
      "88\n",
      "90\n",
      "86\n",
      "91\n",
      "94\n",
      "93\n",
      "90\n",
      "93\n",
      "88\n",
      "88\n",
      "81\n",
      "87\n",
      "82\n",
      "85\n",
      "90\n",
      "90\n",
      "89\n",
      "89\n",
      "88\n",
      "88\n",
      "90\n",
      "91\n",
      "88\n",
      "91\n",
      "87\n",
      "89\n",
      "90\n",
      "91\n",
      "94\n",
      "95\n",
      "92\n",
      "75\n",
      "83\n",
      "87\n",
      "88\n",
      "81\n",
      "93\n",
      "92\n",
      "91\n",
      "85\n",
      "92\n",
      "95\n",
      "94\n",
      "93\n",
      "94\n",
      "94\n",
      "87\n",
      "87\n",
      "80\n",
      "86\n",
      "81\n",
      "82\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "87\n",
      "87\n",
      "92\n",
      "91\n",
      "89\n",
      "92\n",
      "90\n",
      "91\n",
      "89\n",
      "90\n",
      "92\n",
      "93\n",
      "91\n",
      "74\n",
      "83\n",
      "86\n",
      "87\n",
      "81\n",
      "93\n",
      "90\n",
      "89\n",
      "85\n",
      "90\n",
      "93\n",
      "92\n",
      "93\n",
      "96\n",
      "94\n",
      "95\n",
      "87\n",
      "87\n",
      "80\n",
      "86\n",
      "80\n",
      "82\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "86\n",
      "87\n",
      "90\n",
      "90\n",
      "87\n",
      "90\n",
      "86\n",
      "88\n",
      "89\n",
      "89\n",
      "93\n",
      "92\n",
      "90\n",
      "75\n",
      "82\n",
      "85\n",
      "88\n",
      "81\n",
      "90\n",
      "91\n",
      "87\n",
      "84\n",
      "90\n",
      "92\n",
      "91\n",
      "92\n",
      "96\n",
      "91\n",
      "93\n",
      "93\n",
      "88\n",
      "87\n",
      "81\n",
      "88\n",
      "82\n",
      "84\n",
      "90\n",
      "90\n",
      "89\n",
      "89\n",
      "88\n",
      "88\n",
      "89\n",
      "90\n",
      "87\n",
      "90\n",
      "86\n",
      "88\n",
      "90\n",
      "90\n",
      "93\n",
      "93\n",
      "91\n",
      "76\n",
      "83\n",
      "86\n",
      "93\n",
      "80\n",
      "91\n",
      "91\n",
      "88\n",
      "85\n",
      "94\n",
      "93\n",
      "94\n",
      "93\n",
      "95\n",
      "93\n",
      "95\n",
      "94\n",
      "94\n",
      "88\n",
      "88\n",
      "80\n",
      "88\n",
      "82\n",
      "84\n",
      "90\n",
      "90\n",
      "89\n",
      "89\n",
      "89\n",
      "88\n",
      "89\n",
      "91\n",
      "87\n",
      "90\n",
      "87\n",
      "89\n",
      "90\n",
      "91\n",
      "93\n",
      "93\n",
      "91\n",
      "74\n",
      "83\n",
      "87\n",
      "89\n",
      "83\n",
      "92\n",
      "91\n",
      "89\n",
      "85\n",
      "93\n",
      "93\n",
      "94\n",
      "93\n",
      "93\n",
      "92\n",
      "96\n",
      "93\n",
      "93\n",
      "94\n",
      "87\n",
      "87\n",
      "80\n",
      "87\n",
      "82\n",
      "83\n",
      "89\n",
      "89\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "89\n",
      "89\n",
      "86\n",
      "90\n",
      "86\n",
      "88\n",
      "89\n",
      "90\n",
      "92\n",
      "92\n",
      "90\n",
      "75\n",
      "83\n",
      "87\n",
      "89\n",
      "81\n",
      "91\n",
      "92\n",
      "88\n",
      "84\n",
      "92\n",
      "92\n",
      "93\n",
      "92\n",
      "93\n",
      "91\n",
      "93\n",
      "92\n",
      "93\n",
      "94\n",
      "96\n",
      "90\n",
      "90\n",
      "86\n",
      "87\n",
      "84\n",
      "86\n",
      "93\n",
      "93\n",
      "92\n",
      "92\n",
      "92\n",
      "93\n",
      "90\n",
      "95\n",
      "89\n",
      "93\n",
      "88\n",
      "91\n",
      "93\n",
      "92\n",
      "93\n",
      "87\n",
      "94\n",
      "73\n",
      "88\n",
      "89\n",
      "88\n",
      "85\n",
      "93\n",
      "89\n",
      "89\n",
      "91\n",
      "86\n",
      "91\n",
      "89\n",
      "86\n",
      "87\n",
      "88\n",
      "87\n",
      "88\n",
      "87\n",
      "87\n",
      "88\n",
      "87\n",
      "87\n",
      "88\n",
      "83\n",
      "86\n",
      "83\n",
      "84\n",
      "91\n",
      "91\n",
      "90\n",
      "90\n",
      "90\n",
      "91\n",
      "88\n",
      "91\n",
      "85\n",
      "90\n",
      "86\n",
      "88\n",
      "91\n",
      "90\n",
      "90\n",
      "85\n",
      "92\n",
      "72\n",
      "85\n",
      "88\n",
      "86\n",
      "84\n",
      "90\n",
      "86\n",
      "87\n",
      "100\n",
      "85\n",
      "88\n",
      "88\n",
      "84\n",
      "85\n",
      "86\n",
      "85\n",
      "85\n",
      "84\n",
      "85\n",
      "85\n",
      "84\n",
      "91\n",
      "92\n",
      "92\n",
      "87\n",
      "90\n",
      "87\n",
      "88\n",
      "95\n",
      "95\n",
      "93\n",
      "93\n",
      "95\n",
      "94\n",
      "91\n",
      "95\n",
      "88\n",
      "92\n",
      "88\n",
      "90\n",
      "94\n",
      "94\n",
      "95\n",
      "90\n",
      "95\n",
      "75\n",
      "89\n",
      "92\n",
      "91\n",
      "87\n",
      "94\n",
      "91\n",
      "92\n",
      "89\n",
      "89\n",
      "92\n",
      "92\n",
      "88\n",
      "90\n",
      "90\n",
      "91\n",
      "90\n",
      "90\n",
      "90\n",
      "91\n",
      "90\n",
      "91\n",
      "89\n",
      "90\n",
      "90\n",
      "86\n",
      "89\n",
      "85\n",
      "88\n",
      "95\n",
      "95\n",
      "94\n",
      "94\n",
      "94\n",
      "94\n",
      "90\n",
      "96\n",
      "88\n",
      "93\n",
      "88\n",
      "92\n",
      "94\n",
      "93\n",
      "94\n",
      "88\n",
      "95\n",
      "74\n",
      "90\n",
      "91\n",
      "90\n",
      "88\n",
      "94\n",
      "90\n",
      "89\n",
      "93\n",
      "88\n",
      "91\n",
      "90\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "87\n",
      "88\n",
      "89\n",
      "87\n",
      "96\n",
      "93\n",
      "93\n",
      "90\n",
      "91\n",
      "87\n",
      "91\n",
      "86\n",
      "87\n",
      "94\n",
      "94\n",
      "95\n",
      "95\n",
      "93\n",
      "94\n",
      "89\n",
      "94\n",
      "87\n",
      "91\n",
      "88\n",
      "90\n",
      "94\n",
      "93\n",
      "93\n",
      "87\n",
      "94\n",
      "77\n",
      "88\n",
      "90\n",
      "89\n",
      "85\n",
      "92\n",
      "89\n",
      "89\n",
      "91\n",
      "89\n",
      "91\n",
      "90\n",
      "87\n",
      "88\n",
      "88\n",
      "88\n",
      "88\n",
      "87\n",
      "88\n",
      "89\n",
      "87\n",
      "93\n",
      "91\n",
      "93\n",
      "93\n",
      "89\n",
      "89\n",
      "83\n",
      "85\n",
      "84\n",
      "87\n",
      "89\n",
      "89\n",
      "88\n",
      "88\n",
      "90\n",
      "89\n",
      "92\n",
      "91\n",
      "94\n",
      "94\n",
      "95\n",
      "94\n",
      "90\n",
      "91\n",
      "92\n",
      "87\n",
      "92\n",
      "72\n",
      "85\n",
      "89\n",
      "86\n",
      "84\n",
      "92\n",
      "87\n",
      "93\n",
      "87\n",
      "86\n",
      "90\n",
      "89\n",
      "85\n",
      "88\n",
      "89\n",
      "89\n",
      "90\n",
      "86\n",
      "87\n",
      "88\n",
      "87\n",
      "90\n",
      "87\n",
      "90\n",
      "89\n",
      "89\n",
      "91\n",
      "92\n",
      "86\n",
      "89\n",
      "86\n",
      "88\n",
      "95\n",
      "95\n",
      "94\n",
      "94\n",
      "94\n",
      "95\n",
      "91\n",
      "96\n",
      "88\n",
      "94\n",
      "88\n",
      "92\n",
      "95\n",
      "94\n",
      "95\n",
      "89\n",
      "97\n",
      "74\n",
      "90\n",
      "92\n",
      "90\n",
      "89\n",
      "95\n",
      "91\n",
      "91\n",
      "93\n",
      "88\n",
      "92\n",
      "91\n",
      "87\n",
      "89\n",
      "90\n",
      "89\n",
      "88\n",
      "88\n",
      "89\n",
      "90\n",
      "89\n",
      "95\n",
      "93\n",
      "93\n",
      "97\n",
      "92\n",
      "90\n",
      "92\n",
      "92\n",
      "88\n",
      "92\n",
      "88\n",
      "89\n",
      "97\n",
      "97\n",
      "96\n",
      "96\n",
      "98\n",
      "97\n",
      "91\n",
      "97\n",
      "88\n",
      "93\n",
      "88\n",
      "91\n",
      "96\n",
      "96\n",
      "97\n",
      "91\n",
      "96\n",
      "76\n",
      "90\n",
      "93\n",
      "91\n",
      "88\n",
      "94\n",
      "91\n",
      "91\n",
      "91\n",
      "91\n",
      "94\n",
      "94\n",
      "89\n",
      "90\n",
      "92\n",
      "92\n",
      "91\n",
      "90\n",
      "91\n",
      "92\n",
      "91\n",
      "93\n",
      "91\n",
      "95\n",
      "95\n",
      "95\n",
      "91\n",
      "95\n",
      "89\n",
      "90\n",
      "85\n",
      "86\n",
      "82\n",
      "85\n",
      "91\n",
      "91\n",
      "89\n",
      "89\n",
      "89\n",
      "89\n",
      "95\n",
      "93\n",
      "92\n",
      "97\n",
      "92\n",
      "96\n",
      "90\n",
      "91\n",
      "93\n",
      "90\n",
      "93\n",
      "74\n",
      "85\n",
      "89\n",
      "88\n",
      "86\n",
      "97\n",
      "91\n",
      "91\n",
      "88\n",
      "87\n",
      "90\n",
      "89\n",
      "90\n",
      "92\n",
      "90\n",
      "92\n",
      "93\n",
      "90\n",
      "90\n",
      "90\n",
      "89\n",
      "91\n",
      "88\n",
      "91\n",
      "91\n",
      "90\n",
      "92\n",
      "92\n",
      "91\n",
      "89\n",
      "90\n",
      "83\n",
      "85\n",
      "83\n",
      "86\n",
      "89\n",
      "89\n",
      "89\n",
      "89\n",
      "89\n",
      "89\n",
      "93\n",
      "92\n",
      "95\n",
      "96\n",
      "95\n",
      "95\n",
      "90\n",
      "91\n",
      "92\n",
      "88\n",
      "92\n",
      "73\n",
      "85\n",
      "89\n",
      "86\n",
      "83\n",
      "94\n",
      "88\n",
      "92\n",
      "87\n",
      "87\n",
      "90\n",
      "90\n",
      "87\n",
      "90\n",
      "91\n",
      "90\n",
      "92\n",
      "88\n",
      "89\n",
      "89\n",
      "88\n",
      "91\n",
      "87\n",
      "90\n",
      "89\n",
      "89\n",
      "97\n",
      "90\n",
      "91\n",
      "95\n",
      "92\n",
      "93\n",
      "85\n",
      "88\n",
      "88\n",
      "90\n",
      "93\n",
      "93\n",
      "92\n",
      "92\n",
      "94\n",
      "93\n",
      "91\n",
      "93\n",
      "89\n",
      "92\n",
      "89\n",
      "90\n",
      "94\n",
      "95\n",
      "93\n",
      "88\n",
      "93\n",
      "74\n",
      "89\n",
      "93\n",
      "88\n",
      "87\n",
      "92\n",
      "89\n",
      "96\n",
      "88\n",
      "88\n",
      "92\n",
      "91\n",
      "86\n",
      "87\n",
      "89\n",
      "90\n",
      "88\n",
      "88\n",
      "88\n",
      "89\n",
      "88\n",
      "90\n",
      "88\n",
      "93\n",
      "91\n",
      "91\n",
      "91\n",
      "93\n",
      "94\n",
      "90\n",
      "90\n",
      "88\n",
      "89\n",
      "84\n",
      "84\n",
      "83\n",
      "86\n",
      "90\n",
      "90\n",
      "89\n",
      "89\n",
      "89\n",
      "90\n",
      "93\n",
      "92\n",
      "94\n",
      "96\n",
      "96\n",
      "97\n",
      "90\n",
      "91\n",
      "91\n",
      "86\n",
      "92\n",
      "72\n",
      "85\n",
      "89\n",
      "88\n",
      "86\n",
      "94\n",
      "88\n",
      "91\n",
      "88\n",
      "86\n",
      "89\n",
      "88\n",
      "86\n",
      "88\n",
      "88\n",
      "88\n",
      "90\n",
      "87\n",
      "88\n",
      "88\n",
      "87\n",
      "91\n",
      "88\n",
      "90\n",
      "91\n",
      "89\n",
      "96\n",
      "91\n",
      "90\n",
      "95\n",
      "96\n",
      "90\n",
      "86\n",
      "86\n",
      "80\n",
      "82\n",
      "80\n",
      "83\n",
      "85\n",
      "85\n",
      "85\n",
      "85\n",
      "85\n",
      "85\n",
      "91\n",
      "88\n",
      "93\n",
      "93\n",
      "92\n",
      "92\n",
      "86\n",
      "87\n",
      "88\n",
      "84\n",
      "88\n",
      "71\n",
      "82\n",
      "85\n",
      "83\n",
      "80\n",
      "90\n",
      "85\n",
      "90\n",
      "83\n",
      "84\n",
      "87\n",
      "86\n",
      "83\n",
      "86\n",
      "87\n",
      "86\n",
      "88\n",
      "85\n",
      "85\n",
      "85\n",
      "84\n",
      "87\n",
      "83\n",
      "86\n",
      "85\n",
      "85\n",
      "93\n",
      "86\n",
      "86\n",
      "91\n",
      "97\n",
      "87\n",
      "92\n",
      "83\n",
      "84\n",
      "81\n",
      "82\n",
      "81\n",
      "84\n",
      "89\n",
      "89\n",
      "88\n",
      "88\n",
      "89\n",
      "89\n",
      "81\n",
      "88\n",
      "78\n",
      "84\n",
      "78\n",
      "82\n",
      "88\n",
      "88\n",
      "86\n",
      "81\n",
      "88\n",
      "68\n",
      "89\n",
      "85\n",
      "83\n",
      "83\n",
      "86\n",
      "83\n",
      "82\n",
      "84\n",
      "81\n",
      "82\n",
      "83\n",
      "77\n",
      "79\n",
      "81\n",
      "80\n",
      "79\n",
      "78\n",
      "80\n",
      "80\n",
      "79\n",
      "84\n",
      "84\n",
      "85\n",
      "88\n",
      "86\n",
      "82\n",
      "88\n",
      "89\n",
      "81\n",
      "80\n",
      "86\n",
      "81\n",
      "76\n",
      "92\n",
      "92\n",
      "89\n",
      "90\n",
      "88\n",
      "88\n",
      "97\n",
      "97\n",
      "95\n",
      "95\n",
      "95\n",
      "96\n",
      "90\n",
      "96\n",
      "87\n",
      "93\n",
      "87\n",
      "91\n",
      "95\n",
      "94\n",
      "95\n",
      "89\n",
      "96\n",
      "76\n",
      "91\n",
      "93\n",
      "91\n",
      "87\n",
      "94\n",
      "91\n",
      "91\n",
      "91\n",
      "89\n",
      "92\n",
      "92\n",
      "87\n",
      "88\n",
      "89\n",
      "89\n",
      "88\n",
      "88\n",
      "89\n",
      "89\n",
      "88\n",
      "93\n",
      "91\n",
      "94\n",
      "95\n",
      "94\n",
      "90\n",
      "95\n",
      "96\n",
      "90\n",
      "90\n",
      "93\n",
      "90\n",
      "86\n",
      "90\n",
      "92\n",
      "91\n",
      "89\n",
      "90\n",
      "88\n",
      "88\n",
      "96\n",
      "96\n",
      "94\n",
      "94\n",
      "94\n",
      "95\n",
      "91\n",
      "95\n",
      "89\n",
      "93\n",
      "89\n",
      "91\n",
      "95\n",
      "94\n",
      "95\n",
      "90\n",
      "95\n",
      "75\n",
      "90\n",
      "92\n",
      "91\n",
      "87\n",
      "94\n",
      "91\n",
      "92\n",
      "90\n",
      "89\n",
      "94\n",
      "92\n",
      "88\n",
      "89\n",
      "91\n",
      "91\n",
      "89\n",
      "89\n",
      "90\n",
      "90\n",
      "89\n",
      "92\n",
      "90\n",
      "93\n",
      "93\n",
      "92\n",
      "91\n",
      "94\n",
      "95\n",
      "91\n",
      "91\n",
      "93\n",
      "91\n",
      "87\n",
      "86\n",
      "97\n",
      "94\n",
      "94\n",
      "90\n",
      "92\n",
      "91\n",
      "90\n",
      "97\n",
      "97\n",
      "96\n",
      "96\n",
      "97\n",
      "98\n",
      "92\n",
      "96\n",
      "90\n",
      "94\n",
      "90\n",
      "92\n",
      "96\n",
      "97\n",
      "96\n",
      "90\n",
      "97\n",
      "76\n",
      "91\n",
      "94\n",
      "92\n",
      "89\n",
      "95\n",
      "92\n",
      "94\n",
      "92\n",
      "90\n",
      "94\n",
      "94\n",
      "89\n",
      "90\n",
      "91\n",
      "91\n",
      "90\n",
      "89\n",
      "91\n",
      "91\n",
      "90\n",
      "93\n",
      "92\n",
      "95\n",
      "95\n",
      "94\n",
      "92\n",
      "95\n",
      "97\n",
      "92\n",
      "92\n",
      "95\n",
      "92\n",
      "88\n",
      "89\n",
      "98\n",
      "98\n",
      "93\n",
      "92\n",
      "89\n",
      "90\n",
      "89\n",
      "89\n",
      "96\n",
      "96\n",
      "95\n",
      "95\n",
      "95\n",
      "96\n",
      "91\n",
      "95\n",
      "89\n",
      "93\n",
      "88\n",
      "90\n",
      "95\n",
      "95\n",
      "95\n",
      "89\n",
      "95\n",
      "74\n",
      "90\n",
      "92\n",
      "91\n",
      "88\n",
      "94\n",
      "91\n",
      "92\n",
      "90\n",
      "89\n",
      "92\n",
      "92\n",
      "87\n",
      "88\n",
      "90\n",
      "89\n",
      "88\n",
      "87\n",
      "89\n",
      "89\n",
      "88\n",
      "92\n",
      "90\n",
      "94\n",
      "94\n",
      "93\n",
      "90\n",
      "94\n",
      "96\n",
      "90\n",
      "90\n",
      "94\n",
      "90\n",
      "87\n",
      "89\n",
      "96\n",
      "96\n",
      "98\n",
      "93\n",
      "93\n",
      "89\n",
      "91\n",
      "91\n",
      "89\n",
      "95\n",
      "95\n",
      "94\n",
      "94\n",
      "95\n",
      "95\n",
      "90\n",
      "93\n",
      "88\n",
      "91\n",
      "88\n",
      "89\n",
      "95\n",
      "95\n",
      "95\n",
      "90\n",
      "94\n",
      "75\n",
      "88\n",
      "93\n",
      "91\n",
      "85\n",
      "92\n",
      "90\n",
      "92\n",
      "88\n",
      "90\n",
      "93\n",
      "92\n",
      "87\n",
      "89\n",
      "90\n",
      "91\n",
      "89\n",
      "89\n",
      "90\n",
      "91\n",
      "90\n",
      "91\n",
      "88\n",
      "95\n",
      "92\n",
      "93\n",
      "91\n",
      "92\n",
      "96\n",
      "90\n",
      "90\n",
      "93\n",
      "89\n",
      "86\n",
      "85\n",
      "95\n",
      "94\n",
      "97\n",
      "95\n",
      "91\n",
      "90\n",
      "86\n",
      "88\n",
      "87\n",
      "88\n",
      "95\n",
      "95\n",
      "95\n",
      "95\n",
      "95\n",
      "96\n",
      "87\n",
      "94\n",
      "85\n",
      "91\n",
      "85\n",
      "88\n",
      "94\n",
      "94\n",
      "92\n",
      "87\n",
      "94\n",
      "73\n",
      "92\n",
      "91\n",
      "89\n",
      "88\n",
      "92\n",
      "89\n",
      "89\n",
      "89\n",
      "87\n",
      "89\n",
      "90\n",
      "85\n",
      "86\n",
      "87\n",
      "87\n",
      "85\n",
      "84\n",
      "86\n",
      "87\n",
      "86\n",
      "90\n",
      "89\n",
      "91\n",
      "94\n",
      "92\n",
      "88\n",
      "93\n",
      "95\n",
      "88\n",
      "87\n",
      "92\n",
      "88\n",
      "83\n",
      "93\n",
      "95\n",
      "93\n",
      "95\n",
      "95\n",
      "92\n",
      "89\n",
      "89\n",
      "91\n",
      "88\n",
      "85\n",
      "85\n",
      "94\n",
      "94\n",
      "92\n",
      "92\n",
      "93\n",
      "93\n",
      "89\n",
      "93\n",
      "87\n",
      "91\n",
      "87\n",
      "90\n",
      "92\n",
      "93\n",
      "94\n",
      "87\n",
      "93\n",
      "75\n",
      "88\n",
      "90\n",
      "90\n",
      "86\n",
      "92\n",
      "90\n",
      "89\n",
      "89\n",
      "87\n",
      "89\n",
      "89\n",
      "86\n",
      "87\n",
      "87\n",
      "87\n",
      "87\n",
      "87\n",
      "87\n",
      "88\n",
      "87\n",
      "91\n",
      "89\n",
      "97\n",
      "92\n",
      "92\n",
      "88\n",
      "93\n",
      "94\n",
      "90\n",
      "88\n",
      "90\n",
      "89\n",
      "85\n",
      "85\n",
      "93\n",
      "92\n",
      "94\n",
      "92\n",
      "92\n",
      "90\n",
      "93\n",
      "93\n",
      "87\n",
      "90\n",
      "89\n",
      "91\n",
      "96\n",
      "96\n",
      "95\n",
      "95\n",
      "96\n",
      "97\n",
      "89\n",
      "95\n",
      "87\n",
      "92\n",
      "87\n",
      "90\n",
      "96\n",
      "96\n",
      "94\n",
      "89\n",
      "95\n",
      "75\n",
      "92\n",
      "93\n",
      "90\n",
      "88\n",
      "93\n",
      "90\n",
      "92\n",
      "90\n",
      "89\n",
      "92\n",
      "92\n",
      "86\n",
      "88\n",
      "89\n",
      "90\n",
      "88\n",
      "87\n",
      "89\n",
      "89\n",
      "89\n",
      "91\n",
      "90\n",
      "94\n",
      "94\n",
      "93\n",
      "90\n",
      "94\n",
      "96\n",
      "89\n",
      "89\n",
      "94\n",
      "89\n",
      "85\n",
      "94\n",
      "96\n",
      "95\n",
      "97\n",
      "96\n",
      "96\n",
      "97\n",
      "91\n",
      "93\n",
      "92\n",
      "85\n",
      "89\n",
      "87\n",
      "90\n",
      "94\n",
      "94\n",
      "93\n",
      "93\n",
      "94\n",
      "94\n",
      "91\n",
      "94\n",
      "90\n",
      "92\n",
      "90\n",
      "91\n",
      "94\n",
      "95\n",
      "93\n",
      "88\n",
      "94\n",
      "73\n",
      "92\n",
      "93\n",
      "88\n",
      "89\n",
      "92\n",
      "89\n",
      "94\n",
      "91\n",
      "88\n",
      "91\n",
      "92\n",
      "86\n",
      "88\n",
      "88\n",
      "89\n",
      "88\n",
      "87\n",
      "87\n",
      "89\n",
      "88\n",
      "91\n",
      "91\n",
      "92\n",
      "93\n",
      "92\n",
      "92\n",
      "93\n",
      "94\n",
      "90\n",
      "91\n",
      "93\n",
      "91\n",
      "88\n",
      "89\n",
      "94\n",
      "94\n",
      "95\n",
      "94\n",
      "92\n",
      "95\n",
      "91\n",
      "95\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "Figure 2: Similarity of MTEB datasets. We use the best model on MTEB STS (ST5-XXL, see Table 1) to embed\n",
      "100 samples for each dataset. Cosine similarities between the averaged embeddings are computed and visualized.\n",
      "Retrieval\n",
      "Each dataset consists of a corpus,\n",
      "queries and a mapping for each query to relevant\n",
      "documents from the corpus. The aim is to ﬁnd these\n",
      "relevant documents. The provided model is used\n",
      "to embed all queries and all corpus documents and\n",
      "similarity scores are computed using cosine simi-\n",
      "larity. After ranking the corpus documents for each\n",
      "query based on the scores, nDCG@k, MRR@k,\n",
      "MAP@k, precision@k and recall@k are computed\n",
      "for several values of k. nDCG@10 serves as the\n",
      "main metric. MTEB reuses datasets and evaluation\n",
      "from BEIR (Thakur et al., 2021).\n",
      "Semantic Textual Similarity (STS)\n",
      "Given a\n",
      "sentence pair the aim is to determine their simi-\n",
      "larity. Labels are continuous scores with higher\n",
      "numbers indicating more similar sentences. The\n",
      "provided model is used to embed the sentences and\n",
      "their similarity is computed using various distance\n",
      "metrics. Distances are benchmarked with ground\n",
      "truth similarities using Pearson and Spearman cor-\n",
      "relations. Spearman correlation based on cosine\n",
      "similarity serves as the main metric (Reimers et al.,\n",
      "2016).\n",
      "Summarization\n",
      "A set of human-written and\n",
      "machine-generated summaries are provided. The\n",
      "aim is to score the machine summaries. The pro-\n",
      "vided model is ﬁrst used to embed all summaries.\n",
      "For each machine summary embedding, distances\n",
      "to all human summary embeddings are computed.\n",
      "The closest score (e.g. highest cosine similarity)\n",
      "is kept and used as the model’s score of a single\n",
      "machine-generated summary. Pearson and Spear-\n",
      "man correlations with ground truth human assess-\n",
      "ments of the machine-generated summaries are\n",
      "computed. Like for STS, Spearman correlation\n",
      "based on cosine similarity serves as the main met-\n",
      "ric (Reimers et al., 2016).\n",
      "3.3\n",
      "Datasets\n",
      "To further the diversity of MTEB, datasets of vary-\n",
      "ing text lengths are included.\n",
      "All datasets are\n",
      "grouped into three categories:\n",
      "Sentence to sentence (S2S)\n",
      "A sentence is com-\n",
      "pared with another sentence. An example of S2S\n",
      "are all current STS tasks in MTEB, where the simi-\n",
      "larity between two sentences is assessed.\n",
      "Class.\n",
      "Clust.\n",
      "PairClass.\n",
      "Rerank.\n",
      "Retr.\n",
      "STS\n",
      "Summ.\n",
      "Avg.\n",
      "Num. Datasets (→)\n",
      "12\n",
      "11\n",
      "3\n",
      "4\n",
      "15\n",
      "10\n",
      "1\n",
      "56\n",
      "Self-supervised methods\n",
      "Glove\n",
      "57.29\n",
      "27.73\n",
      "70.92\n",
      "43.29\n",
      "21.62\n",
      "61.85\n",
      "28.87\n",
      "41.97\n",
      "Komninos\n",
      "57.65\n",
      "26.57\n",
      "72.94\n",
      "44.75\n",
      "21.22\n",
      "62.47\n",
      "30.49\n",
      "42.06\n",
      "BERT\n",
      "61.66\n",
      "30.12\n",
      "56.33\n",
      "43.44\n",
      "10.59\n",
      "54.36\n",
      "29.82\n",
      "38.33\n",
      "SimCSE-BERT-unsup\n",
      "62.50\n",
      "29.04\n",
      "70.33\n",
      "46.47\n",
      "20.29\n",
      "74.33\n",
      "31.15\n",
      "45.45\n",
      "Supervised methods\n",
      "SimCSE-BERT-sup\n",
      "67.32\n",
      "33.43\n",
      "73.68\n",
      "47.54\n",
      "21.82\n",
      "79.12\n",
      "23.31\n",
      "48.72\n",
      "coCondenser-msmarco\n",
      "64.71\n",
      "37.64\n",
      "81.74\n",
      "51.84\n",
      "32.96\n",
      "76.47\n",
      "29.50\n",
      "52.35\n",
      "Contriever\n",
      "66.68\n",
      "41.10\n",
      "82.53\n",
      "53.14\n",
      "41.88\n",
      "76.51\n",
      "30.36\n",
      "56.00\n",
      "SPECTER\n",
      "52.37\n",
      "34.06\n",
      "61.37\n",
      "48.10\n",
      "15.88\n",
      "61.02\n",
      "27.66\n",
      "40.28\n",
      "LaBSE\n",
      "62.71\n",
      "29.55\n",
      "78.87\n",
      "48.42\n",
      "18.99\n",
      "70.80\n",
      "31.05\n",
      "45.21\n",
      "LASER2\n",
      "53.65\n",
      "15.28\n",
      "68.86\n",
      "41.44\n",
      "7.93\n",
      "55.32\n",
      "26.80\n",
      "33.63\n",
      "MiniLM-L6\n",
      "63.06\n",
      "42.35\n",
      "82.37\n",
      "58.04\n",
      "41.95\n",
      "78.90\n",
      "30.81\n",
      "56.26\n",
      "MiniLM-L12\n",
      "63.21\n",
      "41.81\n",
      "82.41\n",
      "58.44\n",
      "42.69\n",
      "79.80\n",
      "27.90\n",
      "56.53\n",
      "MiniLM-L12-multilingual\n",
      "64.30\n",
      "37.14\n",
      "78.45\n",
      "53.62\n",
      "32.45\n",
      "78.92\n",
      "30.67\n",
      "52.44\n",
      "MPNet\n",
      "65.07\n",
      "43.69\n",
      "83.04\n",
      "59.36\n",
      "43.81\n",
      "80.28\n",
      "27.49\n",
      "57.78\n",
      "MPNet-multilingual\n",
      "67.91\n",
      "38.40\n",
      "80.81\n",
      "53.80\n",
      "35.34\n",
      "80.73\n",
      "31.57\n",
      "54.71\n",
      "OpenAI Ada Similarity\n",
      "70.44\n",
      "37.52\n",
      "76.86\n",
      "49.02\n",
      "18.36\n",
      "78.60\n",
      "26.94\n",
      "49.52\n",
      "SGPT-125M-nli\n",
      "61.46\n",
      "30.95\n",
      "71.78\n",
      "47.56\n",
      "20.90\n",
      "74.71\n",
      "30.26\n",
      "45.97\n",
      "SGPT-5.8B-nli\n",
      "70.14\n",
      "36.98\n",
      "77.03\n",
      "52.33\n",
      "32.34\n",
      "80.53\n",
      "30.38\n",
      "53.74\n",
      "SGPT-125M-msmarco\n",
      "60.72\n",
      "35.79\n",
      "75.23\n",
      "50.58\n",
      "37.04\n",
      "73.41\n",
      "28.90\n",
      "51.23\n",
      "SGPT-1.3B-msmarco\n",
      "66.52\n",
      "39.92\n",
      "79.58\n",
      "54.00\n",
      "44.49\n",
      "75.74\n",
      "25.44\n",
      "56.11\n",
      "SGPT-2.7B-msmarco\n",
      "67.13\n",
      "39.83\n",
      "80.65\n",
      "54.67\n",
      "46.54\n",
      "76.83\n",
      "27.87\n",
      "57.12\n",
      "SGPT-5.8B-msmarco\n",
      "68.13\n",
      "40.35\n",
      "82.00\n",
      "56.56\n",
      "50.25\n",
      "78.10\n",
      "24.75\n",
      "58.81\n",
      "SGPT-BLOOM-7.1B-msmarco\n",
      "66.19\n",
      "38.93\n",
      "81.90\n",
      "55.65\n",
      "48.21\n",
      "77.74\n",
      "24.99\n",
      "57.44\n",
      "GTR-Base\n",
      "65.25\n",
      "38.63\n",
      "83.85\n",
      "54.23\n",
      "44.67\n",
      "77.07\n",
      "29.67\n",
      "56.19\n",
      "GTR-Large\n",
      "67.14\n",
      "41.60\n",
      "85.33\n",
      "55.36\n",
      "47.42\n",
      "78.19\n",
      "29.50\n",
      "58.28\n",
      "GTR-XL\n",
      "67.11\n",
      "41.51\n",
      "86.13\n",
      "55.96\n",
      "47.96\n",
      "77.80\n",
      "30.21\n",
      "58.42\n",
      "GTR-XXL\n",
      "67.41\n",
      "42.42\n",
      "86.12\n",
      "56.65\n",
      "48.48\n",
      "78.38\n",
      "30.64\n",
      "58.97\n",
      "ST5-Base\n",
      "69.81\n",
      "40.21\n",
      "85.17\n",
      "53.09\n",
      "33.63\n",
      "81.14\n",
      "31.39\n",
      "55.27\n",
      "ST5-Large\n",
      "72.31\n",
      "41.65\n",
      "84.97\n",
      "54.00\n",
      "36.71\n",
      "81.83\n",
      "29.64\n",
      "57.06\n",
      "ST5-XL\n",
      "72.84\n",
      "42.34\n",
      "86.06\n",
      "54.71\n",
      "38.47\n",
      "81.66\n",
      "29.91\n",
      "57.87\n",
      "ST5-XXL\n",
      "73.42\n",
      "43.71\n",
      "85.06\n",
      "56.43\n",
      "42.24\n",
      "82.63\n",
      "30.08\n",
      "59.51\n",
      "Table 1: Average of the main metric (see Section 3.2) per task per model on MTEB English subsets.\n",
      "Paragraph to paragraph (P2P)\n",
      "A paragraph is\n",
      "compared with another paragraph. MTEB imposes\n",
      "no limit on the input length, leaving it up to the\n",
      "models to truncate if necessary. Several clustering\n",
      "tasks are framed as both S2S and P2P tasks. The\n",
      "former only compare titles, while the latter include\n",
      "both title and content. For ArxivClustering, for\n",
      "example, abstracts are concatenated to the title in\n",
      "the P2P setting.\n",
      "Sentence to paragraph (S2P)\n",
      "A few retrieval\n",
      "datasets are mixed in a S2P setting. Here a query\n",
      "is a single sentence, while documents are long\n",
      "paragraphs consisting of multiple sentences.\n",
      "Similarities across 56 MTEB datasets are vi-\n",
      "sualized in Figure 2.\n",
      "Several datasets rely on\n",
      "the same corpora, such as ClimateFEVER and\n",
      "FEVER, resulting in a score of 1. Clusters of simi-\n",
      "lar datasets can be seen among CQADupstack vari-\n",
      "ations and STS datasets. S2S and P2P variations of\n",
      "the same dataset tend to also be similar. Scientiﬁc\n",
      "datasets, such as SciDocsRR, SciFact, ArxivClus-\n",
      "tering, show high similarities among each other\n",
      "even when coming from different tasks (Reranking,\n",
      "Retrieval and Clustering in this case).\n",
      "4\n",
      "Results\n",
      "4.1\n",
      "Models\n",
      "We evaluate on the test splits of all datasets except\n",
      "for MSMARCO, where the dev split is used follow-\n",
      "ing Thakur et al. (2021). We benchmark models\n",
      "claiming state-of-the-art results on various embed-\n",
      "0.1B\n",
      "1B\n",
      "2B\n",
      "4B\n",
      "Model Parameters (Billions)\n",
      "0.62\n",
      "0.64\n",
      "0.66\n",
      "0.68\n",
      "0.70\n",
      "0.72\n",
      "0.74\n",
      "Average Performance (accuracy)\n",
      "Classification\n",
      "0.1B\n",
      "1B\n",
      "2B\n",
      "4B\n",
      "Model Parameters (Billions)\n",
      "0.36\n",
      "0.37\n",
      "0.38\n",
      "0.39\n",
      "0.40\n",
      "0.41\n",
      "0.42\n",
      "0.43\n",
      "0.44\n",
      "Average Performance (v_measure)\n",
      "Clustering\n",
      "0.1B\n",
      "1B\n",
      "2B\n",
      "4B\n",
      "Model Parameters (Billions)\n",
      "0.76\n",
      "0.78\n",
      "0.80\n",
      "0.82\n",
      "0.84\n",
      "0.86\n",
      "Average Performance (ap)\n",
      "PairClassification\n",
      "0.1B\n",
      "1B\n",
      "2B\n",
      "4B\n",
      "Model Parameters (Billions)\n",
      "0.51\n",
      "0.52\n",
      "0.53\n",
      "0.54\n",
      "0.55\n",
      "0.56\n",
      "Average Performance (map)\n",
      "Reranking\n",
      "0.1B\n",
      "1B\n",
      "2B\n",
      "4B\n",
      "Model Parameters (Billions)\n",
      "0.350\n",
      "0.375\n",
      "0.400\n",
      "0.425\n",
      "0.450\n",
      "0.475\n",
      "0.500\n",
      "Average Performance (nDCG@10)\n",
      "Retrieval\n",
      "0.1B\n",
      "1B\n",
      "2B\n",
      "4B\n",
      "Model Parameters (Billions)\n",
      "0.74\n",
      "0.76\n",
      "0.78\n",
      "0.80\n",
      "0.82\n",
      "Average Performance (cos. sim. spearman corr.)\n",
      "STS\n",
      "GTR\n",
      "ST5\n",
      "SGPT\n",
      "Figure 3:\n",
      "MTEB performance scales with model\n",
      "size. The smallest SGPT variant underperforms similar-\n",
      "sized GTR and ST5 variants. This may be due to the\n",
      "bias-only ﬁne-tuning SGPT employs, which catches\n",
      "up with full ﬁne-tuning only as model size and thus\n",
      "the number of bias parameters increases (Muennighoff,\n",
      "2022).\n",
      "ding tasks leading to a high representation of trans-\n",
      "formers (Vaswani et al., 2017). We group models\n",
      "into self-supervised and supervised methods.\n",
      "Self-supervised\n",
      "methods\n",
      "(a)\n",
      "Transformer-\n",
      "based BERT (Devlin et al., 2018) is trained using\n",
      "self-supervised mask and sentence prediction tasks.\n",
      "By taking the mean across the sequence length\n",
      "(mean-pooling) the model can directly be used\n",
      "to produce text embeddings.\n",
      "SimCSE-Unsup\n",
      "(Gao et al., 2021b) uses BERT as a foundation\n",
      "and performs additional self-supervised training.\n",
      "(b) Non-transformer:\n",
      "Komninos (Komninos\n",
      "and Manandhar, 2016) and Glove (Pennington\n",
      "et al., 2014) are two word embedding models\n",
      "that directly map words to vectors. Hence, their\n",
      "embeddings lack context awareness, but provide\n",
      "signiﬁcant speed-ups.\n",
      "Supervised methods\n",
      "The original transformer\n",
      "model (Vaswani et al., 2017) consists of an encoder\n",
      "and decoder network. Subsequent transformers\n",
      "often train only encoders like BERT (Devlin et al.,\n",
      "2018) or decoders like GPT (Radford et al., 2019).\n",
      "(a) Transformer encoder methods coCon-\n",
      "denser (Gao and Callan, 2021), Contriever (Izac-\n",
      "ard et al., 2021), LaBSE (Feng et al., 2020) and\n",
      "SimCSE-BERT-sup (Gao et al., 2021b) are based\n",
      "on the pre-trained BERT model (Devlin et al.,\n",
      "2018). coCondenser and Contriever add a self-\n",
      "supervised stage prior to supervised ﬁne-tuning\n",
      "for a total of three training stages. LaBSE uses\n",
      "BERT to perform additional pre-training on par-\n",
      "allel data to produce a competitive bitext mining\n",
      "model. SPECTER (Cohan et al., 2020a) relies on\n",
      "the pre-trained SciBERT (Beltagy et al., 2019) vari-\n",
      "ant instead and ﬁne-tunes on citation graphs. GTR\n",
      "(Ni et al., 2021b) and ST5 (Ni et al., 2021a) are\n",
      "based on the encoder part of the T5 model (Raf-\n",
      "fel et al., 2020) and only differ in their ﬁne-tuning\n",
      "datasets. After additional self-supervised training,\n",
      "ST5 does contrastive ﬁne-tuning on NLI (Ni et al.,\n",
      "2021a; Gao et al., 2021b) being geared towards\n",
      "STS tasks. Meanwhile, GTR ﬁne-tunes on MS-\n",
      "MARCO and focuses on retrieval tasks. MPNet\n",
      "and MiniLM correspond to ﬁne-tuned embedding\n",
      "models (Reimers and Gurevych, 2019) of the pre-\n",
      "trained MPNet (Song et al., 2020) and MiniLM\n",
      "(Wang et al., 2020) models using diverse datasets\n",
      "to target any embedding use case.\n",
      "(b) Transformer decoder methods SGPT Bi-\n",
      "Encoders (Muennighoff, 2022) perform contrastive\n",
      "ﬁne-tuning of <0.1% of pre-trained parameters us-\n",
      "ing weighted-mean pooling. Similar to ST5 and\n",
      "GTR, SGPT-nli models are geared towards STS,\n",
      "while SGPT-msmarco models towards retrieval.\n",
      "SGPT-msmarco models embed queries and doc-\n",
      "uments for retrieval with different special tokens\n",
      "to help the model distinguish their role. For non-\n",
      "retrieval tasks, we use its query representations.\n",
      "We benchmark publicly available SGPT models\n",
      "based on GPT-NeoX (Andonian et al., 2021), GPT-\n",
      "J (Wang and Komatsuzaki, 2021) and BLOOM\n",
      "(Scao et al., 2022). Alternatively, cpt-text (Nee-\n",
      "lakantan et al., 2022) passes pre-trained GPT de-\n",
      "coders through a two-stage process using last token\n",
      "pooling to provide embeddings from decoders. We\n",
      "benchmark their models via the OpenAI Embed-\n",
      "dings API4.\n",
      "(c) Non-transformer LASER (Heffernan et al.,\n",
      "2022) is the only context aware non-transformer\n",
      "model we benchmark, relying on an LSTM\n",
      "4https://beta.openai.com/docs/guides/\n",
      "embeddings\n",
      "102\n",
      "103\n",
      "104\n",
      "Speed (examples per sec)\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "MTEB Score\n",
      "LASER2\n",
      "Komninos\n",
      "Glove\n",
      "SGPT-125M-nli\n",
      "SGPT-125M-msmarco\n",
      "SGPT-5.8B-nli\n",
      "SGPT-5.8B-msmarco\n",
      "MiniLM-L6\n",
      "MPNet\n",
      "ST5-Base\n",
      "ST5-XXL\n",
      "GTR-Base\n",
      "GTR-XXL\n",
      "Contriever\n",
      "coCondenser-msmarco\n",
      "BERT\n",
      "SimCSE-BERT-sup\n",
      "SimCSE-BERT-unsup\n",
      "LaBSE\n",
      "MiniLM-L12\n",
      "SPECTER\n",
      "Base Architecture\n",
      "LASER\n",
      "WordEmbeddings\n",
      "GPT\n",
      "MiniLM\n",
      "MPNet\n",
      "T5\n",
      "BERT\n",
      "SciBERT\n",
      "Figure 4: Performance, speed, and size of produced embeddings (size of the circles) of different embedding\n",
      "models. Embedding sizes range from 1.2 kB (Glove / Komninos) to 16.4 kB (SGPT-5.8B) per example. Speed\n",
      "was benchmarked on STS15 using 1x Nvidia A100 80GB with CUDA 11.6.\n",
      "(Hochreiter and Schmidhuber, 1997) instead. Simi-\n",
      "lar to LaBSE, the model trains on parallel data and\n",
      "focuses on bitext mining applications.\n",
      "4.2\n",
      "Analysis\n",
      "Based on the results in Table 1, we observe that\n",
      "there is considerable variability between tasks. No\n",
      "model claims the state-of-the-art in all seven En-\n",
      "glish tasks. There is even more variability in the\n",
      "results per dataset present in the appendix. Further,\n",
      "there remains a large gap between self-supervised\n",
      "and supervised methods. Self-supervised large lan-\n",
      "guage models have been able to close this gap in\n",
      "many natural language generation tasks (Chowd-\n",
      "hery et al., 2022). However, they appear to still\n",
      "require supervised ﬁne-tuning for competitive em-\n",
      "bedding performance.\n",
      "We ﬁnd that performance strongly correlates\n",
      "with model size, see Figure 3.\n",
      "A majority of\n",
      "MTEB tasks are dominated by multi-billion param-\n",
      "eter models. However, these come at a signiﬁcant\n",
      "cost as we investigate in Section 4.3.\n",
      "Classiﬁcation\n",
      "ST5 models dominate the classiﬁ-\n",
      "cation task across most datasets, as can be seen in\n",
      "detail in the full results in the appendix. ST5-XXL\n",
      "has the highest average performance, 3% ahead of\n",
      "the best non-ST5 model, OpenAI Ada Similarity.\n",
      "Clustering\n",
      "Despite being almost 50x smaller, the\n",
      "MPNet embedding model is on par with the ST5-\n",
      "XXL state-of-the-art on Clustering. This may be\n",
      "due to the large variety of datasets MPNet (and\n",
      "MiniLM) has been ﬁne-tuned on. Clustering re-\n",
      "quires coherent distances between a large number\n",
      "of embeddings. Models like SimCSE-sup or SGPT-\n",
      "nli, which are only ﬁne-tuned on a single dataset,\n",
      "NLI, may produce incoherent embeddings when\n",
      "encountering topics unseen during ﬁne-tuning. Re-\n",
      "latedly, we ﬁnd that the query embeddings of SGPT-\n",
      "msmarco and the Ada Search endpoint are competi-\n",
      "tive with SGPT-nli and the Ada Similarity endpoint,\n",
      "respectively. We refer to the public leaderboard5\n",
      "for Ada Search results. This could be due to the\n",
      "MSMARCO dataset being signiﬁcantly larger than\n",
      "NLI. Thus, while the OpenAI docs recommend us-\n",
      "ing the similarity embeddings for clustering use\n",
      "cases6, the retrieval query embeddings may be the\n",
      "better choice in some cases.\n",
      "5https://huggingface.co/spaces/mteb/l\n",
      "eaderboard\n",
      "6https://beta.openai.com/docs/guides/\n",
      "embeddings/similarity-embeddings\n",
      "deu-eng\n",
      "mal-eng\n",
      "nob-eng\n",
      "spa-eng\n",
      "epo-eng\n",
      "tur-eng\n",
      "tel-eng\n",
      "pol-eng\n",
      "vie-eng\n",
      "hrv-eng\n",
      "ron-eng\n",
      "hin-eng\n",
      "glg-eng\n",
      "sqi-eng\n",
      "ces-eng\n",
      "est-eng\n",
      "hun-eng\n",
      "slk-eng\n",
      "lit-eng\n",
      "fin-eng\n",
      "afr-eng\n",
      "tha-eng\n",
      "nld-eng\n",
      "slv-eng\n",
      "tgl-eng\n",
      "mon-eng\n",
      "lvs-eng\n",
      "dan-eng\n",
      "swe-eng\n",
      "zsm-eng\n",
      "cat-eng\n",
      "jpn-eng\n",
      "ina-eng\n",
      "ell-eng\n",
      "cmn-eng\n",
      "kat-eng\n",
      "eus-eng\n",
      "bel-eng\n",
      "aze-eng\n",
      "bos-eng\n",
      "fra-eng\n",
      "isl-eng\n",
      "pes-eng\n",
      "bul-eng\n",
      "nno-eng\n",
      "srp-eng\n",
      "por-eng\n",
      "hye-eng\n",
      "ukr-eng\n",
      "gle-eng\n",
      "rus-eng\n",
      "ind-eng\n",
      "mkd-eng\n",
      "urd-eng\n",
      "ita-eng\n",
      "mar-eng\n",
      "uig-eng\n",
      "cym-eng\n",
      "xho-eng\n",
      "heb-eng\n",
      "amh-eng\n",
      "kor-eng\n",
      "ast-eng\n",
      "wuu-eng\n",
      "yue-eng\n",
      "ido-eng\n",
      "fry-eng\n",
      "tam-eng\n",
      "ara-eng\n",
      "yid-eng\n",
      "ben-eng\n",
      "kaz-eng\n",
      "fao-eng\n",
      "tat-eng\n",
      "gla-eng\n",
      "ile-eng\n",
      "swh-eng\n",
      "uzb-eng\n",
      "kur-eng\n",
      "lat-eng\n",
      "jav-eng\n",
      "cbk-eng\n",
      "nds-eng\n",
      "khm-eng\n",
      "arz-eng\n",
      "tuk-eng\n",
      "nov-eng\n",
      "awa-eng\n",
      "lfn-eng\n",
      "hsb-eng\n",
      "oci-eng\n",
      "dsb-eng\n",
      "pms-eng\n",
      "ceb-eng\n",
      "max-eng\n",
      "war-eng\n",
      "swg-eng\n",
      "ang-eng\n",
      "tzl-eng\n",
      "csb-eng\n",
      "gsw-eng\n",
      "arq-eng\n",
      "orv-eng\n",
      "cha-eng\n",
      "mhr-eng\n",
      "bre-eng\n",
      "kzj-eng\n",
      "dtp-eng\n",
      "pam-eng\n",
      "cor-eng\n",
      "ber-eng\n",
      "kab-eng\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "F1 score\n",
      "LaBSE\n",
      "LASER2\n",
      "MiniLM-L12-multilingual\n",
      "MPNet-multilingual\n",
      "SGPT-BLOOM-7.1B-msmarco\n",
      "(a) Bitext Mining on Tatoeba\n",
      "en\n",
      "hi\n",
      "zh-CNpt\n",
      "id\n",
      "es\n",
      "th\n",
      "it\n",
      "fr\n",
      "ru\n",
      "de\n",
      "fa\n",
      "sv\n",
      "vi\n",
      "zh-TWnl\n",
      "ms\n",
      "da\n",
      "pl\n",
      "tr\n",
      "sq\n",
      "el\n",
      "ro\n",
      "hu\n",
      "sl\n",
      "ko\n",
      "fi\n",
      "ja\n",
      "nb\n",
      "ml\n",
      "lv\n",
      "he\n",
      "ur\n",
      "bn\n",
      "ar\n",
      "te\n",
      "af\n",
      "ta\n",
      "hy\n",
      "my\n",
      "az\n",
      "mn\n",
      "is\n",
      "kn\n",
      "tl\n",
      "jv\n",
      "sw\n",
      "ka\n",
      "km\n",
      "am\n",
      "cy\n",
      "zh\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "Accuracy\n",
      "(b) Multilingual Classiﬁcation\n",
      "ko\n",
      "fr\n",
      "es\n",
      "en\n",
      "ar\n",
      "it\n",
      "zh\n",
      "ru\n",
      "tr\n",
      "de\n",
      "pl\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "Cos. Sim. Spearman Corr.\n",
      "fr-en\n",
      "en-ar\n",
      "en-de\n",
      "it-en\n",
      "es-en\n",
      "nl-en\n",
      "pl-en\n",
      "zh-en\n",
      "en-tr\n",
      "es-it\n",
      "fr-pl\n",
      "de-fr\n",
      "de-en\n",
      "de-pl\n",
      "(c) Multi- and Crosslingual STS\n",
      "Figure 5: MTEB multilingual performance. Bitext mining is dominated by LaBSE, while classiﬁcation and STS\n",
      "results are mixed. SGPT-BLOOM-7B1-msmarco tends to perform well on the languages BLOOM has been pre-\n",
      "trained on, such as Chinese, French and Portuguese.\n",
      "Pair Classiﬁcation\n",
      "GTR-XL and GTR-XXL\n",
      "have the strongest performance. Pair classiﬁca-\n",
      "tion is closest to STS in its framing, yet models\n",
      "rank signiﬁcantly differently on the two tasks. This\n",
      "highlights the importance of benchmarking on a\n",
      "diverse set of tasks to avoid blindly reusing a model\n",
      "for a different task.\n",
      "Reranking\n",
      "MPNet and MiniLM models perform\n",
      "strongly on reranking tasks. On SciDocsRR (Co-\n",
      "han et al., 2020a) they perform far better than big-\n",
      "ger models, which is likely due to parts of Sci-\n",
      "DocsRR being included in their training data. Our\n",
      "scale of experiments and that of model pre-training\n",
      "make controlling for data contamination challeng-\n",
      "ing. Thus, we ignore overlap of MTEB datasets\n",
      "with model training datasets in MTEB scores. As\n",
      "long as enough datasets are averaged, we believe\n",
      "these effects to be insigniﬁcant.\n",
      "Retrieval\n",
      "SGPT-5.8B-msmarco is the best em-\n",
      "bedding model on the BEIR subset in MTEB\n",
      "as well as on the full BEIR benchmark (Thakur\n",
      "et al., 2021; Muennighoff, 2022). The even larger\n",
      "7.1B SGPT model making use of BLOOM (Scao\n",
      "et al., 2022) performs signiﬁcantly weaker, which\n",
      "is likely due to the multilinguality of BLOOM.\n",
      "Models geared towards STS (SimCSE, ST5, SGPT-\n",
      "nli) perform badly on retrieval tasks. Retrieval\n",
      "tasks are unique in that there are two distinct types\n",
      "of texts: Queries and documents (“asymmetric”),\n",
      "while other tasks only have a single type of text\n",
      "(“symmetric”).\n",
      "On the QuoraRetrieval dataset,\n",
      "which has been shown to be largely symmetric\n",
      "(Muennighoff, 2022), the playing ﬁeld is more\n",
      "even with SGPT-5.8B-nli outperforming SGPT-\n",
      "5.8B-msmarco, see Table 11.\n",
      "STS & Summarization\n",
      "Retrieval models (GTR,\n",
      "SGPT-msmarco) perform badly on STS, while ST5-\n",
      "XXL has the highest performance. This highlights\n",
      "the bifurcation of the ﬁeld into separate embedding\n",
      "models for retrieval (asymmetric) and similarity\n",
      "(symmetric) use cases (Muennighoff, 2022).\n",
      "4.3\n",
      "Efﬁciency\n",
      "We investigate the latency-performance trade-off\n",
      "of models in Figure 4. The graph allows for signiﬁ-\n",
      "cant elimination of model candidates in the model\n",
      "selection process. It brings model selection down\n",
      "to three clusters:\n",
      "Maximum speed\n",
      "Word Embedding models offer\n",
      "maximum speed with Glove taking the lead on both\n",
      "performance and speed, thus making the choice\n",
      "simple in this case.\n",
      "Maximum performance\n",
      "If latency is less impor-\n",
      "tant than performance, the left-hand side of the\n",
      "graph offers a cluster of highly performant, but\n",
      "slow models. Depending on the task at hand, GTR-\n",
      "XXL, ST5-XXL or SGPT-5.8B may be the right\n",
      "choice, see Section 4.2. SGPT-5.8B comes with\n",
      "the additional caveat of its high-dimensional em-\n",
      "beddings requiring more storage.\n",
      "Speed and performance\n",
      "The ﬁne-tuned MPNet\n",
      "and MiniLM models lead the middle cluster mak-\n",
      "ing the choice easy.\n",
      "4.4\n",
      "Multilinguality\n",
      "MTEB comes with 10 multilingual datasets across\n",
      "bitext mining, classiﬁcation and STS tasks. We in-\n",
      "vestigate performance on these in Figure 5. Tabular\n",
      "results can be found in Tables 12, 13 and 14.\n",
      "Bitext Mining\n",
      "LaBSE (Feng et al., 2020) per-\n",
      "forms strongly across a wide array of languages in\n",
      "bitext mining. Meanwhile, LASER2 shows high\n",
      "variance across different languages. While there\n",
      "are additional language-speciﬁc LASER2 models\n",
      "available for some of the languages we benchmark,\n",
      "we use the default multilingual LASER2 model\n",
      "for all languages. This is to provide a fair one-to-\n",
      "one comparison of models. In practice, however,\n",
      "the high variance of LASER2’s performance may\n",
      "be resolved by mixing its model variants. MP-\n",
      "Net, MiniLM and SGPT-BLOOM-7B1-msmarco\n",
      "perform poorly on languages they have not been\n",
      "pre-trained on, such as German for the latter.\n",
      "Classiﬁcation & STS\n",
      "On multilingual classiﬁ-\n",
      "cation and STS, the multilingual MPNet provides\n",
      "the overall strongest performance. It outperforms\n",
      "the slightly faster multilingual MiniLM on almost\n",
      "all languages.\n",
      "Both models have been trained\n",
      "on the same languages, thus bringing decision-\n",
      "making down to performance vs speed. SGPT-\n",
      "BLOOM-7B1-msmarco provides state-of-the-art\n",
      "performance on languages like Hindi, Portuguese,\n",
      "Chinese or French, which the model has seen ex-\n",
      "tensively during pre-training. It also performs com-\n",
      "petitively on languages like Russian or Japanese\n",
      "that unintentionally leaked into its pre-training\n",
      "data (Muennighoff et al., 2022). However, it is not\n",
      "much ahead of the much cheaper MPNet. LASER2\n",
      "performs consistently worse than other models.\n",
      "5\n",
      "Conclusion\n",
      "In this work, we presented the Massive Text Em-\n",
      "bedding Benchmark (MTEB). Consisting of 8 text\n",
      "embedding tasks with up to 15 datasets each and\n",
      "covering 112 languages, MTEB aims to provide re-\n",
      "liable embedding performance estimates. By open-\n",
      "sourcing MTEB alongside a leaderboard, we pro-\n",
      "vide a foundation for further pushing the state-of-\n",
      "the-art of available text embeddings.\n",
      "To introduce MTEB, we have conducted the\n",
      "most comprehensive benchmarking of text embed-\n",
      "dings to date. Through the course of close to 5,000\n",
      "experiments on over 30 different models, we have\n",
      "set up solid baselines for future research to build\n",
      "on. We found model performance on different tasks\n",
      "to vary strongly with no model claiming state-of-\n",
      "the-art on all tasks. Our studies on scaling behav-\n",
      "ior, model efﬁciency and multilinguality revealed\n",
      "various intricacies of models that should ease the\n",
      "decision-making process for future research or in-\n",
      "dustry applications of text embeddings.\n",
      "We welcome task, dataset or metric contributions\n",
      "to the MTEB codebase7 as well as additions to the\n",
      "leaderboard via our automatic submission format8.\n",
      "7https://github.com/embeddings-benchm\n",
      "ark/mteb\n",
      "8https://huggingface.co/spaces/mteb/l\n",
      "eaderboard\n",
      "Acknowledgments\n",
      "This work was granted access to the HPC resources\n",
      "of Institut du développement et des ressources en\n",
      "informatique scientiﬁque (IDRIS) du Centre na-\n",
      "tional de la recherche scientiﬁque (CNRS) under\n",
      "the allocation 2021-A0101012475 made by Grand\n",
      "équipement national de calcul intensif (GENCI). In\n",
      "particular, all the evaluations and data processing\n",
      "ran on the Jean Zay cluster of IDRIS, and we want\n",
      "to thank the IDRIS team for responsive support\n",
      "throughout the project, in particular Rémi Lacroix.\n",
      "We thank Douwe Kiela, Teven Le Scao and Nan-\n",
      "dan Thakur for feedback and suggestions.\n",
      "References\n",
      "Charu C Aggarwal and ChengXiang Zhai. 2012.\n",
      "A\n",
      "survey of text clustering algorithms. In Mining text\n",
      "data, pages 77–128. Springer.\n",
      "Eneko Agirre, Carmen Banea, Claire Cardie, Daniel\n",
      "Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\n",
      "Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada\n",
      "Mihalcea, et al. 2015. Semeval-2015 task 2: Seman-\n",
      "tic textual similarity, english, spanish and pilot on\n",
      "interpretability. In Proceedings of the 9th interna-\n",
      "tional workshop on semantic evaluation (SemEval\n",
      "2015), pages 252–263.\n",
      "Eneko Agirre, Carmen Banea, Claire Cardie, Daniel M\n",
      "Cer, Mona T Diab, Aitor Gonzalez-Agirre, Weiwei\n",
      "Guo, Rada Mihalcea, German Rigau, and Janyce\n",
      "Wiebe. 2014. Semeval-2014 task 10: Multilingual\n",
      "semantic textual similarity. In SemEval@ COLING,\n",
      "pages 81–91.\n",
      "Eneko Agirre, Carmen Banea, Daniel Cer, Mona\n",
      "Diab, Aitor Gonzalez Agirre, Rada Mihalcea, Ger-\n",
      "man Rigau Claramunt, and Janyce Wiebe. 2016.\n",
      "Semeval-2016 task 1:\n",
      "Semantic textual similar-\n",
      "ity, monolingual and cross-lingual evaluation.\n",
      "In\n",
      "SemEval-2016. 10th International Workshop on Se-\n",
      "mantic Evaluation; 2016 Jun 16-17; San Diego, CA.\n",
      "Stroudsburg (PA): ACL; 2016. p. 497-511. ACL (As-\n",
      "sociation for Computational Linguistics).\n",
      "Eneko Agirre, Daniel Cer, Mona Diab, and Aitor\n",
      "Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-\n",
      "lot on semantic textual similarity. In * SEM 2012:\n",
      "The First Joint Conference on Lexical and Compu-\n",
      "tational Semantics–Volume 1: Proceedings of the\n",
      "main conference and the shared task, and Volume\n",
      "2: Proceedings of the Sixth International Workshop\n",
      "on Semantic Evaluation (SemEval 2012), pages 385–\n",
      "393.\n",
      "Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-\n",
      "Agirre, and Weiwei Guo. 2013. * sem 2013 shared\n",
      "task: Semantic textual similarity. In Second joint\n",
      "conference on lexical and computational semantics\n",
      "(* SEM), volume 1: proceedings of the Main confer-\n",
      "ence and the shared task: semantic textual similar-\n",
      "ity, pages 32–43.\n",
      "Loubna Ben Allal, Raymond Li, Denis Kocetkov,\n",
      "Chenghao Mou, Christopher Akiki, Carlos Munoz\n",
      "Ferrandis, Niklas Muennighoff, Mayank Mishra,\n",
      "Alex Gu,\n",
      "Manan Dey,\n",
      "et al. 2023.\n",
      "Santa-\n",
      "coder: don’t reach for the stars!\n",
      "arXiv preprint\n",
      "arXiv:2301.03988.\n",
      "Alex Andonian, Quentin Anthony, Stella Biderman,\n",
      "Sid Black, Preetham Gali, Leo Gao, Eric Hallahan,\n",
      "Josh Levy-Kramer, Connor Leahy, Lucas Nestler,\n",
      "Kip Parker, Michael Pieler, Shivanshu Purohit, Tri\n",
      "Songz, Phil Wang, and Samuel Weinbach. 2021.\n",
      "GPT-NeoX: Large scale autoregressive language\n",
      "modeling in pytorch.\n",
      "Dimo Angelov. 2020. Top2vec: Distributed representa-\n",
      "tions of topics. arXiv preprint arXiv:2008.09470.\n",
      "Akari Asai, Jungo Kasai, Jonathan H Clark, Kenton\n",
      "Lee, Eunsol Choi, and Hannaneh Hajishirzi. 2020.\n",
      "Xor qa: Cross-lingual open-retrieval question an-\n",
      "swering. arXiv preprint arXiv:2010.11856.\n",
      "Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scib-\n",
      "ert: A pretrained language model for scientiﬁc text.\n",
      "arXiv preprint arXiv:1903.10676.\n",
      "Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-\n",
      "mann, Trevor Cai, Eliza Rutherford, Katie Milli-\n",
      "can, George Bm Van Den Driessche, Jean-Baptiste\n",
      "Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\n",
      "Improving language models by retrieving from tril-\n",
      "lions of tokens. In International Conference on Ma-\n",
      "chine Learning, pages 2206–2240. PMLR.\n",
      "Micael Carvalho, Rémi Cadène, David Picard, Laure\n",
      "Soulier, Nicolas Thome, and Matthieu Cord. 2018.\n",
      "Cross-modal retrieval in the cooking context: Learn-\n",
      "ing semantic text-image embeddings. In The 41st\n",
      "International ACM SIGIR Conference on Research\n",
      "& Development in Information Retrieval, pages 35–\n",
      "44.\n",
      "Iñigo Casanueva,\n",
      "Tadas Temˇcinas,\n",
      "Daniela Gerz,\n",
      "Matthew Henderson, and Ivan Vuli´c. 2020. Efﬁcient\n",
      "intent detection with dual sentence encoders.\n",
      "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\n",
      "Maarten Bosma, Gaurav Mishra, Adam Roberts,\n",
      "Paul Barham, Hyung Won Chung, Charles Sutton,\n",
      "Sebastian Gehrmann, et al. 2022.\n",
      "Palm: Scaling\n",
      "language modeling with pathways. arXiv preprint\n",
      "arXiv:2204.02311.\n",
      "Jonathan H Clark, Eunsol Choi, Michael Collins, Dan\n",
      "Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and\n",
      "Jennimaria Palomaki. 2020. Tydi qa: A benchmark\n",
      "for information-seeking question answering in typo-\n",
      "logically diverse languages. Transactions of the As-\n",
      "sociation for Computational Linguistics, 8:454–470.\n",
      "Arman Cohan, Sergey Feldman, Iz Beltagy, Doug\n",
      "Downey, and Daniel S Weld. 2020a.\n",
      "Specter:\n",
      "Document-level\n",
      "representation\n",
      "learning\n",
      "using\n",
      "citation-informed transformers.\n",
      "arXiv preprint\n",
      "arXiv:2004.07180.\n",
      "Arman Cohan, Sergey Feldman, Iz Beltagy, Doug\n",
      "Downey, and Daniel S. Weld. 2020b.\n",
      "Specter:\n",
      "Document-level\n",
      "representation\n",
      "learning\n",
      "using\n",
      "citation-informed transformers.\n",
      "Alexis Conneau and Douwe Kiela. 2018. Senteval: An\n",
      "evaluation toolkit for universal sentence representa-\n",
      "tions. arXiv preprint arXiv:1803.05449.\n",
      "Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic\n",
      "Barrault, and Antoine Bordes. 2017.\n",
      "Supervised\n",
      "learning of universal sentence representations from\n",
      "natural language inference data.\n",
      "arXiv preprint\n",
      "arXiv:1705.02364.\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n",
      "Kristina Toutanova. 2018. Bert: Pre-training of deep\n",
      "bidirectional transformers for language understand-\n",
      "ing. arXiv preprint arXiv:1810.04805.\n",
      "Alexander R. Fabbri, Wojciech Kry´sci´nski, Bryan\n",
      "McCann, Caiming Xiong, Richard Socher, and\n",
      "Dragomir Radev. 2020. Summeval: Re-evaluating\n",
      "summarization evaluation.\n",
      "Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen\n",
      "Arivazhagan, and Wei Wang. 2020.\n",
      "Language-\n",
      "agnostic bert sentence embedding. arXiv preprint\n",
      "arXiv:2007.01852.\n",
      "Jack FitzGerald, Christopher Hench, Charith Peris,\n",
      "Scott Mackie, Kay Rottmann, Ana Sanchez, Aaron\n",
      "Nash, Liam Urbach, Vishesh Kakarala, Richa Singh,\n",
      "Swetha Ranganath, Laurie Crist, Misha Britan,\n",
      "Wouter Leeuwis, Gokhan Tur, and Prem Natara-\n",
      "jan. 2022.\n",
      "Massive:\n",
      "A 1m-example multilin-\n",
      "gual natural language understanding dataset with 51\n",
      "typologically-diverse languages.\n",
      "Leo Gao, Jonathan Tow, Stella Biderman, Sid Black,\n",
      "Anthony DiPoﬁ, Charles Foster, Laurence Golding,\n",
      "Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,\n",
      "et al. 2021a. A framework for few-shot language\n",
      "model evaluation. Version v0. 0.1. Sept.\n",
      "Luyu Gao and Jamie Callan. 2021. Unsupervised cor-\n",
      "pus aware language model pre-training for dense\n",
      "passage retrieval. arXiv preprint arXiv:2108.05540.\n",
      "Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021b.\n",
      "Simcse: Simple contrastive learning of sentence em-\n",
      "beddings. arXiv preprint arXiv:2104.08821.\n",
      "Gregor Geigle, Nils Reimers, Andreas Rücklé, and\n",
      "Iryna Gurevych. 2021. Tweac: Transformer with ex-\n",
      "tendable qa agent classiﬁers.\n",
      "Kevin Heffernan, Onur Çelebi, and Holger Schwenk.\n",
      "2022.\n",
      "Bitext mining using distilled sentence rep-\n",
      "resentations for low-resource languages.\n",
      "arXiv\n",
      "preprint arXiv:2205.12654.\n",
      "Sepp Hochreiter and Jürgen Schmidhuber. 1997.\n",
      "Long short-term memory.\n",
      "Neural computation,\n",
      "9(8):1735–1780.\n",
      "Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia,\n",
      "David Zhang, Philip Pronin, Janani Padmanab-\n",
      "han, Giuseppe Ottaviano, and Linjun Yang. 2020.\n",
      "Embedding-based retrieval in facebook search. In\n",
      "Proceedings of the 26th ACM SIGKDD Interna-\n",
      "tional Conference on Knowledge Discovery & Data\n",
      "Mining, pages 2553–2561.\n",
      "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\n",
      "Allamanis, and Marc Brockschmidt. 2019.\n",
      "Code-\n",
      "searchnet challenge: Evaluating the state of seman-\n",
      "tic code search. arXiv preprint arXiv:1909.09436.\n",
      "Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\n",
      "bastian Riedel, Piotr Bojanowski, Armand Joulin,\n",
      "and Edouard Grave. 2021.\n",
      "Towards unsupervised\n",
      "dense information retrieval with contrastive learning.\n",
      "arXiv preprint arXiv:2112.09118.\n",
      "Alexandros Komninos and Suresh Manandhar. 2016.\n",
      "Dependency based embeddings for sentence classi-\n",
      "ﬁcation tasks. In Proceedings of the 2016 confer-\n",
      "ence of the North American chapter of the associa-\n",
      "tion for computational linguistics: human language\n",
      "technologies, pages 1490–1500.\n",
      "Wuwei Lan, Siyu Qiu, Hua He, and Wei Xu. 2017.\n",
      "A continuously growing dataset of sentential para-\n",
      "phrases. In Proceedings of The 2017 Conference on\n",
      "Empirical Methods on Natural Language Process-\n",
      "ing (EMNLP), pages 1235–1245. Association for\n",
      "Computational Linguistics.\n",
      "Quentin Lhoest, Albert Villanova del Moral, Yacine\n",
      "Jernite, Abhishek Thakur, Patrick von Platen, Suraj\n",
      "Patil, Julien Chaumond, Mariama Drame, Julien Plu,\n",
      "Lewis Tunstall, et al. 2021.\n",
      "Datasets: A commu-\n",
      "nity library for natural language processing. arXiv\n",
      "preprint arXiv:2109.02846.\n",
      "Haoran Li, Abhinav Arora, Shuohui Chen, Anchit\n",
      "Gupta, Sonal Gupta, and Yashar Mehdad. 2020.\n",
      "Mtop: A comprehensive multilingual task-oriented\n",
      "semantic parsing benchmark.\n",
      "Xueqing Liu, Chi Wang, Yue Leng, and ChengXiang\n",
      "Zhai. 2018. Linkso: a dataset for learning to retrieve\n",
      "similar question answer pairs on software develop-\n",
      "ment forums. In Proceedings of the 4th ACM SIG-\n",
      "SOFT International Workshop on NLP for Software\n",
      "Engineering, pages 2–5.\n",
      "Andrew L. Maas, Raymond E. Daly, Peter T. Pham,\n",
      "Dan Huang, Andrew Y. Ng, and Christopher Potts.\n",
      "2011. Learning word vectors for sentiment analy-\n",
      "sis. In Proceedings of the 49th Annual Meeting of\n",
      "the Association for Computational Linguistics: Hu-\n",
      "man Language Technologies, pages 142–150, Port-\n",
      "land, Oregon, USA. Association for Computational\n",
      "Linguistics.\n",
      "Julian McAuley and Jure Leskovec. 2013. Hidden fac-\n",
      "tors and hidden topics: Understanding rating dimen-\n",
      "sions with review text. RecSys ’13, New York, NY,\n",
      "USA. Association for Computing Machinery.\n",
      "Niklas Muennighoff. 2020.\n",
      "Vilio:\n",
      "State-of-the-art\n",
      "visio-linguistic models applied to hateful memes.\n",
      "arXiv preprint arXiv:2012.07788.\n",
      "Niklas Muennighoff. 2022.\n",
      "Sgpt:\n",
      "Gpt sentence\n",
      "embeddings for semantic search.\n",
      "arXiv preprint\n",
      "arXiv:2202.08904.\n",
      "Niklas Muennighoff, Thomas Wang, Lintang Sutawika,\n",
      "Adam Roberts, Stella Biderman, Teven Le Scao,\n",
      "M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hai-\n",
      "ley Schoelkopf, et al. 2022. Crosslingual general-\n",
      "ization through multitask ﬁnetuning. arXiv preprint\n",
      "arXiv:2211.01786.\n",
      "Pandu Nayak. 2019.\n",
      "Understanding searches better\n",
      "than ever before.\n",
      "Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford,\n",
      "Jesse Michael Han, Jerry Tworek, Qiming Yuan,\n",
      "Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al.\n",
      "2022. Text and code embeddings by contrastive pre-\n",
      "training. arXiv preprint arXiv:2201.10005.\n",
      "Jianmo Ni, Gustavo Hernández Ábrego, Noah Con-\n",
      "stant, Ji Ma, Keith B Hall, Daniel Cer, and Yinfei\n",
      "Yang. 2021a.\n",
      "Sentence-t5: Scalable sentence en-\n",
      "coders from pre-trained text-to-text models. arXiv\n",
      "preprint arXiv:2108.08877.\n",
      "Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\n",
      "tavo Hernández Ábrego, Ji Ma, Vincent Y Zhao,\n",
      "Yi Luan, Keith B Hall, Ming-Wei Chang, et al.\n",
      "2021b.\n",
      "Large dual encoders are generalizable re-\n",
      "trievers. arXiv preprint arXiv:2112.07899.\n",
      "Alex Nichol,\n",
      "Prafulla Dhariwal,\n",
      "Aditya Ramesh,\n",
      "Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya\n",
      "Sutskever, and Mark Chen. 2021.\n",
      "Glide:\n",
      "To-\n",
      "wards photorealistic image generation and editing\n",
      "with text-guided diffusion models. arXiv preprint\n",
      "arXiv:2112.10741.\n",
      "James O’Neill, Polina Rozenshtein, Ryuichi Kiryo,\n",
      "Motoko Kubota, and Danushka Bollegala. 2021. I\n",
      "wish i would have loved this one, but i didn’t – a\n",
      "multilingual dataset for counterfactual detection in\n",
      "product reviews.\n",
      "F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,\n",
      "B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\n",
      "R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,\n",
      "D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\n",
      "esnay. 2011.\n",
      "Scikit-learn:\n",
      "Machine learning in\n",
      "Python.\n",
      "Journal of Machine Learning Research,\n",
      "12:2825–2830.\n",
      "Jeffrey Pennington, Richard Socher, and Christopher D\n",
      "Manning. 2014. Glove: Global vectors for word rep-\n",
      "resentation. In Proceedings of the 2014 conference\n",
      "on empirical methods in natural language process-\n",
      "ing (EMNLP), pages 1532–1543.\n",
      "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\n",
      "Dario Amodei, Ilya Sutskever, et al. 2019.\n",
      "Lan-\n",
      "guage models are unsupervised multitask learners.\n",
      "OpenAI blog, 1(8):9.\n",
      "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\n",
      "Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\n",
      "Wei Li, Peter J Liu, et al. 2020. Exploring the limits\n",
      "of transfer learning with a uniﬁed text-to-text trans-\n",
      "former. J. Mach. Learn. Res., 21(140):1–67.\n",
      "Nils Reimers, Philip Beyer, and Iryna Gurevych. 2016.\n",
      "Task-oriented intrinsic evaluation of semantic tex-\n",
      "tual similarity.\n",
      "In Proceedings of COLING 2016,\n",
      "the 26th International Conference on Computational\n",
      "Linguistics: Technical Papers, pages 87–96.\n",
      "Nils Reimers and Iryna Gurevych. 2019.\n",
      "Sentence-\n",
      "bert:\n",
      "Sentence embeddings using siamese bert-\n",
      "networks. arXiv preprint arXiv:1908.10084.\n",
      "Facebook Research. Tatoeba multilingual test set.\n",
      "Andrew Rosenberg and Julia Hirschberg. 2007.\n",
      "V-\n",
      "measure: A conditional entropy-based external clus-\n",
      "ter evaluation measure. pages 410–420.\n",
      "Chitwan Saharia, William Chan, Saurabh Saxena, Lala\n",
      "Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed\n",
      "Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi,\n",
      "Rapha Gontijo Lopes, et al. 2022.\n",
      "Photorealistic\n",
      "text-to-image diffusion models with deep language\n",
      "understanding. arXiv preprint arXiv:2205.11487.\n",
      "Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang,\n",
      "Junlin Wu, and Yi-Shin Chen. 2018. CARER: Con-\n",
      "textualized affect representations for emotion recog-\n",
      "nition. In Proceedings of the 2018 Conference on\n",
      "Empirical Methods in Natural Language Processing,\n",
      "pages 3687–3697, Brussels, Belgium. Association\n",
      "for Computational Linguistics.\n",
      "Teven Le Scao, Angela Fan, Christopher Akiki, El-\n",
      "lie Pavlick,\n",
      "Suzana Ili´c,\n",
      "Daniel Hesslow,\n",
      "Ro-\n",
      "man Castagné, Alexandra Sasha Luccioni, François\n",
      "Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b-\n",
      "parameter open-access multilingual language model.\n",
      "arXiv preprint arXiv:2211.05100.\n",
      "Darsh Shah, Tao Lei, Alessandro Moschitti, Salva-\n",
      "tore Romeo, and Preslav Nakov. 2018.\n",
      "Adversar-\n",
      "ial domain adaptation for duplicate question detec-\n",
      "tion.\n",
      "In Proceedings of the 2018 Conference on\n",
      "Empirical Methods in Natural Language Processing,\n",
      "pages 1056–1063, Brussels, Belgium. Association\n",
      "for Computational Linguistics.\n",
      "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\n",
      "Yan Liu. 2020. Mpnet: Masked and permuted pre-\n",
      "training for language understanding.\n",
      "Advances in\n",
      "Neural Information Processing Systems, 33:16857–\n",
      "16867.\n",
      "Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\n",
      "Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\n",
      "Adam R Brown, Adam Santoro, Aditya Gupta,\n",
      "Adrià Garriga-Alonso, et al. 2022.\n",
      "Beyond the\n",
      "imitation game: Quantifying and extrapolating the\n",
      "capabilities of language models.\n",
      "arXiv preprint\n",
      "arXiv:2206.04615.\n",
      "Hao Tan and Mohit Bansal. 2019. Lxmert: Learning\n",
      "cross-modality encoder representations from trans-\n",
      "formers. arXiv preprint arXiv:1908.07490.\n",
      "Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\n",
      "hishek Srivastava, and Iryna Gurevych. 2021. Beir:\n",
      "A heterogenous benchmark for zero-shot evaluation\n",
      "of information retrieval models.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n",
      "Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\n",
      "Kaiser, and Illia Polosukhin. 2017. Attention is all\n",
      "you need. Advances in neural information process-\n",
      "ing systems, 30.\n",
      "Alex Wang,\n",
      "Yada Pruksachatkun,\n",
      "Nikita Nangia,\n",
      "Amanpreet Singh, Julian Michael, Felix Hill, Omer\n",
      "Levy, and Samuel Bowman. 2019.\n",
      "Superglue: A\n",
      "stickier benchmark for general-purpose language un-\n",
      "derstanding systems. Advances in neural informa-\n",
      "tion processing systems, 32.\n",
      "Alex Wang, Amanpreet Singh, Julian Michael, Felix\n",
      "Hill, Omer Levy, and Samuel R Bowman. 2018.\n",
      "Glue: A multi-task benchmark and analysis platform\n",
      "for natural language understanding. arXiv preprint\n",
      "arXiv:1804.07461.\n",
      "Ben Wang and Aran Komatsuzaki. 2021.\n",
      "GPT-J-\n",
      "6B: A 6 Billion Parameter Autoregressive Language\n",
      "Model. https://github.com/kingoflol\n",
      "z/mesh-transformer-jax.\n",
      "Kexin Wang, Nils Reimers, and Iryna Gurevych. 2021.\n",
      "Tsdae: Using transformer-based sequential denois-\n",
      "ing auto-encoder for unsupervised sentence embed-\n",
      "ding learning. arXiv preprint arXiv:2104.06979.\n",
      "Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\n",
      "Yang, and Ming Zhou. 2020. Minilm: Deep self-\n",
      "attention distillation for task-agnostic compression\n",
      "of pre-trained transformers. Advances in Neural In-\n",
      "formation Processing Systems, 33:5776–5788.\n",
      "Samuel Weinbach,\n",
      "Marco Bellagente,\n",
      "Constantin\n",
      "Eichenberg,\n",
      "Andrew\n",
      "Dai,\n",
      "Robert\n",
      "Baldock,\n",
      "Souradeep Nanda, Björn Deiseroth, Koen Oost-\n",
      "ermeijer,\n",
      "Hannah\n",
      "Teufel,\n",
      "and\n",
      "Andres\n",
      "Felipe\n",
      "Cruz-Salinas. 2022.\n",
      "M-vader: A model for dif-\n",
      "fusion with multimodal context.\n",
      "arXiv preprint\n",
      "arXiv:2212.02936.\n",
      "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien\n",
      "Chaumond, Clement Delangue, Anthony Moi, Pier-\n",
      "ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\n",
      "icz, et al. 2020. Transformers: State-of-the-art nat-\n",
      "ural language processing.\n",
      "In Proceedings of the\n",
      "2020 conference on empirical methods in natural\n",
      "language processing: system demonstrations, pages\n",
      "38–45.\n",
      "Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan\n",
      "Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie,\n",
      "Jianfeng Gao, Winnie Wu, et al. 2020.\n",
      "Mind: A\n",
      "large-scale dataset for news recommendation.\n",
      "In\n",
      "Proceedings of the 58th Annual Meeting of the Asso-\n",
      "ciation for Computational Linguistics, pages 3597–\n",
      "3606.\n",
      "Wei Xu, Chris Callison-Burch, and William B Dolan.\n",
      "2015. Semeval-2015 task 1: Paraphrase and seman-\n",
      "tic similarity in twitter (pit). In Proceedings of the\n",
      "9th international workshop on semantic evaluation\n",
      "(SemEval 2015), pages 1–11.\n",
      "Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo,\n",
      "Ehsan Kamalloo,\n",
      "David Alfonso-Hermelo,\n",
      "Xi-\n",
      "aoguang Li, Qun Liu, Mehdi Rezagholizadeh, and\n",
      "Jimmy Lin. 2022. Making a miracl: Multilingual in-\n",
      "formation retrieval across a continuum of languages.\n",
      "arXiv preprint arXiv:2210.09984.\n",
      "Jeffrey Zhu, Mingqin Li, Jason Li, and Cassandra\n",
      "Oduola. 2021.\n",
      "Bing delivers more contextualized\n",
      "search using quantized transformer inference on\n",
      "nvidia gpus in azure.\n",
      "Pierre Zweigenbaum, Serge Sharoff, and Reinhard\n",
      "Rapp. 2016. Towards preparation of the second bucc\n",
      "shared task: Detecting parallel sentences in compa-\n",
      "rable corpora. In Proceedings of the Ninth Workshop\n",
      "on Building and Using Comparable Corpora. Euro-\n",
      "pean Language Resources Association (ELRA), Por-\n",
      "toroz, Slovenia, pages 38–43.\n",
      "Pierre Zweigenbaum, Serge Sharoff, and Reinhard\n",
      "Rapp. 2017. Overview of the second bucc shared\n",
      "task: Spotting parallel sentences in comparable cor-\n",
      "pora. In Proceedings of the 10th Workshop on Build-\n",
      "ing and Using Comparable Corpora, pages 60–67.\n",
      "Pierre Zweigenbaum, Serge Sharoff, and Reinhard\n",
      "Rapp. 2018. Overview of the third bucc shared task:\n",
      "Spotting parallel sentences in comparable corpora.\n",
      "In Proceedings of 11th workshop on building and\n",
      "using comparable corpora, pages 39–42.\n",
      "A\n",
      "Datasets\n",
      "Table 2 provides a summary along with statistics of\n",
      "all MTEB tasks. In the following, we give a brief\n",
      "description of each dataset included in MTEB.\n",
      "A.1\n",
      "Clustering\n",
      "ArxivClusteringS2S,\n",
      "ArxivClusteringP2P,\n",
      "BiorxivClusteringS2S,\n",
      "BiorxivClusteringP2P,\n",
      "MedrxivClusteringP2P,\n",
      "MedrxivCluster-\n",
      "ingS2S\n",
      "These datasets are custom-made for\n",
      "MTEB using the public APIs from arXiv9 and\n",
      "bioRxiv/medRxiv10. For S2S datasets, the input\n",
      "text is simply the title of the paper, while for\n",
      "P2P the input text is the concatenation of the\n",
      "title and the abstract.\n",
      "The cluster labels are\n",
      "generated using categories given to the papers by\n",
      "humans. For bioRxiv and medRxiv this category\n",
      "is unique, but for arXiv multiple categories can\n",
      "be given to a single paper so we only use the\n",
      "ﬁrst one.\n",
      "For bioRxiv and medRxiv there is\n",
      "only one level of category (e.g.\n",
      "biochemistry,\n",
      "genetics, microbiology, etc.)\n",
      "hence we only\n",
      "perform clustering based on that label. For arXiv\n",
      "there is a main category and secondary category:\n",
      "for example \"cs.AI\" means the main category is\n",
      "Computer Science and the sub-category is AI,\n",
      "math.AG means the main category is Mathematics\n",
      "and the sub-category is Algrebraic Geometry etc.\n",
      "Hence, we create three types of splits:\n",
      "(a) Main category clustering\n",
      "Articles are only\n",
      "clustered based on the main category (Math,\n",
      "Physics, Computer Science etc.). This split evalu-\n",
      "ates coarse clustering capacity of a model.\n",
      "(b) Secondary category clustering within the\n",
      "same main category\n",
      "Articles are clustered\n",
      "based on their secondary category, but within a\n",
      "given main category, for example only Math papers\n",
      "that need to be clustered into Algebraic Geometry,\n",
      "Functional Analysis, Numerical Analysis etc. This\n",
      "split evaluates ﬁne-grained clustering capacity of a\n",
      "model, as differentiating some sub-categories can\n",
      "be very difﬁcult.\n",
      "(c) Secondary category clustering\n",
      "Articles are\n",
      "clustered based on their secondary category for all\n",
      "main categories, so the labels can be Number The-\n",
      "ory, Computational Complexity, Astrophysics of\n",
      "Galaxies etc. These splits evaluate ﬁne-grained\n",
      "9https://arxiv.org/help/api/\n",
      "10https://api.biorxiv.org/\n",
      "clustering capacity, as well as multi-scale capac-\n",
      "ities i.e. is a model able to both separate Maths\n",
      "from Physics as well as Probability from Algebraic\n",
      "Topology at the same time.\n",
      "For every dataset, split and strategy, we select\n",
      "subsets of all labels and then sample articles from\n",
      "those labels.\n",
      "This yields splits with a varying\n",
      "amount and size of clusters.\n",
      "RedditClustering\n",
      "(Geigle et al., 2021): Cluster-\n",
      "ing of titles from 199 subreddits. Clustering of 25\n",
      "splits, each with 10-50 classes, and each class with\n",
      "100 - 1000 sentences\n",
      "RedditClusteringP2P\n",
      "Dataset\n",
      "created\n",
      "for\n",
      "MTEB using available data from Reddit posts11.\n",
      "The task consists of clustering the concatenation of\n",
      "title+post according to their subreddit. It contains\n",
      "10 splits, with 10 and 100 clusters per split and\n",
      "1,000 to 100,000 posts.\n",
      "StackExchangeClustering\n",
      "(Geigle et al., 2021)\n",
      "Clustering of titles from 121 stackexchanges. Clus-\n",
      "tering of 25 splits, each with 10-50 classes, and\n",
      "each class with 100-1000 sentences.\n",
      "StackExchangeClusteringP2P\n",
      "Dataset created\n",
      "for MTEB using available data from StackEx-\n",
      "change posts12. The task consists of clustering\n",
      "the concatenation of title and post according to\n",
      "their subreddit. It contains 10 splits, with 10 to 100\n",
      "clusters and 5,000 to 10,000 posts per split.\n",
      "TwentyNewsgroupsClustering13\n",
      "Clustering of\n",
      "the 20 Newsgroups dataset, given titles of article\n",
      "the goal is to ﬁnd the newsgroup (20 in total). Con-\n",
      "tains 10 splits, each with 20 classes, with each split\n",
      "containing between 1,000 and 10,000 titles.\n",
      "A.2\n",
      "Classiﬁcation\n",
      "AmazonCounterfactual\n",
      "(O’Neill et al., 2021) A\n",
      "collection of Amazon customer reviews annotated\n",
      "for counterfactual detection pair classiﬁcation. For\n",
      "each review the label is either \"counterfactual\" or\n",
      "\"not-counterfactual\". This is a multilingual dataset\n",
      "with 4 available languages.\n",
      "11https://huggingface.co/datasets/sent\n",
      "ence-transformers/reddit-title-body\n",
      "12https://huggingface.co/datasets/flax\n",
      "-sentence-embeddings/stackexchange_title\n",
      "_body_jsonl\n",
      "13https://scikit-learn.org/0.19/datase\n",
      "ts/twenty_newsgroups.html\n",
      "Name\n",
      "Type\n",
      "Categ.\n",
      "#Lang.\n",
      "Train\n",
      "Dev\n",
      "Test\n",
      "Train avg.\n",
      "Dev avg.\n",
      "Test avg.\n",
      "Samples\n",
      "Samples\n",
      "Samples\n",
      "chars\n",
      "chars\n",
      "chars\n",
      "BUCC\n",
      "BitextMining\n",
      "s2s\n",
      "4\n",
      "0\n",
      "0\n",
      "641684\n",
      "0\n",
      "0\n",
      "101.3\n",
      "Tatoeba\n",
      "BitextMining\n",
      "s2s\n",
      "112\n",
      "0\n",
      "0\n",
      "2000\n",
      "0\n",
      "0\n",
      "39.4\n",
      "AmazonCounterfactualClassiﬁcation\n",
      "Classiﬁcation\n",
      "s2s\n",
      "4\n",
      "4018\n",
      "335\n",
      "670\n",
      "107.3\n",
      "109.2\n",
      "106.1\n",
      "AmazonPolarityClassiﬁcation\n",
      "Classiﬁcation\n",
      "p2p\n",
      "1\n",
      "3600000\n",
      "0\n",
      "400000\n",
      "431.6\n",
      "0\n",
      "431.4\n",
      "AmazonReviewsClassiﬁcation\n",
      "Classiﬁcation\n",
      "s2s\n",
      "6\n",
      "1200000\n",
      "30000\n",
      "30000\n",
      "160.5\n",
      "159.2\n",
      "160.4\n",
      "Banking77Classiﬁcation\n",
      "Classiﬁcation\n",
      "s2s\n",
      "1\n",
      "10003\n",
      "0\n",
      "3080\n",
      "59.5\n",
      "0\n",
      "54.2\n",
      "EmotionClassiﬁcation\n",
      "Classiﬁcation\n",
      "s2s\n",
      "1\n",
      "16000\n",
      "2000\n",
      "2000\n",
      "96.8\n",
      "95.3\n",
      "96.6\n",
      "ImdbClassiﬁcation\n",
      "Classiﬁcation\n",
      "p2p\n",
      "1\n",
      "25000\n",
      "0\n",
      "25000\n",
      "1325.1\n",
      "0\n",
      "1293.8\n",
      "MassiveIntentClassiﬁcation\n",
      "Classiﬁcation\n",
      "s2s\n",
      "51\n",
      "11514\n",
      "2033\n",
      "2974\n",
      "35.0\n",
      "34.8\n",
      "34.6\n",
      "MassiveScenarioClassiﬁcation\n",
      "Classiﬁcation\n",
      "s2s\n",
      "51\n",
      "11514\n",
      "2033\n",
      "2974\n",
      "35.0\n",
      "34.8\n",
      "34.6\n",
      "MTOPDomainClassiﬁcation\n",
      "Classiﬁcation\n",
      "s2s\n",
      "6\n",
      "15667\n",
      "2235\n",
      "4386\n",
      "36.6\n",
      "36.5\n",
      "36.8\n",
      "MTOPIntentClassiﬁcation\n",
      "Classiﬁcation\n",
      "s2s\n",
      "6\n",
      "15667\n",
      "2235\n",
      "4386\n",
      "36.6\n",
      "36.5\n",
      "36.8\n",
      "ToxicConversationsClassiﬁcation\n",
      "Classiﬁcation\n",
      "s2s\n",
      "1\n",
      "50000\n",
      "0\n",
      "50000\n",
      "298.8\n",
      "0\n",
      "296.6\n",
      "TweetSentimentExtractionClassiﬁcation\n",
      "Classiﬁcation\n",
      "s2s\n",
      "1\n",
      "27481\n",
      "0\n",
      "3534\n",
      "68.3\n",
      "0\n",
      "67.8\n",
      "ArxivClusteringP2P\n",
      "Clustering\n",
      "p2p\n",
      "1\n",
      "0\n",
      "0\n",
      "732723\n",
      "0\n",
      "0\n",
      "1009.9\n",
      "ArxivClusteringS2S\n",
      "Clustering\n",
      "s2s\n",
      "1\n",
      "0\n",
      "0\n",
      "732723\n",
      "0\n",
      "0\n",
      "74.0\n",
      "BiorxivClusteringP2P\n",
      "Clustering\n",
      "p2p\n",
      "1\n",
      "0\n",
      "0\n",
      "75000\n",
      "0\n",
      "0\n",
      "1666.2\n",
      "BiorxivClusteringS2S\n",
      "Clustering\n",
      "s2s\n",
      "1\n",
      "0\n",
      "0\n",
      "75000\n",
      "0\n",
      "0\n",
      "101.6\n",
      "MedrxivClusteringP2P\n",
      "Clustering\n",
      "p2p\n",
      "1\n",
      "0\n",
      "0\n",
      "37500\n",
      "0\n",
      "0\n",
      "1981.2\n",
      "MedrxivClusteringS2S\n",
      "Clustering\n",
      "s2s\n",
      "1\n",
      "0\n",
      "0\n",
      "37500\n",
      "0\n",
      "0\n",
      "114.7\n",
      "RedditClustering\n",
      "Clustering\n",
      "s2s\n",
      "1\n",
      "0\n",
      "420464\n",
      "420464\n",
      "0\n",
      "64.7\n",
      "64.7\n",
      "RedditClusteringP2P\n",
      "Clustering\n",
      "p2p\n",
      "1\n",
      "0\n",
      "0\n",
      "459399\n",
      "0\n",
      "0\n",
      "727.7\n",
      "StackExchangeClustering\n",
      "Clustering\n",
      "s2s\n",
      "1\n",
      "0\n",
      "417060\n",
      "373850\n",
      "0\n",
      "56.8\n",
      "57.0\n",
      "StackExchangeClusteringP2P\n",
      "Clustering\n",
      "p2p\n",
      "1\n",
      "0\n",
      "0\n",
      "75000\n",
      "0\n",
      "0\n",
      "1090.7\n",
      "TwentyNewsgroupsClustering\n",
      "Clustering\n",
      "s2s\n",
      "1\n",
      "0\n",
      "0\n",
      "59545\n",
      "0\n",
      "0\n",
      "32.0\n",
      "SprintDuplicateQuestions\n",
      "PairClassiﬁcation\n",
      "s2s\n",
      "1\n",
      "0\n",
      "101000\n",
      "101000\n",
      "0\n",
      "65.2\n",
      "67.9\n",
      "TwitterSemEval2015\n",
      "PairClassiﬁcation\n",
      "s2s\n",
      "1\n",
      "0\n",
      "0\n",
      "16777\n",
      "0\n",
      "0\n",
      "38.3\n",
      "TwitterURLCorpus\n",
      "PairClassiﬁcation\n",
      "s2s\n",
      "1\n",
      "0\n",
      "0\n",
      "51534\n",
      "0\n",
      "0\n",
      "79.5\n",
      "AskUbuntuDupQuestions\n",
      "Reranking\n",
      "s2s\n",
      "1\n",
      "0\n",
      "0\n",
      "2255\n",
      "0\n",
      "0\n",
      "52.5\n",
      "MindSmallReranking\n",
      "Reranking\n",
      "s2s\n",
      "1\n",
      "231530\n",
      "0\n",
      "107968\n",
      "69.0\n",
      "0\n",
      "70.9\n",
      "SciDocsRR\n",
      "Reranking\n",
      "s2s\n",
      "1\n",
      "0\n",
      "19594\n",
      "19599\n",
      "0\n",
      "69.4\n",
      "69.0\n",
      "StackOverﬂowDupQuestions\n",
      "Reranking\n",
      "s2s\n",
      "1\n",
      "23018\n",
      "3467\n",
      "3467\n",
      "49.6\n",
      "49.8\n",
      "49.8\n",
      "ArguAna\n",
      "Retrieval\n",
      "p2p\n",
      "1\n",
      "0\n",
      "0\n",
      "10080\n",
      "0\n",
      "0\n",
      "1052.9\n",
      "ClimateFEVER\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "5418128\n",
      "0\n",
      "0\n",
      "539.1\n",
      "CQADupstackAndroidRetrieval\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "23697\n",
      "0\n",
      "0\n",
      "578.7\n",
      "CQADupstackEnglishRetrieval\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "41791\n",
      "0\n",
      "0\n",
      "467.1\n",
      "CQADupstackGamingRetrieval\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "46896\n",
      "0\n",
      "0\n",
      "474.7\n",
      "CQADupstackGisRetrieval\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "38522\n",
      "0\n",
      "0\n",
      "991.1\n",
      "CQADupstackMathematicaRetrieval\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "17509\n",
      "0\n",
      "0\n",
      "1103.7\n",
      "CQADupstackPhysicsRetrieval\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "39355\n",
      "0\n",
      "0\n",
      "799.4\n",
      "CQADupstackProgrammersRetrieval\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "33052\n",
      "0\n",
      "0\n",
      "1030.2\n",
      "CQADupstackStatsRetrieval\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "42921\n",
      "0\n",
      "0\n",
      "1041.0\n",
      "CQADupstackTexRetrieval\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "71090\n",
      "0\n",
      "0\n",
      "1246.9\n",
      "CQADupstackUnixRetrieval\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "48454\n",
      "0\n",
      "0\n",
      "984.7\n",
      "CQADupstackWebmastersRetrieval\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "17911\n",
      "0\n",
      "0\n",
      "689.8\n",
      "CQADupstackWordpressRetrieval\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "49146\n",
      "0\n",
      "0\n",
      "1111.9\n",
      "DBPedia\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "4635989\n",
      "4636322\n",
      "0\n",
      "310.2\n",
      "310.1\n",
      "FEVER\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "5423234\n",
      "0\n",
      "0\n",
      "538.6\n",
      "FiQA2018\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "58286\n",
      "0\n",
      "0\n",
      "760.4\n",
      "HotpotQA\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "5240734\n",
      "0\n",
      "0\n",
      "288.6\n",
      "MSMARCO\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "8848803\n",
      "8841866\n",
      "0\n",
      "336.6\n",
      "336.8\n",
      "MSMARCOv2\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "138641342\n",
      "138368101\n",
      "0\n",
      "341.4\n",
      "342.0\n",
      "0\n",
      "NFCorpus\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "3956\n",
      "0\n",
      "0\n",
      "1462.7\n",
      "NQ\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "2684920\n",
      "0\n",
      "0\n",
      "492.7\n",
      "QuoraRetrieval\n",
      "Retrieval\n",
      "s2s\n",
      "1\n",
      "0\n",
      "0\n",
      "532931\n",
      "0\n",
      "0\n",
      "62.9\n",
      "SCIDOCS\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "26657\n",
      "0\n",
      "0\n",
      "1161.9\n",
      "SciFact\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "5483\n",
      "0\n",
      "0\n",
      "1422.3\n",
      "Touche2020\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "382594\n",
      "0\n",
      "0\n",
      "1720.1\n",
      "TRECCOVID\n",
      "Retrieval\n",
      "s2p\n",
      "1\n",
      "0\n",
      "0\n",
      "171382\n",
      "0\n",
      "0\n",
      "1117.4\n",
      "BIOSSES\n",
      "STS\n",
      "s2s\n",
      "1\n",
      "200\n",
      "200\n",
      "200\n",
      "156.6\n",
      "156.6\n",
      "156.6\n",
      "SICK-R\n",
      "STS\n",
      "s2s\n",
      "1\n",
      "19854\n",
      "19854\n",
      "19854\n",
      "46.1\n",
      "46.1\n",
      "46.1\n",
      "STS12\n",
      "STS\n",
      "s2s\n",
      "1\n",
      "4468\n",
      "0\n",
      "6216\n",
      "100.7\n",
      "0\n",
      "64.7\n",
      "STS13\n",
      "STS\n",
      "s2s\n",
      "1\n",
      "0\n",
      "0\n",
      "3000\n",
      "0\n",
      "0\n",
      "54.0\n",
      "STS14\n",
      "STS\n",
      "s2s\n",
      "1\n",
      "0\n",
      "0\n",
      "7500\n",
      "0\n",
      "0\n",
      "54.3\n",
      "STS15\n",
      "STS\n",
      "s2s\n",
      "1\n",
      "0\n",
      "0\n",
      "6000\n",
      "0\n",
      "0\n",
      "57.7\n",
      "STS16\n",
      "STS\n",
      "s2s\n",
      "1\n",
      "0\n",
      "0\n",
      "2372\n",
      "0\n",
      "0\n",
      "65.3\n",
      "STS17\n",
      "STS\n",
      "s2s\n",
      "11\n",
      "0\n",
      "0\n",
      "500\n",
      "0\n",
      "0\n",
      "43.3\n",
      "STS22\n",
      "STS\n",
      "p2p\n",
      "18\n",
      "0\n",
      "0\n",
      "8060\n",
      "0\n",
      "0\n",
      "1992.8\n",
      "STSBenchmark\n",
      "STS\n",
      "s2s\n",
      "1\n",
      "11498\n",
      "3000\n",
      "2758\n",
      "57.6\n",
      "64.0\n",
      "53.6\n",
      "SummEval\n",
      "Summarization\n",
      "p2p\n",
      "1\n",
      "0\n",
      "0\n",
      "2800\n",
      "0\n",
      "0\n",
      "359.8\n",
      "Table 2: Tasks in MTEB\n",
      "AmazonPolarity\n",
      "(McAuley\n",
      "and\n",
      "Leskovec,\n",
      "2013) A collection of Amazon customer reviews\n",
      "annotated for polarity classiﬁcation.\n",
      "For each\n",
      "review the label is either \"positive\" or \"negative\".\n",
      "AmazonReviews\n",
      "(McAuley\n",
      "and\n",
      "Leskovec,\n",
      "2013) A collection of Amazon reviews designed\n",
      "to aid research in multilingual text classiﬁcation.\n",
      "For each review the label is the score given by\n",
      "the review between 0 and 4 (1-5 stars). This is a\n",
      "multilingual dataset with 6 available languages.\n",
      "Banking77\n",
      "(Casanueva et al., 2020) Dataset\n",
      "composed of online banking queries annotated with\n",
      "their corresponding intents. For each user query\n",
      "the label is an intent among 77 intents like ’acti-\n",
      "vate_my_card’, ’apple_pay’, ’bank_transfer’, etc.\n",
      "Emotion\n",
      "(Saravia et al., 2018) Dataset of English\n",
      "Twitter messages with six basic emotions: anger,\n",
      "fear, joy, love, sadness, and surprise.\n",
      "Imdb\n",
      "(Maas et al., 2011) Large movie review\n",
      "dataset with labels being positive or negative.\n",
      "MassiveIntent\n",
      "(FitzGerald et al., 2022) A col-\n",
      "lection of Amazon Alexa virtual assistant utter-\n",
      "ances annotated with the associated intent. For\n",
      "each user utterance the label is one of 60 intents\n",
      "like ’play_music’, ’alarm_set’, etc. This is a multi-\n",
      "lingual dataset with 51 available languages.\n",
      "MassiveScenario\n",
      "(FitzGerald et al., 2022) A col-\n",
      "lection of Amazon Alexa virtual assistant utter-\n",
      "ances annotated with the associated intent. For\n",
      "each user utterance the label is a theme among 60\n",
      "scenarios like ’music’, ’weather’, etc. This is a\n",
      "multilingual dataset with 51 available languages.\n",
      "MTOPDomain\n",
      "/\n",
      "MTOPIntent\n",
      "Multilingual\n",
      "sentence datasets from the MTOP (Li et al., 2020)\n",
      "benchmark. We refer to their paper for details.\n",
      "ToxicConversations\n",
      "Dataset from Kaggle com-\n",
      "petition14. Collection of comments from the Civil\n",
      "Comments platform together with annotations if\n",
      "the comment is toxic or not.\n",
      "TweetSentimentExtraction\n",
      "Dataset from Kag-\n",
      "gle competition15.\n",
      "Sentiment classiﬁcation of\n",
      "tweets as neutral, positive or negative.\n",
      "A.3\n",
      "Pair Classiﬁcation\n",
      "SprintDuplicateQuestions\n",
      "(Shah et al., 2018):\n",
      "Collection of questions from the Sprint commu-\n",
      "nity. The goal is to classify a pair of sentences as\n",
      "duplicates or not.\n",
      "TwitterSemEval2015\n",
      "(Xu\n",
      "et\n",
      "al.,\n",
      "2015)\n",
      "Paraphrase-Pairs of Tweets from the SemEval\n",
      "2015 workshop. The goal is to classify a pair of\n",
      "tweets as paraphrases or not.\n",
      "14https://www.kaggle.com/competitions/\n",
      "jigsaw-unintended-bias-in-toxicity-class\n",
      "ification\n",
      "15https://www.kaggle.com/competitions/\n",
      "tweet-sentiment-extraction\n",
      "TwitterURLCorpus\n",
      "(Lan\n",
      "et\n",
      "al.,\n",
      "2017)\n",
      "Paraphrase-Pairs of Tweets.\n",
      "The goal is to\n",
      "classify a pair of tweets as paraphrases or not.\n",
      "A.4\n",
      "Bitext Mining\n",
      "BUCC\n",
      "(Zweigenbaum et al., 2016, 2017, 2018)\n",
      "BUCC provides big set of sentences (∼ 10-70k\n",
      "each) for English, French, Russian, German and\n",
      "Chinese, along with associated pairs annotation.\n",
      "The annotated pairs here corresponds to a pairs of\n",
      "translated sentences, i.e. a sentence and its transla-\n",
      "tion in the other language.\n",
      "Tatoeba\n",
      "(Research) Tatoeba provides sets of sen-\n",
      "tences (1000 sentences each) for 112 languages\n",
      "with annoated associated pairs. Each pair is one\n",
      "sentence and its translation in another language.\n",
      "A.5\n",
      "Reranking\n",
      "AskUbuntuDupQuestions16\n",
      "Questions\n",
      "from\n",
      "AskUbuntu with manual annotations marking pairs\n",
      "of questions as similar or dissimilar.\n",
      "MindSmall\n",
      "(Wu et al., 2020) Large-scale En-\n",
      "glish Dataset for News Recommendation Research.\n",
      "Ranking news article titles given the title of a news\n",
      "article. The idea is to recommend other news from\n",
      "the one you are reading.\n",
      "SciDocsRR\n",
      "(Cohan et al., 2020b) Ranking of re-\n",
      "lated scientiﬁc papers based on their title.\n",
      "StackOverﬂowDupQuestions\n",
      "(Liu et al., 2018)\n",
      "Stack Overﬂow Duplicate Questions Task for ques-\n",
      "tions with the tags Java, JavaScript and Python,\n",
      "ranking questions as duplicates or not.\n",
      "A.6\n",
      "Semantic Textual Similarity (STS)\n",
      "STS12, STS13, STS14, STS15, STS16, STS17,\n",
      "STS22, STSBenchmark\n",
      "(Agirre et al., 2012,\n",
      "2013)17181920 Original STS benchmark, with\n",
      "scores from 0 to 5. The selection of sentences\n",
      "includes text from image captions, news headlines\n",
      "and user forums. In total they contain between\n",
      "1,000 and 20,000 sentences. STS12 - STS16 and\n",
      "16https://github.com/taolei87/askubuntu\n",
      "17https://alt.qcri.org/semeval2014/tas\n",
      "k10/\n",
      "18https://alt.qcri.org/semeval2015/tas\n",
      "k2/\n",
      "19https://alt.qcri.org/semeval2016/tas\n",
      "k1/\n",
      "20https://competitions.codalab.org/com\n",
      "petitions/33835\n",
      "STSBenchmark are monolingual english bench-\n",
      "marks. STS17 and STS22 contain crosslingual\n",
      "pairs of sentences, where the goal is to assess the\n",
      "similarity of two sentences in different languages.\n",
      "STS17 has 11 language pairs (among Korean, Ara-\n",
      "bic, English, French, German, Turkish, Spanish,\n",
      "Italian and Dutch) and STS22 has 18 language pairs\n",
      "(among Arabic, English, French, German, Turkish,\n",
      "Spanish, Polish, Italian, Russian and Chinese).\n",
      "BIOSSES21\n",
      "Contains 100 sentence pairs from\n",
      "the biomedical ﬁeld.\n",
      "SICK-R\n",
      "(Agirre et al., 2014) Sentences Involv-\n",
      "ing Compositional Knowledge (SICK) contains a\n",
      "large number of sentence pairs (10 0000) that are\n",
      "lexically, syntactically and semantically rich.\n",
      "A.7\n",
      "Summarization\n",
      "SummEval\n",
      "(Fabbri et al., 2020) Summaries gen-\n",
      "erated by recent summarization models trained on\n",
      "CNN or DailyMail alongside human annotations.\n",
      "A.8\n",
      "Retrieval\n",
      "We refer to the BEIR paper (Thakur et al., 2021),\n",
      "which contains description of each dataset. For\n",
      "MTEB, we include all publicly available datasets:\n",
      "ArguAna, ClimateFEVER, CQADupstack, DB-\n",
      "Pedia, FEVER, FiQA2018, HotpotQA, MS-\n",
      "MARCO, NFCorpus, NQ, Quora, SCIDOCS,\n",
      "SciFact, Touche2020, TRECCOVID.\n",
      "B\n",
      "Limitations of MTEB\n",
      "While MTEB aims to be a diverse benchmark to\n",
      "provide holistic performance reviews, the bench-\n",
      "mark has its limitations. We list them here:\n",
      "1. Long document datasets\n",
      "MTEB covers mul-\n",
      "tiple text lengths (S2S, P2P, S2P), but very long\n",
      "documents are still missing. The longest datasets in\n",
      "MTEB have a few hundred words, and longer text\n",
      "sizes could be relevant for use cases like retrieval.\n",
      "2. Task imbalance\n",
      "Tasks in MTEB have a differ-\n",
      "ent amount of datasets with summarization consist-\n",
      "ing of only a single dataset. This means MTEB av-\n",
      "erage scores, which are computed over all datasets,\n",
      "are biased towards tasks with many datasets, no-\n",
      "tably retrieval, classiﬁcation and clustering. As\n",
      "MTEB grows, we hope to add more datasets to cur-\n",
      "rently underrepresented tasks like summarization\n",
      "or pair classiﬁcation.\n",
      "21https://tabilab.cmpe.boun.edu.tr/BIO\n",
      "SSES/DataSet.html\n",
      "3. Multinguality\n",
      "MTEB contains multilingual\n",
      "classiﬁcation, STS and bitext mining datasets.\n",
      "However, retrieval and clustering are English-only.\n",
      "SGPT-BLOOM-7B1-msmarco is geared towards\n",
      "multilingual retrieval datasets and due to the lack\n",
      "thereof cannot be comprehensively benchmarked\n",
      "in MTEB. Further, MTEB does not contain any\n",
      "code datasets that could be used to benchmark code\n",
      "models (Neelakantan et al., 2022; Allal et al., 2023).\n",
      "It should be easy to extend MTEB with datasets,\n",
      "such as CodeSearchNet (Husain et al., 2019), TyDI\n",
      "QA (Clark et al., 2020), XOR QA (Asai et al., 2020)\n",
      "or MIRACL (Zhang et al., 2022).\n",
      "4. Additional modalities\n",
      "Text embeddings are\n",
      "commonly used as input features for downstream\n",
      "models, such as in our classiﬁcation task. This\n",
      "can involve other modalities, notably image con-\n",
      "tent (Carvalho et al., 2018; Tan and Bansal, 2019;\n",
      "Muennighoff, 2020; Nichol et al., 2021; Saharia\n",
      "et al., 2022; Weinbach et al., 2022). We have fo-\n",
      "cused solely on natural language applications and\n",
      "leave extensive benchmarking of text embeddings\n",
      "as inputs for other modalities to future work.\n",
      "C\n",
      "Examples\n",
      "Tables 3-9 provide examples for each dataset for\n",
      "each task. For retrieval datasets, we refer to the\n",
      "BEIR paper (Thakur et al., 2021).\n",
      "D\n",
      "Correlations\n",
      "Figure 6 provides correlation heatmaps for model\n",
      "performance and MTEB tasks.\n",
      "E\n",
      "Models\n",
      "Table 10 provides publicly available model check-\n",
      "points used for MTEB evaluation.\n",
      "F\n",
      "Additional results\n",
      "Tables 11 until the end provide results on individ-\n",
      "ual datasets of MTEB. The results are additionally\n",
      "available in json format on the Hugging Face Hub22\n",
      "and can be inspected on the leaderboard23.\n",
      "22https://huggingface.co/datasets/mteb\n",
      "/results\n",
      "23https://huggingface.co/spaces/mteb/l\n",
      "eaderboard\n",
      "Dataset\n",
      "Text\n",
      "Label\n",
      "AmazonCounterfactualClassiﬁcation\n",
      "In person it looks as though it would have cost a lot more.\n",
      "counterfactual\n",
      "AmazonPolarityClassiﬁcation\n",
      "an absolute masterpiece I am quite sure any of you actually taking the time to read this have played the game at least\n",
      "once, and heard at least a few of the tracks here. And whether you were aware of it or not, Mitsuda’s music contributed\n",
      "greatly to the...\n",
      "positive\n",
      "AmazonReviewsClassiﬁcation\n",
      "solo llega una unidad cuando te obligan a comprar dos Te obligan a comprar dos unidades y te llega solo una y no hay\n",
      "forma de reclamar, una autentica estafa, no compreis!!\n",
      "0\n",
      "Banking77Classiﬁcation\n",
      "What currencies is an exchange rate calculated in?\n",
      "exchange_rate\n",
      "EmotionClassiﬁcation\n",
      "i feel so inhibited in someone elses kitchen like im painting on someone elses picture\n",
      "sadness\n",
      "ImdbClassiﬁcation\n",
      "When I ﬁrst saw a glimpse of this movie, I quickly noticed the actress who was playing the role of Lucille Ball. Rachel\n",
      "York’s portrayal of Lucy is absolutely awful. Lucille Ball was an astounding comedian with incredible talent. To think\n",
      "about a legend like Lucille Ball being portrayed the way she was in the movie is horrendous. I cannot believe...\n",
      "negative\n",
      "MassiveIntentClassiﬁcation\n",
      "réveille-moi à neuf heures du matin le vendredi\n",
      "alarm_set\n",
      "MassiveScenarioClassiﬁcation\n",
      "tell me the artist of this song\n",
      "music\n",
      "MTOPDomainClassiﬁcation\n",
      "Maricopa County weather forecast for this week\n",
      "weather\n",
      "MTOPIntentClassiﬁcation\n",
      "what ingredients do is have left\n",
      "GET_INFO_RECIPES\n",
      "ToxicConversationsClassiﬁcation\n",
      "The guy’s a damn cop, so what do you expect?\n",
      "toxic\n",
      "TweetSentimentExtractionClassiﬁcation\n",
      "I really really like the song Love Story by Taylor Swift\n",
      "positive\n",
      "Table 3: Classiﬁcation examples\n",
      "Dataset\n",
      "Text\n",
      "Cluster\n",
      "ArxivClusteringP2P\n",
      "Finite groups of rank two which do not involve Qd(p). Let p > 3 be a prime. We show that if G is a ﬁnite group\n",
      "with p-rank equal to 2, then G involves Qd(p) if and only if G p′-involves Qd(p). This allows us to use a version\n",
      "of Glauberman’s ZJ-theorem to give a more direct construction of ﬁnite group actions on mod-p homotopy spheres.\n",
      "We give an example to illustrate that the above conclusion does not hold for p ≤ 3.\n",
      "math\n",
      "ArxivClusteringS2S\n",
      "Vertical shift and simultaneous Diophantine approximation on polynomial curves\n",
      "math\n",
      "BiorxivClusteringP2P\n",
      "Innate Immune sensing of Inﬂuenza A viral RNA through IFI16 promotes pyroptotic cell death Programmed cell death\n",
      "pathways are triggered by various stresses or stimuli, including viral infections. The mechanism underlying the regula-\n",
      "tion of these pathways upon Inﬂuenza A virus IAV infection is not well characterized. We report that a cytosolic DNA\n",
      "sensor IFI16 is...\n",
      "immunology\n",
      "BiorxivClusteringS2S\n",
      "Association of CDH11 with ASD revealed by matched-gene co-expression analysis and mouse behavioral\n",
      "neuroscience\n",
      "MedrxivClusteringP2P\n",
      "Temporal trends in the incidence of haemophagocytic lymphohistiocytosis: a nationwide cohort study from England\n",
      "2003-2018. Haemophagocytic lymphohistiocytosis (HLH) is rare, results in high mortality and is increasingly being\n",
      "diagnosed. Little is known about what is driving the apparent rise in the incidence of this disease. Using national linked\n",
      "electronic health data from hospital admissions and death certiﬁcation cases of HLH that were diagnosed in England\n",
      "between 1/1/2003 and 31/12/2018 were identiﬁed using a previously validated approach. We calculated incidence...\n",
      "infectious diseases\n",
      "MedrxivClusteringS2S\n",
      "Current and Lifetime Somatic Symptom Burden Among Transition-aged Young Adults on the Autism Spectrum\n",
      "psychiatry and clinical psychology\n",
      "RedditClustering\n",
      "Could anyone tell me what breed my bicolor kitten is?\n",
      "r/cats\n",
      "RedditClusteringP2P\n",
      "Headaches after working out? Hey guys! I’ve been diagnosed with adhd since I was seven. I just recently got rediag-\n",
      "nosed (22f) and I’ve been out on a different medication, adderall I was normally taking vyvanse but because of cost and\n",
      "no insurance adderall was more affordable. I’ve noticed that if I take adderall and workout...\n",
      "r/ADHD\n",
      "StackExchangeClustering\n",
      "Does this property characterize a space as Hausdorff?\n",
      "math.stackexchange.com\n",
      "StackExchangeClusteringP2P\n",
      "Google play services error DEBUG: Application is pausing, which disconnects the RTMP client. I am having this issue\n",
      "from past day with Google Play Services Unity. What happens is, when I install app directly ot device via Unity, the\n",
      "Google Play Services work ﬁne but when I upload it as beta to play store console and install it via that then it starts to\n",
      "give \" DEBUG: Application is pausing, which disconnects the RTMP client\" error. I have a proper SHA1 key.\n",
      "unity\n",
      "TwentyNewsgroupsClustering\n",
      "Commercial mining activities on the moon\n",
      "14\n",
      "Table 4: Clustering examples\n",
      "Dataset\n",
      "Sentence 1\n",
      "Sentence 2\n",
      "Label\n",
      "SprintDuplicateQuestions\n",
      "Franklin U722 USB modem signal strength\n",
      "How do I know if my Franklin U772 USB Modem has a\n",
      "weak signal ?\n",
      "1\n",
      "TwitterSemEval2015\n",
      "All the home alones watching 8 mile\",\"All the home alones\n",
      "watching 8 mile\n",
      "The last rap battle in 8 Mile nevr gets old ahah\n",
      "0\n",
      "TwitterURLCorpus\n",
      "How the metaphors we use to describe discovery affect men\n",
      "and women in the sciences\n",
      "Light Bulbs or Seeds ? How Metaphors for Ideas Inﬂuence\n",
      "Judgments About Genius\n",
      "0\n",
      "Table 5: Pair classiﬁcation examples. Labels are binary.\n",
      "Dataset\n",
      "Query\n",
      "Positive\n",
      "Negative\n",
      "AskUbuntuDupQuestions\n",
      "change the application icon theme but not changing the\n",
      "panel icons\n",
      "change folder icons in ubuntu-mono-dark theme\n",
      "change steam tray icon back to default\n",
      "MindSmallReranking\n",
      "Man accused in probe of Giuliani associates is freed on bail\n",
      "Studies show these are the best and worst states for your\n",
      "retirement\n",
      "There are 14 cheap days to ﬂy left in 2019: When are they\n",
      "and what deals can you score?\n",
      "SciDocsRR\n",
      "Discovering social circles in ego networks\n",
      "Benchmarks for testing community detection algorithms on\n",
      "directed and weighted graphs with overlapping communi-\n",
      "ties.\n",
      "Improving www proxies performance with greedy-dual-\n",
      "size-frequency caching policy\n",
      "StackOverﬂowDupQuestions\n",
      "Java launch error selection does not contain a main type\n",
      "Error: Selection does not contain a main type\n",
      "Selection Sort in Java\n",
      "Table 6: Reranking examples\n",
      "Dataset\n",
      "Sentence 1\n",
      "Sentence 2\n",
      "Score\n",
      "BIOSSES\n",
      "It has recently been shown that Craf is essential for Kras\n",
      "G12D-induced NSCLC.\n",
      "It has recently become evident that Craf is essential for the\n",
      "onset of Kras-driven non-small cell lung cancer.\n",
      "4.0\n",
      "SICK-R\n",
      "A group of children is playing in the house and there is no\n",
      "man standing in the background\n",
      "A group of kids is playing in a yard and an old man is stand-\n",
      "ing in the background\n",
      "3.2\n",
      "STS12\n",
      "Nationally, the federal Centers for Disease Control and Pre-\n",
      "vention recorded 4,156 cases of West Nile, including 284\n",
      "deaths.\n",
      "There were 293 human cases of West Nile in Indiana in\n",
      "2002, including 11 deaths statewide.\n",
      "1.7\n",
      "STS13\n",
      "this frame has to do with people ( the residents ) residing in\n",
      "locations , sometimes with a co-resident .\n",
      "inhabit or live in ; be an inhabitant of ;\n",
      "2.8\n",
      "STS14\n",
      "then the captain was gone.\n",
      "then the captain came back.\n",
      "0.8\n",
      "STS15\n",
      "you ’ll need to check the particular policies of each pub-\n",
      "lisher to see what is allowed and what is not allowed.\n",
      "if you need to publish the book and you have found one\n",
      "publisher that allows it.\n",
      "3.0\n",
      "STS16\n",
      "you do not need to worry.\n",
      "you don ’t have to worry.\n",
      "5.0\n",
      "STS17\n",
      "La gente muestra su afecto el uno por el otro.\n",
      "A women giving something to other lady.\n",
      "1.4\n",
      "STS22\n",
      "El secretario general de la Asociación Gremial de los Tra-\n",
      "bajadores del Subte y Premetro de Metrodelegados, Beto\n",
      "Pianelli, dijo que el Gobierno porteño debe convocar “in-\n",
      "mediatamente” a licitación para la compra de nuevos trenes\n",
      "y retirar los que quedan en circulación...\n",
      "En diálogo con el servicio informativo de la Radio Pública,\n",
      "el ministro de Salud de la Nación, Ginés González García,\n",
      "habló sobre el avance del coronavirus en la Argentina y se\n",
      "manifestó a favor de prorrogar la cuarentena obligatoria dis-\n",
      "puesta por...\n",
      "1\n",
      "STSBenchmark\n",
      "A man is playing the cello.\n",
      "A man seated is playing the cello.\n",
      "4.25\n",
      "Table 7: STS examples. Scores are continuous between 0 and 5 (included).\n",
      "Dataset\n",
      "First set sentence\n",
      "Second set sentence\n",
      "BUCC\n",
      "Morales remporte l’élection présidentielle de 2005 à la ma-\n",
      "jorité absolue.\n",
      "Morales went on to win the 2005 presidential election with\n",
      "an absolute majority.\n",
      "Tatoeba\n",
      "Chi le ha detto che Tom l’ha fatto?\n",
      "Who told you that Tom did that?\n",
      "Table 8: Bitext mining examples\n",
      "Dataset\n",
      "Human Summary\n",
      "Machine Summary\n",
      "Relevance\n",
      "SummEval\n",
      "V. Stiviano must pay back $2.6 million in gifts from Donald\n",
      "Sterling. Sterling’s wife claimed the ex-Clippers used the\n",
      "couple’s money for the gifts. The items included a Ferrari,\n",
      "two Bentleys and a Range Rover.\n",
      "donald sterling , nba team last year . sterling ’s wife sued\n",
      "for $ 2.6 million in gifts . sterling says he is the former\n",
      "female companion who has lost the . sterling has ordered\n",
      "v. stiviano to pay back $ 2.6 m in gifts after his wife sued .\n",
      "sterling also includes a $ 391 easter bunny costume , $ 299\n",
      "and a $ 299 .\n",
      "1.7\n",
      "Table 9: Summarization example\n",
      "Glove\n",
      "Komninos\n",
      "BERT\n",
      "SimCSE-BERT-unsup\n",
      "SimCSE-BERT-sup\n",
      "coCondenser-msmarco\n",
      "Contriever\n",
      "SPECTER\n",
      "LaBSE\n",
      "LASER2\n",
      "MiniLM-L6\n",
      "MiniLM-L12\n",
      "MiniLM-L12-multilingual\n",
      "MPNet\n",
      "MPNet-multilingual\n",
      "SGPT-125M-nli\n",
      "SGPT-5.8B-nli\n",
      "SGPT-125M-msmarco\n",
      "SGPT-1.3B-msmarco\n",
      "SGPT-2.7B-msmarco\n",
      "SGPT-5.8B-msmarco\n",
      "SGPT-BLOOM-7.1B-msmarco\n",
      "GTR-Base\n",
      "GTR-Large\n",
      "GTR-XL\n",
      "GTR-XXL\n",
      "ST5-Base\n",
      "ST5-Large\n",
      "ST5-XL\n",
      "ST5-XXL\n",
      "Glove\n",
      "Komninos\n",
      "BERT\n",
      "SimCSE-BERT-unsup\n",
      "SimCSE-BERT-sup\n",
      "coCondenser-msmarco\n",
      "Contriever\n",
      "SPECTER\n",
      "LaBSE\n",
      "LASER2\n",
      "MiniLM-L6\n",
      "MiniLM-L12\n",
      "MiniLM-L12-multilingual\n",
      "MPNet\n",
      "MPNet-multilingual\n",
      "SGPT-125M-nli\n",
      "SGPT-5.8B-nli\n",
      "SGPT-125M-msmarco\n",
      "SGPT-1.3B-msmarco\n",
      "SGPT-2.7B-msmarco\n",
      "SGPT-5.8B-msmarco\n",
      "SGPT-BLOOM-7.1B-msmarco\n",
      "GTR-Base\n",
      "GTR-Large\n",
      "GTR-XL\n",
      "GTR-XXL\n",
      "ST5-Base\n",
      "ST5-Large\n",
      "ST5-XL\n",
      "ST5-XXL\n",
      "98\n",
      "90\n",
      "88\n",
      "97\n",
      "95\n",
      "93\n",
      "95\n",
      "94\n",
      "92\n",
      "99\n",
      "95\n",
      "93\n",
      "85\n",
      "95\n",
      "94\n",
      "90\n",
      "89\n",
      "79\n",
      "91\n",
      "90\n",
      "98\n",
      "93\n",
      "92\n",
      "89\n",
      "92\n",
      "91\n",
      "90\n",
      "84\n",
      "96\n",
      "94\n",
      "92\n",
      "98\n",
      "97\n",
      "95\n",
      "91\n",
      "94\n",
      "94\n",
      "94\n",
      "91\n",
      "97\n",
      "96\n",
      "90\n",
      "85\n",
      "90\n",
      "97\n",
      "91\n",
      "90\n",
      "78\n",
      "91\n",
      "90\n",
      "97\n",
      "97\n",
      "88\n",
      "92\n",
      "87\n",
      "90\n",
      "88\n",
      "77\n",
      "91\n",
      "90\n",
      "97\n",
      "97\n",
      "87\n",
      "91\n",
      "86\n",
      "100\n",
      "95\n",
      "93\n",
      "85\n",
      "96\n",
      "96\n",
      "98\n",
      "96\n",
      "91\n",
      "95\n",
      "92\n",
      "97\n",
      "97\n",
      "89\n",
      "88\n",
      "78\n",
      "90\n",
      "90\n",
      "96\n",
      "96\n",
      "87\n",
      "90\n",
      "86\n",
      "99\n",
      "99\n",
      "96\n",
      "94\n",
      "93\n",
      "85\n",
      "97\n",
      "96\n",
      "98\n",
      "97\n",
      "89\n",
      "95\n",
      "93\n",
      "97\n",
      "97\n",
      "99\n",
      "95\n",
      "97\n",
      "96\n",
      "91\n",
      "99\n",
      "98\n",
      "95\n",
      "91\n",
      "94\n",
      "97\n",
      "96\n",
      "92\n",
      "91\n",
      "97\n",
      "91\n",
      "96\n",
      "96\n",
      "95\n",
      "89\n",
      "98\n",
      "98\n",
      "97\n",
      "94\n",
      "91\n",
      "96\n",
      "94\n",
      "94\n",
      "94\n",
      "97\n",
      "93\n",
      "98\n",
      "99\n",
      "92\n",
      "90\n",
      "78\n",
      "91\n",
      "89\n",
      "96\n",
      "96\n",
      "87\n",
      "89\n",
      "85\n",
      "95\n",
      "96\n",
      "96\n",
      "94\n",
      "95\n",
      "93\n",
      "95\n",
      "89\n",
      "87\n",
      "75\n",
      "88\n",
      "87\n",
      "96\n",
      "97\n",
      "83\n",
      "87\n",
      "82\n",
      "96\n",
      "96\n",
      "94\n",
      "94\n",
      "95\n",
      "89\n",
      "94\n",
      "99\n",
      "87\n",
      "86\n",
      "73\n",
      "87\n",
      "85\n",
      "95\n",
      "97\n",
      "81\n",
      "85\n",
      "80\n",
      "95\n",
      "96\n",
      "94\n",
      "94\n",
      "94\n",
      "88\n",
      "93\n",
      "98\n",
      "100\n",
      "84\n",
      "82\n",
      "69\n",
      "83\n",
      "81\n",
      "92\n",
      "95\n",
      "78\n",
      "81\n",
      "77\n",
      "93\n",
      "94\n",
      "91\n",
      "92\n",
      "91\n",
      "84\n",
      "90\n",
      "97\n",
      "99\n",
      "99\n",
      "84\n",
      "83\n",
      "69\n",
      "83\n",
      "81\n",
      "92\n",
      "94\n",
      "78\n",
      "82\n",
      "78\n",
      "94\n",
      "95\n",
      "91\n",
      "93\n",
      "91\n",
      "84\n",
      "90\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "87\n",
      "85\n",
      "74\n",
      "88\n",
      "87\n",
      "96\n",
      "98\n",
      "79\n",
      "87\n",
      "83\n",
      "96\n",
      "97\n",
      "95\n",
      "95\n",
      "95\n",
      "88\n",
      "92\n",
      "96\n",
      "97\n",
      "97\n",
      "96\n",
      "95\n",
      "85\n",
      "83\n",
      "72\n",
      "87\n",
      "86\n",
      "95\n",
      "98\n",
      "76\n",
      "84\n",
      "80\n",
      "95\n",
      "96\n",
      "93\n",
      "95\n",
      "94\n",
      "86\n",
      "90\n",
      "94\n",
      "97\n",
      "97\n",
      "95\n",
      "94\n",
      "100\n",
      "85\n",
      "83\n",
      "72\n",
      "86\n",
      "85\n",
      "95\n",
      "97\n",
      "76\n",
      "84\n",
      "79\n",
      "95\n",
      "96\n",
      "93\n",
      "95\n",
      "94\n",
      "85\n",
      "90\n",
      "94\n",
      "97\n",
      "97\n",
      "95\n",
      "95\n",
      "100\n",
      "100\n",
      "84\n",
      "82\n",
      "71\n",
      "85\n",
      "85\n",
      "94\n",
      "97\n",
      "75\n",
      "84\n",
      "79\n",
      "95\n",
      "96\n",
      "92\n",
      "95\n",
      "93\n",
      "84\n",
      "89\n",
      "93\n",
      "96\n",
      "96\n",
      "94\n",
      "94\n",
      "99\n",
      "100\n",
      "100\n",
      "94\n",
      "92\n",
      "87\n",
      "97\n",
      "97\n",
      "96\n",
      "93\n",
      "88\n",
      "95\n",
      "93\n",
      "94\n",
      "93\n",
      "96\n",
      "94\n",
      "97\n",
      "97\n",
      "97\n",
      "91\n",
      "90\n",
      "89\n",
      "85\n",
      "85\n",
      "92\n",
      "91\n",
      "91\n",
      "91\n",
      "92\n",
      "90\n",
      "85\n",
      "95\n",
      "96\n",
      "94\n",
      "93\n",
      "86\n",
      "93\n",
      "91\n",
      "93\n",
      "93\n",
      "95\n",
      "94\n",
      "96\n",
      "95\n",
      "97\n",
      "90\n",
      "90\n",
      "89\n",
      "86\n",
      "86\n",
      "92\n",
      "92\n",
      "92\n",
      "92\n",
      "99\n",
      "92\n",
      "90\n",
      "84\n",
      "94\n",
      "95\n",
      "94\n",
      "93\n",
      "85\n",
      "92\n",
      "90\n",
      "93\n",
      "93\n",
      "94\n",
      "94\n",
      "95\n",
      "94\n",
      "96\n",
      "91\n",
      "91\n",
      "90\n",
      "87\n",
      "87\n",
      "93\n",
      "93\n",
      "93\n",
      "92\n",
      "99\n",
      "100\n",
      "90\n",
      "88\n",
      "81\n",
      "92\n",
      "93\n",
      "94\n",
      "94\n",
      "83\n",
      "90\n",
      "87\n",
      "93\n",
      "94\n",
      "94\n",
      "95\n",
      "95\n",
      "92\n",
      "96\n",
      "92\n",
      "93\n",
      "92\n",
      "90\n",
      "90\n",
      "95\n",
      "95\n",
      "95\n",
      "94\n",
      "97\n",
      "99\n",
      "99\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "(a) Model correlation based on all results\n",
      "Class.\n",
      "Clust.\n",
      "PairClass.\n",
      "Rerank.\n",
      "Retr.\n",
      "STS\n",
      "Summ.\n",
      "Class.\n",
      "Clust.\n",
      "PairClass.\n",
      "Rerank.\n",
      "Retr.\n",
      "STS\n",
      "Summ.\n",
      "68\n",
      "72\n",
      "81\n",
      "58\n",
      "95\n",
      "83\n",
      "57\n",
      "85\n",
      "87\n",
      "90\n",
      "79\n",
      "75\n",
      "85\n",
      "78\n",
      "69\n",
      "3\n",
      "-4\n",
      "8\n",
      "-8\n",
      "-16\n",
      "0\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "(b) Task correlation based on average task results\n",
      "Figure 6: Pearson correlations across model and task results. Left: Size variants of the same architecture show\n",
      "high correlations. Right: Performance on clustering and reranking correlates strongest, while summarization and\n",
      "classiﬁcation show weaker correlation with other tasks.\n",
      "Model\n",
      "Public Checkpoint\n",
      "Glove\n",
      "https://huggingface.co/sentence-transformers/average_word_embeddings_glove.6B.300d\n",
      "Komninos\n",
      "https://huggingface.co/sentence-transformers/average_word_embeddings_komninos\n",
      "BERT\n",
      "https://huggingface.co/bert-base-uncased\n",
      "SimCSE-BERT-unsup\n",
      "https://huggingface.co/princeton-nlp/unsup-simcse-bert-base-uncased\n",
      "SimCSE-BERT-sup\n",
      "https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased\n",
      "coCondenser-msmarco\n",
      "https://huggingface.co/sentence-transformers/msmarco-bert-co-condensor\n",
      "Contriever\n",
      "https://huggingface.co/nthakur/contriever-base-msmarco\n",
      "SPECTER\n",
      "https://huggingface.co/sentence-transformers/allenai-specter\n",
      "LaBSE\n",
      "https://huggingface.co/sentence-transformers/LaBSE\n",
      "LASER2\n",
      "https://github.com/facebookresearch/LASER\n",
      "MiniLM-L6\n",
      "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
      "MiniLM-L12\n",
      "https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2\n",
      "MiniLM-L12-multilingual\n",
      "https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "MPNet\n",
      "https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
      "MPNet-multilingual\n",
      "https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
      "MiniLM-L12-multilingual\n",
      "https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "SGPT-125M-nli\n",
      "https://huggingface.co/Muennighoff/SGPT-125M-weightedmean-nli-bitfit\n",
      "SGPT-5.8B-nli\n",
      "https://huggingface.co/Muennighoff/SGPT-5.8B-weightedmean-nli-bitfit\n",
      "SGPT-125M-msmarco\n",
      "https://huggingface.co/Muennighoff/SGPT-125M-weightedmean-msmarco-specb-bitfit\n",
      "SGPT-1.3B-msmarco\n",
      "https://huggingface.co/Muennighoff/SGPT-1.3B-weightedmean-msmarco-specb-bitfit\n",
      "SGPT-2.7B-msmarco\n",
      "https://huggingface.co/Muennighoff/SGPT-2.7B-weightedmean-msmarco-specb-bitfit\n",
      "SGPT-5.8B-msmarco\n",
      "https://huggingface.co/Muennighoff/SGPT-5.8B-weightedmean-msmarco-specb-bitfit\n",
      "SGPT-BLOOM-7.1B-msmarco\n",
      "https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco\n",
      "SGPT-BLOOM-1.7B-nli\n",
      "https://huggingface.co/bigscience-data/sgpt-bloom-1b7-nli\n",
      "GTR-Base\n",
      "https://huggingface.co/sentence-transformers/gtr-t5-base\n",
      "GTR-Large\n",
      "https://huggingface.co/sentence-transformers/gtr-t5-large\n",
      "GTR-XL\n",
      "https://huggingface.co/sentence-transformers/gtr-t5-xl\n",
      "GTR-XXL\n",
      "https://huggingface.co/sentence-transformers/gtr-t5-xxl\n",
      "ST5-Base\n",
      "https://huggingface.co/sentence-transformers/sentence-t5-base\n",
      "ST5-Large\n",
      "https://huggingface.co/sentence-transformers/sentence-t5-large\n",
      "ST5-XL\n",
      "https://huggingface.co/sentence-transformers/sentence-t5-xl\n",
      "ST5-XXL\n",
      "https://huggingface.co/sentence-transformers/sentence-t5-xxl\n",
      "Table 10: Publicly available model links used for evaluation\n",
      "Dataset\n",
      "Glove\n",
      "Komninos\n",
      "BERT\n",
      "SimCSE-\n",
      "SimCSE-\n",
      "coCondenser-\n",
      "Contr-\n",
      "SPECTER\n",
      "LaBSE\n",
      "LASER2\n",
      "MiniLM-\n",
      "MiniLM-\n",
      "MiniLM-\n",
      "MPNet\n",
      "MPNet-\n",
      "OpenAI\n",
      "SGPT-125M-\n",
      "SGPT-5.8B-\n",
      "SGPT-125M-\n",
      "SGPT-1.3B-\n",
      "SGPT-2.7B-\n",
      "SGPT-5.8B-\n",
      "SGPT-\n",
      "GTR-\n",
      "GTR-\n",
      "GTR-\n",
      "GTR-\n",
      "ST5-\n",
      "ST5-\n",
      "ST5-\n",
      "ST5-\n",
      "BERT-\n",
      "BERT-\n",
      "msmarco\n",
      "iever\n",
      "L6\n",
      "L12-\n",
      "L12-\n",
      "multilingual\n",
      "Ada\n",
      "nli\n",
      "nli\n",
      "msmarco\n",
      "msmarco\n",
      "msmarco\n",
      "msmarco\n",
      "BLOOM-7.1B-\n",
      "Base\n",
      "Large\n",
      "XL\n",
      "XXL\n",
      "Base\n",
      "Large\n",
      "XL\n",
      "XXL\n",
      "unsup\n",
      "sup\n",
      "multilingual\n",
      "Similarity\n",
      "msmarco\n",
      "AmazonCounterfactualClassiﬁcation\n",
      "56.91\n",
      "60.54\n",
      "74.25\n",
      "67.09\n",
      "75.75\n",
      "64.06\n",
      "72.19\n",
      "58.70\n",
      "75.93\n",
      "76.84\n",
      "64.15\n",
      "65.28\n",
      "71.57\n",
      "65.27\n",
      "75.81\n",
      "76.40\n",
      "65.88\n",
      "74.07\n",
      "61.24\n",
      "65.21\n",
      "67.57\n",
      "69.22\n",
      "68.06\n",
      "69.33\n",
      "70.03\n",
      "68.60\n",
      "67.30\n",
      "75.82\n",
      "75.51\n",
      "76.01\n",
      "77.07\n",
      "AmazonPolarityClassiﬁcation\n",
      "60.32\n",
      "59.59\n",
      "71.33\n",
      "74.48\n",
      "82.47\n",
      "66.88\n",
      "68.63\n",
      "57.77\n",
      "68.95\n",
      "61.01\n",
      "62.58\n",
      "62.98\n",
      "69.21\n",
      "67.13\n",
      "76.41\n",
      "92.83\n",
      "74.94\n",
      "82.31\n",
      "65.40\n",
      "73.21\n",
      "71.44\n",
      "71.26\n",
      "68.97\n",
      "67.82\n",
      "73.92\n",
      "74.58\n",
      "75.05\n",
      "85.12\n",
      "92.87\n",
      "93.17\n",
      "92.79\n",
      "AmazonReviewsClassiﬁcation\n",
      "29.67\n",
      "31.01\n",
      "33.56\n",
      "33.85\n",
      "39.60\n",
      "34.85\n",
      "37.42\n",
      "26.26\n",
      "35.80\n",
      "28.71\n",
      "31.79\n",
      "30.79\n",
      "35.11\n",
      "31.92\n",
      "38.51\n",
      "47.45\n",
      "35.10\n",
      "41.58\n",
      "31.17\n",
      "34.96\n",
      "35.75\n",
      "39.19\n",
      "33.86\n",
      "38.48\n",
      "37.21\n",
      "38.20\n",
      "37.30\n",
      "44.94\n",
      "47.12\n",
      "48.18\n",
      "48.93\n",
      "Banking77Classiﬁcation\n",
      "67.69\n",
      "67.05\n",
      "63.41\n",
      "73.55\n",
      "75.76\n",
      "82.35\n",
      "80.02\n",
      "66.66\n",
      "69.85\n",
      "57.76\n",
      "79.75\n",
      "80.40\n",
      "79.77\n",
      "81.86\n",
      "81.07\n",
      "68.04\n",
      "74.68\n",
      "81.74\n",
      "77.70\n",
      "82.06\n",
      "83.22\n",
      "84.49\n",
      "84.33\n",
      "79.26\n",
      "81.21\n",
      "82.22\n",
      "82.32\n",
      "76.48\n",
      "78.46\n",
      "80.88\n",
      "82.31\n",
      "EmotionClassiﬁcation\n",
      "36.93\n",
      "33.18\n",
      "35.28\n",
      "42.22\n",
      "44.81\n",
      "41.91\n",
      "44.77\n",
      "24.82\n",
      "37.22\n",
      "24.83\n",
      "38.43\n",
      "41.17\n",
      "42.37\n",
      "39.73\n",
      "45.84\n",
      "50.32\n",
      "42.23\n",
      "49.92\n",
      "39.08\n",
      "46.39\n",
      "49.21\n",
      "49.66\n",
      "44.87\n",
      "42.20\n",
      "46.32\n",
      "45.55\n",
      "43.19\n",
      "51.36\n",
      "51.73\n",
      "51.95\n",
      "48.57\n",
      "ImdbClassiﬁcation\n",
      "62.57\n",
      "63.98\n",
      "65.35\n",
      "69.63\n",
      "73.53\n",
      "60.17\n",
      "67.04\n",
      "56.35\n",
      "62.04\n",
      "57.58\n",
      "60.66\n",
      "59.76\n",
      "60.46\n",
      "70.72\n",
      "64.57\n",
      "89.38\n",
      "62.90\n",
      "74.33\n",
      "58.67\n",
      "64.05\n",
      "63.53\n",
      "66.64\n",
      "61.77\n",
      "65.99\n",
      "70.86\n",
      "68.15\n",
      "70.8\n",
      "77.34\n",
      "87.01\n",
      "87.54\n",
      "90.23\n",
      "MassiveIntentClassiﬁcation\n",
      "56.19\n",
      "57.21\n",
      "59.88\n",
      "59.84\n",
      "65.95\n",
      "70.40\n",
      "67.78\n",
      "51.73\n",
      "61.46\n",
      "47.91\n",
      "67.40\n",
      "67.15\n",
      "66.84\n",
      "69.57\n",
      "69.32\n",
      "65.17\n",
      "58.08\n",
      "70.0\n",
      "61.41\n",
      "68.65\n",
      "69.01\n",
      "70.39\n",
      "69.67\n",
      "67.05\n",
      "70.06\n",
      "70.23\n",
      "70.61\n",
      "69.74\n",
      "71.78\n",
      "72.09\n",
      "73.44\n",
      "MassiveScenarioClassiﬁcation\n",
      "66.03\n",
      "66.11\n",
      "64.28\n",
      "66.25\n",
      "70.78\n",
      "73.73\n",
      "76.00\n",
      "58.58\n",
      "66.41\n",
      "55.92\n",
      "75.76\n",
      "74.58\n",
      "71.51\n",
      "76.01\n",
      "75.35\n",
      "67.67\n",
      "66.34\n",
      "75.03\n",
      "69.74\n",
      "76.04\n",
      "75.90\n",
      "76.28\n",
      "75.34\n",
      "75.40\n",
      "75.49\n",
      "75.94\n",
      "77.77\n",
      "72.32\n",
      "73.16\n",
      "73.26\n",
      "74.82\n",
      "MTOPDomainClassiﬁcation\n",
      "79.11\n",
      "78.57\n",
      "82.63\n",
      "81.71\n",
      "84.29\n",
      "91.34\n",
      "93.18\n",
      "74.53\n",
      "86.06\n",
      "75.36\n",
      "91.56\n",
      "91.90\n",
      "87.06\n",
      "92.08\n",
      "89.24\n",
      "89.89\n",
      "81.52\n",
      "89.64\n",
      "86.96\n",
      "92.08\n",
      "92.56\n",
      "93.47\n",
      "93.68\n",
      "92.42\n",
      "94.01\n",
      "93.60\n",
      "93.84\n",
      "90.34\n",
      "90.99\n",
      "90.73\n",
      "92.49\n",
      "MTOPIntentClassiﬁcation\n",
      "55.85\n",
      "57.07\n",
      "68.14\n",
      "59.23\n",
      "63.14\n",
      "71.07\n",
      "69.31\n",
      "50.05\n",
      "63.03\n",
      "49.47\n",
      "62.18\n",
      "62.84\n",
      "65.52\n",
      "70.21\n",
      "68.69\n",
      "64.80\n",
      "58.24\n",
      "70.68\n",
      "62.25\n",
      "71.19\n",
      "71.85\n",
      "72.42\n",
      "71.34\n",
      "62.44\n",
      "63.86\n",
      "65.93\n",
      "67.71\n",
      "63.32\n",
      "64.98\n",
      "68.15\n",
      "68.33\n",
      "ToxicConversationsClassiﬁcation\n",
      "65.40\n",
      "67.76\n",
      "70.0\n",
      "68.82\n",
      "72.04\n",
      "64.01\n",
      "67.77\n",
      "57.44\n",
      "66.90\n",
      "54.05\n",
      "66.99\n",
      "67.47\n",
      "66.07\n",
      "60.86\n",
      "71.02\n",
      "70.00\n",
      "62.79\n",
      "69.93\n",
      "62.66\n",
      "68.73\n",
      "68.84\n",
      "67.71\n",
      "66.55\n",
      "66.60\n",
      "68.65\n",
      "67.56\n",
      "68.48\n",
      "68.20\n",
      "71.73\n",
      "70.95\n",
      "70.04\n",
      "TweetSentimentExtractionClassiﬁcation\n",
      "50.80\n",
      "49.68\n",
      "51.81\n",
      "53.36\n",
      "59.73\n",
      "55.74\n",
      "56.10\n",
      "45.52\n",
      "58.82\n",
      "48.73\n",
      "55.41\n",
      "54.25\n",
      "56.12\n",
      "55.46\n",
      "59.03\n",
      "63.35\n",
      "54.82\n",
      "62.44\n",
      "52.41\n",
      "55.67\n",
      "56.69\n",
      "56.85\n",
      "55.85\n",
      "56.02\n",
      "54.09\n",
      "54.77\n",
      "54.54\n",
      "62.71\n",
      "62.33\n",
      "61.21\n",
      "62.01\n",
      "ArxivClusteringP2P\n",
      "32.56\n",
      "34.73\n",
      "35.19\n",
      "32.61\n",
      "35.18\n",
      "36.94\n",
      "42.61\n",
      "44.75\n",
      "32.13\n",
      "17.77\n",
      "46.55\n",
      "46.07\n",
      "38.33\n",
      "48.38\n",
      "37.78\n",
      "41.49\n",
      "34.74\n",
      "40.55\n",
      "39.71\n",
      "43.38\n",
      "44.72\n",
      "45.59\n",
      "44.59\n",
      "35.49\n",
      "37.50\n",
      "37.90\n",
      "37.90\n",
      "39.28\n",
      "41.62\n",
      "41.62\n",
      "42.89\n",
      "ArxivClusteringS2S\n",
      "23.14\n",
      "26.01\n",
      "27.51\n",
      "24.68\n",
      "27.54\n",
      "29.03\n",
      "32.32\n",
      "35.27\n",
      "22.05\n",
      "12.39\n",
      "37.86\n",
      "37.50\n",
      "31.55\n",
      "39.72\n",
      "31.68\n",
      "28.47\n",
      "24.68\n",
      "32.49\n",
      "28.24\n",
      "33.71\n",
      "35.08\n",
      "38.86\n",
      "38.03\n",
      "27.18\n",
      "30.55\n",
      "30.45\n",
      "32.39\n",
      "27.26\n",
      "29.44\n",
      "31.17\n",
      "33.47\n",
      "BiorxivClusteringP2P\n",
      "29.27\n",
      "29.76\n",
      "30.12\n",
      "24.90\n",
      "30.15\n",
      "32.35\n",
      "34.97\n",
      "39.52\n",
      "29.84\n",
      "12.40\n",
      "38.48\n",
      "36.99\n",
      "33.49\n",
      "39.62\n",
      "33.09\n",
      "36.86\n",
      "28.93\n",
      "33.59\n",
      "33.63\n",
      "35.06\n",
      "34.41\n",
      "36.55\n",
      "36.03\n",
      "27.66\n",
      "29.59\n",
      "30.52\n",
      "30.48\n",
      "33.99\n",
      "35.99\n",
      "36.43\n",
      "36.53\n",
      "BiorxivClusteringS2S\n",
      "19.18\n",
      "20.71\n",
      "24.77\n",
      "19.55\n",
      "24.67\n",
      "28.16\n",
      "29.08\n",
      "34.53\n",
      "20.57\n",
      "8.83\n",
      "33.17\n",
      "33.21\n",
      "29.44\n",
      "35.02\n",
      "29.60\n",
      "27.55\n",
      "23.08\n",
      "29.13\n",
      "27.04\n",
      "30.71\n",
      "30.53\n",
      "33.70\n",
      "32.48\n",
      "23.25\n",
      "25.72\n",
      "26.06\n",
      "27.50\n",
      "22.92\n",
      "24.02\n",
      "26.47\n",
      "28.66\n",
      "MedrxivClusteringP2P\n",
      "26.12\n",
      "26.65\n",
      "26.09\n",
      "23.60\n",
      "26.25\n",
      "30.23\n",
      "31.19\n",
      "35.04\n",
      "30.13\n",
      "17.91\n",
      "34.41\n",
      "34.25\n",
      "31.52\n",
      "35.58\n",
      "31.96\n",
      "31.09\n",
      "28.30\n",
      "30.33\n",
      "31.37\n",
      "32.08\n",
      "31.35\n",
      "31.51\n",
      "31.05\n",
      "27.57\n",
      "28.72\n",
      "28.69\n",
      "29.12\n",
      "33.20\n",
      "32.40\n",
      "32.30\n",
      "32.09\n",
      "MedrxivClusteringS2S\n",
      "20.38\n",
      "21.50\n",
      "23.60\n",
      "21.97\n",
      "24.12\n",
      "27.01\n",
      "27.27\n",
      "31.66\n",
      "24.82\n",
      "16.63\n",
      "32.29\n",
      "32.24\n",
      "30.87\n",
      "32.87\n",
      "31.70\n",
      "26.50\n",
      "24.93\n",
      "28.02\n",
      "26.87\n",
      "29.45\n",
      "28.77\n",
      "28.76\n",
      "29.26\n",
      "25.13\n",
      "27.39\n",
      "26.69\n",
      "27.56\n",
      "26.13\n",
      "26.33\n",
      "26.93\n",
      "26.82\n",
      "RedditClustering\n",
      "28.46\n",
      "28.84\n",
      "27.24\n",
      "32.18\n",
      "40.23\n",
      "48.04\n",
      "54.89\n",
      "24.13\n",
      "28.79\n",
      "9.96\n",
      "50.67\n",
      "51.18\n",
      "42.02\n",
      "54.82\n",
      "45.24\n",
      "42.47\n",
      "33.76\n",
      "42.17\n",
      "40.23\n",
      "48.23\n",
      "46.47\n",
      "40.45\n",
      "35.53\n",
      "56.13\n",
      "61.69\n",
      "61.34\n",
      "64.13\n",
      "52.93\n",
      "54.53\n",
      "57.03\n",
      "58.99\n",
      "RedditClusteringP2P\n",
      "35.82\n",
      "7.37\n",
      "43.32\n",
      "45.14\n",
      "47.74\n",
      "53.53\n",
      "57.58\n",
      "35.06\n",
      "49.14\n",
      "26.42\n",
      "54.15\n",
      "54.80\n",
      "50.73\n",
      "56.77\n",
      "51.31\n",
      "58.10\n",
      "41.01\n",
      "48.02\n",
      "49.09\n",
      "53.18\n",
      "54.17\n",
      "55.75\n",
      "54.52\n",
      "58.53\n",
      "61.67\n",
      "61.11\n",
      "62.84\n",
      "59.67\n",
      "62.50\n",
      "62.34\n",
      "64.46\n",
      "StackExchangeClustering\n",
      "35.80\n",
      "39.04\n",
      "43.58\n",
      "43.07\n",
      "47.55\n",
      "59.54\n",
      "63.15\n",
      "39.01\n",
      "35.43\n",
      "15.79\n",
      "53.36\n",
      "53.05\n",
      "49.60\n",
      "53.80\n",
      "52.98\n",
      "53.52\n",
      "44.59\n",
      "54.13\n",
      "52.74\n",
      "60.86\n",
      "59.19\n",
      "59.21\n",
      "55.13\n",
      "64.21\n",
      "69.93\n",
      "69.95\n",
      "71.43\n",
      "63.13\n",
      "65.11\n",
      "67.13\n",
      "70.78\n",
      "StackExchangeClusteringP2P\n",
      "28.51\n",
      "30.23\n",
      "26.55\n",
      "28.50\n",
      "29.45\n",
      "30.48\n",
      "32.25\n",
      "31.46\n",
      "28.83\n",
      "18.63\n",
      "38.00\n",
      "33.13\n",
      "31.69\n",
      "34.28\n",
      "32.94\n",
      "30.43\n",
      "28.23\n",
      "31.12\n",
      "32.66\n",
      "32.36\n",
      "32.57\n",
      "33.95\n",
      "34.31\n",
      "33.01\n",
      "33.21\n",
      "32.73\n",
      "32.85\n",
      "35.68\n",
      "36.86\n",
      "34.79\n",
      "35.25\n",
      "TwentyNewsgroupsClustering\n",
      "25.83\n",
      "27.42\n",
      "23.35\n",
      "23.21\n",
      "34.86\n",
      "38.68\n",
      "46.82\n",
      "24.22\n",
      "23.28\n",
      "11.38\n",
      "46.86\n",
      "47.47\n",
      "39.28\n",
      "49.74\n",
      "44.10\n",
      "36.26\n",
      "28.24\n",
      "37.20\n",
      "32.13\n",
      "40.06\n",
      "40.89\n",
      "39.46\n",
      "37.28\n",
      "46.72\n",
      "51.64\n",
      "51.15\n",
      "50.44\n",
      "48.10\n",
      "49.33\n",
      "49.53\n",
      "50.93\n",
      "SprintDuplicateQuestions\n",
      "86.96\n",
      "85.55\n",
      "36.81\n",
      "69.41\n",
      "69.39\n",
      "96.09\n",
      "95.55\n",
      "71.63\n",
      "89.26\n",
      "65.54\n",
      "94.55\n",
      "92.45\n",
      "89.46\n",
      "90.15\n",
      "90.55\n",
      "77.85\n",
      "77.73\n",
      "80.54\n",
      "89.89\n",
      "92.58\n",
      "93.47\n",
      "93.84\n",
      "94.93\n",
      "94.55\n",
      "95.05\n",
      "95.45\n",
      "95.68\n",
      "91.23\n",
      "89.01\n",
      "91.44\n",
      "88.89\n",
      "TwitterSemEval2015\n",
      "48.45\n",
      "53.85\n",
      "55.90\n",
      "60.21\n",
      "67.75\n",
      "65.95\n",
      "66.85\n",
      "43.25\n",
      "62.78\n",
      "59.57\n",
      "67.86\n",
      "70.02\n",
      "62.06\n",
      "73.85\n",
      "66.75\n",
      "69.04\n",
      "57.09\n",
      "66.00\n",
      "54.75\n",
      "62.37\n",
      "63.68\n",
      "66.87\n",
      "65.31\n",
      "72.23\n",
      "76.03\n",
      "77.81\n",
      "77.54\n",
      "78.25\n",
      "79.75\n",
      "80.89\n",
      "80.28\n",
      "TwitterURLCorpus\n",
      "77.35\n",
      "79.41\n",
      "76.29\n",
      "81.37\n",
      "83.89\n",
      "83.17\n",
      "85.21\n",
      "69.22\n",
      "84.58\n",
      "81.47\n",
      "84.70\n",
      "84.77\n",
      "83.83\n",
      "85.11\n",
      "85.14\n",
      "83.69\n",
      "80.51\n",
      "84.54\n",
      "81.06\n",
      "83.79\n",
      "84.80\n",
      "85.29\n",
      "85.46\n",
      "84.77\n",
      "84.89\n",
      "85.14\n",
      "85.13\n",
      "86.05\n",
      "86.14\n",
      "85.86\n",
      "86.01\n",
      "AskUbuntuDupQuestions\n",
      "49.57\n",
      "50.88\n",
      "45.84\n",
      "51.57\n",
      "51.80\n",
      "58.99\n",
      "56.69\n",
      "50.07\n",
      "52.75\n",
      "48.99\n",
      "63.48\n",
      "64.06\n",
      "60.49\n",
      "65.85\n",
      "60.16\n",
      "53.49\n",
      "52.63\n",
      "55.90\n",
      "55.84\n",
      "58.13\n",
      "59.63\n",
      "61.63\n",
      "59.97\n",
      "60.86\n",
      "61.64\n",
      "63.08\n",
      "63.23\n",
      "59.73\n",
      "61.51\n",
      "62.86\n",
      "66.16\n",
      "MindSmallReranking\n",
      "27.01\n",
      "28.92\n",
      "28.37\n",
      "28.62\n",
      "29.30\n",
      "27.13\n",
      "31.58\n",
      "24.80\n",
      "29.81\n",
      "24.79\n",
      "30.80\n",
      "31.02\n",
      "30.37\n",
      "30.97\n",
      "30.15\n",
      "30.71\n",
      "29.27\n",
      "31.11\n",
      "30.40\n",
      "31.34\n",
      "31.72\n",
      "32.29\n",
      "31.79\n",
      "31.33\n",
      "31.84\n",
      "31.50\n",
      "31.93\n",
      "30.20\n",
      "30.27\n",
      "29.77\n",
      "30.60\n",
      "SciDocsRR\n",
      "62.56\n",
      "63.55\n",
      "64.94\n",
      "66.33\n",
      "70.14\n",
      "72.78\n",
      "76.51\n",
      "81.31\n",
      "68.72\n",
      "54.99\n",
      "87.12\n",
      "87.20\n",
      "77.78\n",
      "88.65\n",
      "78.09\n",
      "71.04\n",
      "68.36\n",
      "77.54\n",
      "71.34\n",
      "77.21\n",
      "77.72\n",
      "80.79\n",
      "79.77\n",
      "73.71\n",
      "76.39\n",
      "76.49\n",
      "77.96\n",
      "73.96\n",
      "74.88\n",
      "75.16\n",
      "76.09\n",
      "StackOverﬂowDupQuestions\n",
      "34.03\n",
      "35.65\n",
      "34.62\n",
      "39.35\n",
      "38.90\n",
      "48.48\n",
      "47.78\n",
      "36.22\n",
      "42.42\n",
      "36.98\n",
      "50.76\n",
      "51.47\n",
      "45.85\n",
      "51.98\n",
      "46.79\n",
      "40.85\n",
      "39.97\n",
      "44.77\n",
      "44.74\n",
      "49.32\n",
      "49.61\n",
      "51.53\n",
      "51.07\n",
      "51.01\n",
      "51.58\n",
      "52.79\n",
      "53.50\n",
      "48.46\n",
      "49.34\n",
      "51.05\n",
      "52.85\n",
      "ArguAna\n",
      "36.30\n",
      "30.96\n",
      "28.29\n",
      "38.34\n",
      "38.33\n",
      "45.15\n",
      "48.32\n",
      "32.67\n",
      "34.18\n",
      "12.86\n",
      "50.17\n",
      "47.13\n",
      "44.88\n",
      "46.52\n",
      "48.91\n",
      "39.65\n",
      "31.04\n",
      "35.07\n",
      "45.42\n",
      "49.68\n",
      "50.49\n",
      "51.38\n",
      "47.28\n",
      "50.83\n",
      "52.09\n",
      "52.81\n",
      "53.77\n",
      "44.85\n",
      "39.27\n",
      "39.40\n",
      "39.85\n",
      "ClimateFEVER\n",
      "14.44\n",
      "14.87\n",
      "5.41\n",
      "11.80\n",
      "11.98\n",
      "16.96\n",
      "24.79\n",
      "6.86\n",
      "3.83\n",
      "0.36\n",
      "20.27\n",
      "21.57\n",
      "18.49\n",
      "21.97\n",
      "15.27\n",
      "2.83\n",
      "11.01\n",
      "17.57\n",
      "21.86\n",
      "26.6\n",
      "27.11\n",
      "30.46\n",
      "29.39\n",
      "24.88\n",
      "26.90\n",
      "27.01\n",
      "27.21\n",
      "10.37\n",
      "11.36\n",
      "10.61\n",
      "14.63\n",
      "CQADupstackRetrieval\n",
      "15.47\n",
      "16.79\n",
      "5.51\n",
      "13.22\n",
      "14.50\n",
      "27.72\n",
      "33.67\n",
      "14.60\n",
      "18.75\n",
      "4.12\n",
      "41.32\n",
      "42.53\n",
      "30.71\n",
      "44.96\n",
      "31.32\n",
      "10.17\n",
      "20.29\n",
      "29.98\n",
      "27.25\n",
      "33.33\n",
      "36.53\n",
      "39.40\n",
      "39.62\n",
      "34.55\n",
      "36.62\n",
      "37.35\n",
      "38.56\n",
      "35.23\n",
      "38.96\n",
      "40.78\n",
      "44.65\n",
      "DBPedia\n",
      "18.29\n",
      "15.88\n",
      "4.13\n",
      "15.04\n",
      "19.73\n",
      "27.86\n",
      "38.10\n",
      "4.14\n",
      "15.57\n",
      "1.53\n",
      "32.33\n",
      "33.36\n",
      "22.63\n",
      "32.09\n",
      "26.22\n",
      "3.48\n",
      "10.87\n",
      "26.10\n",
      "22.72\n",
      "31.51\n",
      "34.70\n",
      "39.87\n",
      "39.03\n",
      "35.24\n",
      "39.55\n",
      "39.74\n",
      "41.28\n",
      "27.77\n",
      "31.55\n",
      "33.65\n",
      "39.19\n",
      "FEVER\n",
      "14.99\n",
      "15.56\n",
      "3.30\n",
      "21.05\n",
      "20.41\n",
      "45.68\n",
      "59.29\n",
      "5.45\n",
      "12.17\n",
      "0.77\n",
      "51.93\n",
      "55.91\n",
      "52.66\n",
      "50.86\n",
      "56.76\n",
      "4.45\n",
      "18.40\n",
      "38.64\n",
      "60.45\n",
      "68.12\n",
      "72.73\n",
      "78.24\n",
      "73.97\n",
      "68.93\n",
      "72.66\n",
      "72.18\n",
      "74.08\n",
      "26.16\n",
      "36.21\n",
      "36.12\n",
      "51.20\n",
      "FiQA2018\n",
      "10.09\n",
      "10.49\n",
      "2.19\n",
      "9.84\n",
      "10.41\n",
      "15.62\n",
      "27.42\n",
      "5.64\n",
      "7.00\n",
      "1.73\n",
      "36.87\n",
      "37.27\n",
      "20.33\n",
      "49.96\n",
      "22.96\n",
      "7.54\n",
      "8.94\n",
      "18.59\n",
      "21.12\n",
      "29.99\n",
      "33.29\n",
      "37.20\n",
      "35.84\n",
      "35.15\n",
      "42.79\n",
      "44.19\n",
      "46.78\n",
      "34.83\n",
      "43.55\n",
      "44.71\n",
      "46.68\n",
      "HotpotQA\n",
      "19.18\n",
      "20.77\n",
      "8.26\n",
      "19.75\n",
      "22.89\n",
      "35.61\n",
      "56.81\n",
      "5.46\n",
      "18.75\n",
      "5.50\n",
      "46.51\n",
      "44.59\n",
      "30.01\n",
      "39.29\n",
      "37.03\n",
      "12.6\n",
      "17.73\n",
      "33.99\n",
      "40.88\n",
      "49.93\n",
      "52.84\n",
      "59.26\n",
      "57.26\n",
      "54.93\n",
      "57.85\n",
      "58.91\n",
      "59.67\n",
      "33.20\n",
      "33.95\n",
      "37.17\n",
      "42.14\n",
      "MSMARCO\n",
      "9.60\n",
      "9.75\n",
      "1.91\n",
      "9.35\n",
      "11.00\n",
      "29.57\n",
      "36.77\n",
      "5.58\n",
      "7.60\n",
      "1.09\n",
      "36.54\n",
      "39.03\n",
      "23.72\n",
      "39.75\n",
      "26.60\n",
      "10.53\n",
      "6.27\n",
      "15.83\n",
      "27.98\n",
      "36.05\n",
      "38.83\n",
      "39.91\n",
      "41.12\n",
      "41.16\n",
      "42.73\n",
      "43.52\n",
      "44.05\n",
      "20.71\n",
      "23.96\n",
      "25.17\n",
      "27.68\n",
      "NFCorpus\n",
      "13.87\n",
      "11.79\n",
      "4.30\n",
      "9.88\n",
      "12.42\n",
      "22.29\n",
      "31.31\n",
      "0.84\n",
      "16.54\n",
      "2.44\n",
      "31.59\n",
      "32.25\n",
      "23.45\n",
      "33.29\n",
      "25.49\n",
      "20.59\n",
      "11.80\n",
      "28.26\n",
      "22.79\n",
      "32.08\n",
      "33.89\n",
      "36.21\n",
      "35.78\n",
      "30.22\n",
      "32.63\n",
      "33.34\n",
      "34.18\n",
      "28.64\n",
      "31.10\n",
      "33.18\n",
      "35.08\n",
      "NQ\n",
      "12.87\n",
      "12.75\n",
      "2.61\n",
      "11.69\n",
      "16.08\n",
      "29.85\n",
      "41.83\n",
      "5.99\n",
      "8.42\n",
      "0.64\n",
      "43.87\n",
      "46.47\n",
      "29.80\n",
      "50.45\n",
      "33.60\n",
      "2.02\n",
      "7.63\n",
      "24.63\n",
      "29.73\n",
      "42.94\n",
      "46.70\n",
      "52.41\n",
      "53.15\n",
      "50.47\n",
      "55.09\n",
      "56.16\n",
      "57.24\n",
      "36.32\n",
      "42.02\n",
      "46.29\n",
      "52.87\n",
      "QuoraRetrieval\n",
      "71.32\n",
      "71.58\n",
      "61.03\n",
      "78.03\n",
      "79.62\n",
      "86.51\n",
      "86.72\n",
      "64.65\n",
      "77.03\n",
      "71.14\n",
      "87.56\n",
      "87.75\n",
      "86.55\n",
      "87.46\n",
      "86.41\n",
      "82.18\n",
      "78.96\n",
      "84.68\n",
      "72.98\n",
      "85.28\n",
      "85.60\n",
      "84.58\n",
      "74.71\n",
      "87.98\n",
      "88.47\n",
      "88.91\n",
      "89.09\n",
      "85.49\n",
      "85.73\n",
      "85.85\n",
      "85.96\n",
      "SCIDOCS\n",
      "8.04\n",
      "8.47\n",
      "2.81\n",
      "5.50\n",
      "7.53\n",
      "10.13\n",
      "17.12\n",
      "0.00\n",
      "5.63\n",
      "0.78\n",
      "21.64\n",
      "21.82\n",
      "0.03\n",
      "23.77\n",
      "13.96\n",
      "6.28\n",
      "7.13\n",
      "13.55\n",
      "12.21\n",
      "16.18\n",
      "16.57\n",
      "19.87\n",
      "18.62\n",
      "14.00\n",
      "15.51\n",
      "15.71\n",
      "15.88\n",
      "14.16\n",
      "15.38\n",
      "15.97\n",
      "17.17\n",
      "SciFact\n",
      "29.58\n",
      "29.53\n",
      "13.34\n",
      "25.72\n",
      "29.59\n",
      "52.31\n",
      "65.51\n",
      "47.88\n",
      "38.20\n",
      "4.04\n",
      "64.51\n",
      "62.64\n",
      "48.37\n",
      "65.57\n",
      "50.30\n",
      "45.46\n",
      "31.79\n",
      "46.66\n",
      "56.90\n",
      "68.29\n",
      "70.17\n",
      "74.70\n",
      "72.11\n",
      "59.74\n",
      "63.42\n",
      "64.20\n",
      "66.77\n",
      "45.76\n",
      "49.91\n",
      "50.91\n",
      "55.38\n",
      "Touche2020\n",
      "13.99\n",
      "13.17\n",
      "0.97\n",
      "8.90\n",
      "9.89\n",
      "8.57\n",
      "15.79\n",
      "8.46\n",
      "4.88\n",
      "1.06\n",
      "16.90\n",
      "17.22\n",
      "16.06\n",
      "19.93\n",
      "17.40\n",
      "3.1\n",
      "12.27\n",
      "16.18\n",
      "22.97\n",
      "24.45\n",
      "23.44\n",
      "25.43\n",
      "23.98\n",
      "25.89\n",
      "28.29\n",
      "25.26\n",
      "26.76\n",
      "20.30\n",
      "21.63\n",
      "22.51\n",
      "21.65\n",
      "TRECCOVID\n",
      "36.22\n",
      "35.92\n",
      "14.74\n",
      "26.2\n",
      "22.93\n",
      "40.54\n",
      "44.77\n",
      "29.91\n",
      "16.34\n",
      "10.97\n",
      "47.25\n",
      "50.82\n",
      "39.12\n",
      "51.33\n",
      "37.87\n",
      "24.56\n",
      "39.31\n",
      "55.35\n",
      "70.30\n",
      "72.98\n",
      "75.17\n",
      "84.88\n",
      "81.37\n",
      "56.05\n",
      "56.68\n",
      "60.09\n",
      "51.90\n",
      "40.70\n",
      "46.11\n",
      "54.77\n",
      "59.48\n",
      "BIOSSES\n",
      "44.93\n",
      "50.25\n",
      "54.70\n",
      "72.31\n",
      "68.38\n",
      "77.32\n",
      "83.32\n",
      "64.95\n",
      "78.70\n",
      "62.01\n",
      "81.64\n",
      "83.57\n",
      "74.18\n",
      "80.43\n",
      "76.27\n",
      "78.04\n",
      "70.93\n",
      "79.50\n",
      "75.21\n",
      "83.02\n",
      "84.84\n",
      "86.25\n",
      "85.31\n",
      "79.00\n",
      "84.86\n",
      "78.94\n",
      "81.91\n",
      "75.89\n",
      "78.93\n",
      "73.12\n",
      "80.43\n",
      "SICK-R\n",
      "55.43\n",
      "55.49\n",
      "58.65\n",
      "72.24\n",
      "80.77\n",
      "72.00\n",
      "70.20\n",
      "56.39\n",
      "69.99\n",
      "62.86\n",
      "77.58\n",
      "79.32\n",
      "79.61\n",
      "80.59\n",
      "79.62\n",
      "77.48\n",
      "74.57\n",
      "79.59\n",
      "65.93\n",
      "67.23\n",
      "68.20\n",
      "69.63\n",
      "69.82\n",
      "71.45\n",
      "73.39\n",
      "73.63\n",
      "74.29\n",
      "80.18\n",
      "80.34\n",
      "79.98\n",
      "80.47\n",
      "STS12\n",
      "54.64\n",
      "53.51\n",
      "30.87\n",
      "66.05\n",
      "75.30\n",
      "68.19\n",
      "64.34\n",
      "62.49\n",
      "65.08\n",
      "62.60\n",
      "72.37\n",
      "73.08\n",
      "76.02\n",
      "72.63\n",
      "77.90\n",
      "72.30\n",
      "69.17\n",
      "74.29\n",
      "66.53\n",
      "66.59\n",
      "66.99\n",
      "67.50\n",
      "69.66\n",
      "68.59\n",
      "70.33\n",
      "69.11\n",
      "70.12\n",
      "78.05\n",
      "79.11\n",
      "79.02\n",
      "78.85\n",
      "STS13\n",
      "69.16\n",
      "70.80\n",
      "59.89\n",
      "81.49\n",
      "84.67\n",
      "80.40\n",
      "80.03\n",
      "58.70\n",
      "67.98\n",
      "59.62\n",
      "80.60\n",
      "82.13\n",
      "80.70\n",
      "83.48\n",
      "85.11\n",
      "81.49\n",
      "77.23\n",
      "85.35\n",
      "76.17\n",
      "77.33\n",
      "77.58\n",
      "79.16\n",
      "79.67\n",
      "79.09\n",
      "82.19\n",
      "81.82\n",
      "82.72\n",
      "85.85\n",
      "87.33\n",
      "88.80\n",
      "88.94\n",
      "STS14\n",
      "60.81\n",
      "63.56\n",
      "47.73\n",
      "73.61\n",
      "80.19\n",
      "74.02\n",
      "74.51\n",
      "54.87\n",
      "64.03\n",
      "57.03\n",
      "75.59\n",
      "76.73\n",
      "78.85\n",
      "78.00\n",
      "80.81\n",
      "74.74\n",
      "70.99\n",
      "79.21\n",
      "69.05\n",
      "71.83\n",
      "72.78\n",
      "74.46\n",
      "74.61\n",
      "74.64\n",
      "77.16\n",
      "77.07\n",
      "78.24\n",
      "82.19\n",
      "83.17\n",
      "84.33\n",
      "84.86\n",
      "STS15\n",
      "72.31\n",
      "74.08\n",
      "60.29\n",
      "79.72\n",
      "85.40\n",
      "82.57\n",
      "83.30\n",
      "62.54\n",
      "76.59\n",
      "71.57\n",
      "85.39\n",
      "85.58\n",
      "85.84\n",
      "85.66\n",
      "87.48\n",
      "84.28\n",
      "79.74\n",
      "85.52\n",
      "79.24\n",
      "80.66\n",
      "82.62\n",
      "84.47\n",
      "83.81\n",
      "84.85\n",
      "86.31\n",
      "86.01\n",
      "86.26\n",
      "87.46\n",
      "88.28\n",
      "88.89\n",
      "89.32\n",
      "STS16\n",
      "65.34\n",
      "64.60\n",
      "63.73\n",
      "78.12\n",
      "80.82\n",
      "79.78\n",
      "79.67\n",
      "64.27\n",
      "72.98\n",
      "70.75\n",
      "78.99\n",
      "80.23\n",
      "81.05\n",
      "80.03\n",
      "83.20\n",
      "82.06\n",
      "77.93\n",
      "82.54\n",
      "76.07\n",
      "78.91\n",
      "80.10\n",
      "80.96\n",
      "80.40\n",
      "81.57\n",
      "81.85\n",
      "82.23\n",
      "81.61\n",
      "84.03\n",
      "84.36\n",
      "85.31\n",
      "84.67\n",
      "STS17\n",
      "77.95\n",
      "76.91\n",
      "64.10\n",
      "83.58\n",
      "89.44\n",
      "85.94\n",
      "86.32\n",
      "69.63\n",
      "79.45\n",
      "76.73\n",
      "87.59\n",
      "88.63\n",
      "86.87\n",
      "90.60\n",
      "86.99\n",
      "87.08\n",
      "87.33\n",
      "90.44\n",
      "84.95\n",
      "86.99\n",
      "87.25\n",
      "87.78\n",
      "87.07\n",
      "85.80\n",
      "83.93\n",
      "84.90\n",
      "85.18\n",
      "89.57\n",
      "88.99\n",
      "88.91\n",
      "89.46\n",
      "STS22\n",
      "56.35\n",
      "53.89\n",
      "56.37\n",
      "59.65\n",
      "61.96\n",
      "67.54\n",
      "64.64\n",
      "55.06\n",
      "60.97\n",
      "39.75\n",
      "67.21\n",
      "65.67\n",
      "61.72\n",
      "67.95\n",
      "63.06\n",
      "64.71\n",
      "59.64\n",
      "63.20\n",
      "65.66\n",
      "67.30\n",
      "68.75\n",
      "69.35\n",
      "66.13\n",
      "66.17\n",
      "64.30\n",
      "66.61\n",
      "65.76\n",
      "62.66\n",
      "62.39\n",
      "64.32\n",
      "65.33\n",
      "STSBenchmark\n",
      "61.54\n",
      "61.55\n",
      "47.29\n",
      "76.52\n",
      "84.25\n",
      "76.97\n",
      "78.81\n",
      "61.26\n",
      "72.25\n",
      "69.77\n",
      "82.03\n",
      "83.09\n",
      "84.42\n",
      "83.42\n",
      "86.82\n",
      "83.78\n",
      "79.54\n",
      "85.67\n",
      "75.34\n",
      "77.59\n",
      "79.21\n",
      "81.39\n",
      "80.90\n",
      "79.58\n",
      "77.60\n",
      "77.65\n",
      "77.73\n",
      "85.52\n",
      "85.36\n",
      "83.93\n",
      "84.01\n",
      "SummEval\n",
      "28.87\n",
      "30.49\n",
      "29.82\n",
      "31.15\n",
      "23.31\n",
      "29.50\n",
      "30.36\n",
      "27.66\n",
      "31.05\n",
      "26.8\n",
      "30.81\n",
      "27.9\n",
      "30.67\n",
      "27.49\n",
      "31.57\n",
      "26.94\n",
      "30.26\n",
      "30.38\n",
      "28.90\n",
      "25.44\n",
      "27.87\n",
      "24.75\n",
      "24.99\n",
      "29.67\n",
      "29.50\n",
      "30.21\n",
      "30.64\n",
      "31.39\n",
      "29.64\n",
      "29.91\n",
      "30.08\n",
      "Average\n",
      "41.97\n",
      "42.06\n",
      "38.33\n",
      "45.45\n",
      "48.72\n",
      "52.35\n",
      "56.00\n",
      "40.28\n",
      "45.21\n",
      "34.95\n",
      "56.26\n",
      "56.53\n",
      "52.44\n",
      "57.78\n",
      "54.71\n",
      "49.52\n",
      "45.97\n",
      "53.74\n",
      "51.23\n",
      "56.11\n",
      "57.12\n",
      "58.81\n",
      "57.44\n",
      "56.19\n",
      "58.28\n",
      "58.42\n",
      "58.97\n",
      "55.27\n",
      "57.06\n",
      "57.87\n",
      "59.51\n",
      "Table 11: All English results. The main score for each task is reported as described in Section 3.2.\n",
      "Dataset\n",
      "Language\n",
      "LASER2\n",
      "LaBSE\n",
      "MiniLM-L12-multilingual\n",
      "MPNet-multilingual\n",
      "SGPT-BLOOM-7.1B-msmarco\n",
      "BUCC\n",
      "de-en\n",
      "99.21\n",
      "99.35\n",
      "97.11\n",
      "98.59\n",
      "54.00\n",
      "BUCC\n",
      "fr-en\n",
      "98.39\n",
      "98.72\n",
      "94.99\n",
      "96.89\n",
      "97.06\n",
      "BUCC\n",
      "ru-en\n",
      "97.62\n",
      "97.78\n",
      "95.06\n",
      "96.44\n",
      "45.30\n",
      "BUCC\n",
      "zh-en\n",
      "97.70\n",
      "99.16\n",
      "95.63\n",
      "97.56\n",
      "97.96\n",
      "Tatoeba\n",
      "sqi-eng\n",
      "97.22\n",
      "96.76\n",
      "98.17\n",
      "98.57\n",
      "10.38\n",
      "Tatoeba\n",
      "fry-eng\n",
      "42.07\n",
      "89.31\n",
      "31.13\n",
      "43.54\n",
      "24.62\n",
      "Tatoeba\n",
      "kur-eng\n",
      "19.09\n",
      "83.59\n",
      "46.94\n",
      "61.44\n",
      "8.26\n",
      "Tatoeba\n",
      "tur-eng\n",
      "98.03\n",
      "98.00\n",
      "95.08\n",
      "96.17\n",
      "6.15\n",
      "Tatoeba\n",
      "deu-eng\n",
      "99.07\n",
      "99.20\n",
      "97.02\n",
      "97.73\n",
      "70.10\n",
      "Tatoeba\n",
      "nld-eng\n",
      "95.35\n",
      "96.07\n",
      "94.58\n",
      "95.50\n",
      "29.74\n",
      "Tatoeba\n",
      "ron-eng\n",
      "96.52\n",
      "96.92\n",
      "95.30\n",
      "96.43\n",
      "27.23\n",
      "Tatoeba\n",
      "ang-eng\n",
      "25.22\n",
      "59.28\n",
      "10.24\n",
      "16.72\n",
      "28.76\n",
      "Tatoeba\n",
      "ido-eng\n",
      "80.86\n",
      "89.42\n",
      "40.25\n",
      "43.91\n",
      "43.91\n",
      "Tatoeba\n",
      "jav-eng\n",
      "9.95\n",
      "79.77\n",
      "17.04\n",
      "23.39\n",
      "15.02\n",
      "Tatoeba\n",
      "isl-eng\n",
      "94.32\n",
      "94.75\n",
      "24.07\n",
      "59.25\n",
      "6.29\n",
      "Tatoeba\n",
      "slv-eng\n",
      "95.40\n",
      "96.03\n",
      "96.92\n",
      "97.08\n",
      "10.14\n",
      "Tatoeba\n",
      "cym-eng\n",
      "5.85\n",
      "92.00\n",
      "13.25\n",
      "22.31\n",
      "6.97\n",
      "Tatoeba\n",
      "kaz-eng\n",
      "53.30\n",
      "87.49\n",
      "34.89\n",
      "61.49\n",
      "3.32\n",
      "Tatoeba\n",
      "est-eng\n",
      "96.43\n",
      "96.55\n",
      "97.33\n",
      "98.40\n",
      "4.76\n",
      "Tatoeba\n",
      "heb-eng\n",
      "0.00\n",
      "91.53\n",
      "86.88\n",
      "88.26\n",
      "1.69\n",
      "Tatoeba\n",
      "gla-eng\n",
      "1.52\n",
      "85.66\n",
      "3.61\n",
      "4.72\n",
      "2.09\n",
      "Tatoeba\n",
      "mar-eng\n",
      "92.93\n",
      "92.65\n",
      "92.38\n",
      "93.83\n",
      "45.53\n",
      "Tatoeba\n",
      "lat-eng\n",
      "64.81\n",
      "80.07\n",
      "19.47\n",
      "24.25\n",
      "28.76\n",
      "Tatoeba\n",
      "bel-eng\n",
      "79.54\n",
      "95.00\n",
      "67.73\n",
      "79.94\n",
      "8.03\n",
      "Tatoeba\n",
      "pms-eng\n",
      "36.23\n",
      "64.57\n",
      "30.70\n",
      "34.19\n",
      "31.94\n",
      "Tatoeba\n",
      "gle-eng\n",
      "4.20\n",
      "93.80\n",
      "11.62\n",
      "16.85\n",
      "3.26\n",
      "Tatoeba\n",
      "pes-eng\n",
      "93.13\n",
      "94.70\n",
      "92.59\n",
      "93.47\n",
      "12.13\n",
      "Tatoeba\n",
      "nob-eng\n",
      "95.77\n",
      "98.40\n",
      "97.73\n",
      "98.53\n",
      "21.07\n",
      "Tatoeba\n",
      "bul-eng\n",
      "93.57\n",
      "94.58\n",
      "92.65\n",
      "93.52\n",
      "20.09\n",
      "Tatoeba\n",
      "cbk-eng\n",
      "77.17\n",
      "79.44\n",
      "55.37\n",
      "58.68\n",
      "64.63\n",
      "Tatoeba\n",
      "hun-eng\n",
      "95.20\n",
      "96.55\n",
      "91.58\n",
      "94.18\n",
      "5.07\n",
      "Tatoeba\n",
      "uig-eng\n",
      "56.49\n",
      "92.40\n",
      "24.39\n",
      "48.35\n",
      "1.27\n",
      "Tatoeba\n",
      "rus-eng\n",
      "92.58\n",
      "93.75\n",
      "91.87\n",
      "92.92\n",
      "59.84\n",
      "Tatoeba\n",
      "spa-eng\n",
      "97.33\n",
      "98.40\n",
      "95.42\n",
      "97.00\n",
      "94.48\n",
      "Tatoeba\n",
      "hye-eng\n",
      "88.72\n",
      "94.09\n",
      "93.28\n",
      "94.38\n",
      "0.50\n",
      "Tatoeba\n",
      "tel-eng\n",
      "96.72\n",
      "97.86\n",
      "36.40\n",
      "79.73\n",
      "64.62\n",
      "Tatoeba\n",
      "afr-eng\n",
      "92.59\n",
      "96.18\n",
      "58.22\n",
      "72.96\n",
      "16.62\n",
      "Tatoeba\n",
      "mon-eng\n",
      "3.42\n",
      "95.91\n",
      "95.04\n",
      "96.14\n",
      "2.85\n",
      "Tatoeba\n",
      "arz-eng\n",
      "66.16\n",
      "76.00\n",
      "51.26\n",
      "55.69\n",
      "70.66\n",
      "Tatoeba\n",
      "hrv-eng\n",
      "96.72\n",
      "96.95\n",
      "95.98\n",
      "97.00\n",
      "12.79\n",
      "Tatoeba\n",
      "nov-eng\n",
      "60.02\n",
      "74.38\n",
      "47.99\n",
      "50.23\n",
      "52.23\n",
      "Tatoeba\n",
      "gsw-eng\n",
      "27.52\n",
      "46.50\n",
      "25.74\n",
      "25.12\n",
      "21.03\n",
      "Tatoeba\n",
      "nds-eng\n",
      "77.13\n",
      "79.42\n",
      "32.16\n",
      "38.88\n",
      "23.92\n",
      "Tatoeba\n",
      "ukr-eng\n",
      "93.52\n",
      "93.97\n",
      "92.82\n",
      "92.67\n",
      "22.06\n",
      "Tatoeba\n",
      "uzb-eng\n",
      "23.20\n",
      "84.23\n",
      "17.14\n",
      "23.19\n",
      "4.71\n",
      "Tatoeba\n",
      "lit-eng\n",
      "96.20\n",
      "96.47\n",
      "93.16\n",
      "95.37\n",
      "4.49\n",
      "Tatoeba\n",
      "ina-eng\n",
      "93.93\n",
      "95.37\n",
      "79.13\n",
      "84.32\n",
      "73.67\n",
      "Tatoeba\n",
      "lfn-eng\n",
      "63.39\n",
      "67.54\n",
      "47.02\n",
      "49.56\n",
      "44.85\n",
      "Tatoeba\n",
      "zsm-eng\n",
      "95.41\n",
      "95.62\n",
      "95.31\n",
      "95.80\n",
      "79.95\n",
      "Tatoeba\n",
      "ita-eng\n",
      "94.32\n",
      "92.72\n",
      "93.05\n",
      "93.76\n",
      "65.04\n",
      "Tatoeba\n",
      "cmn-eng\n",
      "85.62\n",
      "95.10\n",
      "94.93\n",
      "95.83\n",
      "91.45\n",
      "Tatoeba\n",
      "lvs-eng\n",
      "95.33\n",
      "95.88\n",
      "97.87\n",
      "97.53\n",
      "6.55\n",
      "Tatoeba\n",
      "glg-eng\n",
      "96.14\n",
      "96.82\n",
      "94.00\n",
      "95.32\n",
      "79.86\n",
      "Tatoeba\n",
      "ceb-eng\n",
      "9.93\n",
      "64.42\n",
      "8.05\n",
      "7.39\n",
      "6.64\n",
      "Tatoeba\n",
      "bre-eng\n",
      "31.2\n",
      "15.07\n",
      "5.56\n",
      "6.42\n",
      "4.67\n",
      "Tatoeba\n",
      "ben-eng\n",
      "89.43\n",
      "88.55\n",
      "36.48\n",
      "64.90\n",
      "75.98\n",
      "Tatoeba\n",
      "swg-eng\n",
      "33.10\n",
      "59.36\n",
      "26.31\n",
      "22.80\n",
      "16.89\n",
      "Tatoeba\n",
      "arq-eng\n",
      "26.63\n",
      "42.69\n",
      "18.60\n",
      "19.84\n",
      "27.75\n",
      "Tatoeba\n",
      "kab-eng\n",
      "65.88\n",
      "4.31\n",
      "1.16\n",
      "1.41\n",
      "1.69\n",
      "Tatoeba\n",
      "fra-eng\n",
      "94.28\n",
      "94.86\n",
      "91.72\n",
      "93.12\n",
      "91.44\n",
      "Tatoeba\n",
      "por-eng\n",
      "94.54\n",
      "94.14\n",
      "92.13\n",
      "93.02\n",
      "92.62\n",
      "Tatoeba\n",
      "tat-eng\n",
      "34.74\n",
      "85.92\n",
      "10.25\n",
      "10.89\n",
      "3.59\n",
      "Tatoeba\n",
      "oci-eng\n",
      "58.13\n",
      "65.81\n",
      "38.57\n",
      "43.49\n",
      "40.17\n",
      "Tatoeba\n",
      "pol-eng\n",
      "97.32\n",
      "97.22\n",
      "94.28\n",
      "96.95\n",
      "14.09\n",
      "Tatoeba\n",
      "war-eng\n",
      "8.25\n",
      "60.29\n",
      "7.25\n",
      "7.42\n",
      "10.38\n",
      "Tatoeba\n",
      "aze-eng\n",
      "82.41\n",
      "94.93\n",
      "62.10\n",
      "76.36\n",
      "6.32\n",
      "Tatoeba\n",
      "vie-eng\n",
      "96.73\n",
      "97.20\n",
      "95.12\n",
      "97.23\n",
      "94.20\n",
      "Tatoeba\n",
      "nno-eng\n",
      "72.75\n",
      "94.48\n",
      "76.34\n",
      "81.41\n",
      "16.28\n",
      "Tatoeba\n",
      "cha-eng\n",
      "14.86\n",
      "31.77\n",
      "15.98\n",
      "12.59\n",
      "23.26\n",
      "Tatoeba\n",
      "mhr-eng\n",
      "6.86\n",
      "15.74\n",
      "6.89\n",
      "7.57\n",
      "1.56\n",
      "Tatoeba\n",
      "dan-eng\n",
      "95.22\n",
      "95.71\n",
      "94.80\n",
      "96.17\n",
      "23.52\n",
      "Tatoeba\n",
      "ell-eng\n",
      "96.20\n",
      "95.35\n",
      "95.43\n",
      "94.93\n",
      "5.34\n",
      "Tatoeba\n",
      "amh-eng\n",
      "80.82\n",
      "91.47\n",
      "36.21\n",
      "53.49\n",
      "0.03\n",
      "Tatoeba\n",
      "pam-eng\n",
      "3.24\n",
      "10.73\n",
      "5.41\n",
      "5.39\n",
      "5.85\n",
      "Tatoeba\n",
      "hsb-eng\n",
      "45.75\n",
      "67.11\n",
      "36.10\n",
      "44.32\n",
      "9.68\n",
      "Tatoeba\n",
      "srp-eng\n",
      "93.64\n",
      "94.43\n",
      "92.24\n",
      "94.12\n",
      "11.69\n",
      "Tatoeba\n",
      "epo-eng\n",
      "96.61\n",
      "98.20\n",
      "41.73\n",
      "55.12\n",
      "26.20\n",
      "Tatoeba\n",
      "kzj-eng\n",
      "4.46\n",
      "11.33\n",
      "6.24\n",
      "5.88\n",
      "5.17\n",
      "Tatoeba\n",
      "awa-eng\n",
      "33.74\n",
      "71.70\n",
      "33.43\n",
      "42.83\n",
      "35.01\n",
      "Tatoeba\n",
      "fao-eng\n",
      "57.04\n",
      "87.40\n",
      "27.51\n",
      "38.24\n",
      "12.61\n",
      "Tatoeba\n",
      "mal-eng\n",
      "98.16\n",
      "98.45\n",
      "32.20\n",
      "88.46\n",
      "83.30\n",
      "Tatoeba\n",
      "ile-eng\n",
      "87.88\n",
      "85.58\n",
      "57.71\n",
      "60.36\n",
      "59.59\n",
      "Tatoeba\n",
      "bos-eng\n",
      "95.86\n",
      "94.92\n",
      "93.27\n",
      "94.02\n",
      "13.65\n",
      "Tatoeba\n",
      "cor-eng\n",
      "4.45\n",
      "10.11\n",
      "3.42\n",
      "3.53\n",
      "2.83\n",
      "Tatoeba\n",
      "cat-eng\n",
      "95.80\n",
      "95.38\n",
      "94.42\n",
      "96.05\n",
      "88.31\n",
      "Tatoeba\n",
      "eus-eng\n",
      "93.32\n",
      "95.01\n",
      "23.18\n",
      "31.33\n",
      "53.38\n",
      "Tatoeba\n",
      "yue-eng\n",
      "87.75\n",
      "89.58\n",
      "71.45\n",
      "77.58\n",
      "77.03\n",
      "Tatoeba\n",
      "swe-eng\n",
      "95.31\n",
      "95.63\n",
      "94.42\n",
      "95.45\n",
      "19.53\n",
      "Tatoeba\n",
      "dtp-eng\n",
      "7.39\n",
      "10.85\n",
      "5.69\n",
      "5.03\n",
      "3.41\n",
      "Tatoeba\n",
      "kat-eng\n",
      "81.16\n",
      "95.02\n",
      "95.44\n",
      "95.46\n",
      "0.42\n",
      "Tatoeba\n",
      "jpn-eng\n",
      "93.78\n",
      "95.38\n",
      "90.41\n",
      "92.51\n",
      "71.36\n",
      "Tatoeba\n",
      "csb-eng\n",
      "27.03\n",
      "52.57\n",
      "21.56\n",
      "23.73\n",
      "10.03\n",
      "Tatoeba\n",
      "xho-eng\n",
      "4.68\n",
      "91.55\n",
      "4.52\n",
      "6.53\n",
      "5.51\n",
      "Tatoeba\n",
      "orv-eng\n",
      "23.24\n",
      "38.93\n",
      "15.10\n",
      "23.77\n",
      "5.79\n",
      "Tatoeba\n",
      "ind-eng\n",
      "92.98\n",
      "93.66\n",
      "92.74\n",
      "93.50\n",
      "88.04\n",
      "Tatoeba\n",
      "tuk-eng\n",
      "16.35\n",
      "75.27\n",
      "15.16\n",
      "14.91\n",
      "5.48\n",
      "Tatoeba\n",
      "max-eng\n",
      "36.96\n",
      "63.26\n",
      "45.25\n",
      "48.77\n",
      "36.14\n",
      "Tatoeba\n",
      "swh-eng\n",
      "55.66\n",
      "84.50\n",
      "14.48\n",
      "16.02\n",
      "16.74\n",
      "Tatoeba\n",
      "hin-eng\n",
      "95.32\n",
      "96.87\n",
      "97.62\n",
      "97.75\n",
      "85.23\n",
      "Tatoeba\n",
      "dsb-eng\n",
      "42.34\n",
      "64.81\n",
      "33.43\n",
      "36.85\n",
      "8.78\n",
      "Tatoeba\n",
      "ber-eng\n",
      "77.63\n",
      "8.40\n",
      "4.43\n",
      "4.88\n",
      "4.92\n",
      "Tatoeba\n",
      "tam-eng\n",
      "87.32\n",
      "89.0\n",
      "24.64\n",
      "73.60\n",
      "72.76\n",
      "Tatoeba\n",
      "slk-eng\n",
      "95.82\n",
      "96.5\n",
      "95.15\n",
      "96.62\n",
      "9.98\n",
      "Tatoeba\n",
      "tgl-eng\n",
      "63.19\n",
      "96.02\n",
      "13.09\n",
      "17.67\n",
      "10.70\n",
      "Tatoeba\n",
      "ast-eng\n",
      "76.35\n",
      "90.68\n",
      "62.17\n",
      "70.08\n",
      "71.13\n",
      "Tatoeba\n",
      "mkd-eng\n",
      "93.63\n",
      "93.6\n",
      "91.00\n",
      "93.02\n",
      "10.47\n",
      "Tatoeba\n",
      "khm-eng\n",
      "74.19\n",
      "78.37\n",
      "32.11\n",
      "58.80\n",
      "0.37\n",
      "Tatoeba\n",
      "ces-eng\n",
      "95.52\n",
      "96.68\n",
      "95.12\n",
      "95.73\n",
      "9.55\n",
      "Tatoeba\n",
      "tzl-eng\n",
      "36.56\n",
      "58.88\n",
      "25.46\n",
      "34.21\n",
      "27.82\n",
      "Tatoeba\n",
      "urd-eng\n",
      "84.23\n",
      "93.22\n",
      "94.57\n",
      "95.12\n",
      "70.10\n",
      "Tatoeba\n",
      "ara-eng\n",
      "90.14\n",
      "88.80\n",
      "87.93\n",
      "90.19\n",
      "85.37\n",
      "Tatoeba\n",
      "kor-eng\n",
      "87.97\n",
      "90.95\n",
      "92.52\n",
      "93.07\n",
      "22.39\n",
      "Tatoeba\n",
      "yid-eng\n",
      "2.49\n",
      "88.79\n",
      "14.38\n",
      "30.73\n",
      "0.16\n",
      "Tatoeba\n",
      "ﬁn-eng\n",
      "96.98\n",
      "96.37\n",
      "93.10\n",
      "95.92\n",
      "3.41\n",
      "Tatoeba\n",
      "tha-eng\n",
      "96.38\n",
      "96.14\n",
      "96.72\n",
      "95.99\n",
      "2.22\n",
      "Tatoeba\n",
      "wuu-eng\n",
      "75.09\n",
      "90.18\n",
      "76.00\n",
      "78.25\n",
      "79.58\n",
      "Average\n",
      "mix\n",
      "67.42\n",
      "81.75\n",
      "57.98\n",
      "63.38\n",
      "31.08\n",
      "Table 12: Multilingual bitext mining results. Scores are f1.\n",
      "Dataset\n",
      "Language\n",
      "LASER2\n",
      "LaBSE\n",
      "MiniLM-L12-multilingual\n",
      "MPNet-multilingual\n",
      "SGPT-BLOOM-7.1B-msmarco\n",
      "AmazonCounterfactualClassiﬁcation\n",
      "de\n",
      "67.82\n",
      "73.17\n",
      "68.35\n",
      "69.95\n",
      "61.35\n",
      "AmazonCounterfactualClassiﬁcation\n",
      "ja\n",
      "68.76\n",
      "76.42\n",
      "63.45\n",
      "69.79\n",
      "58.23\n",
      "AmazonReviewsClassiﬁcation\n",
      "de\n",
      "31.07\n",
      "39.92\n",
      "35.91\n",
      "39.52\n",
      "29.70\n",
      "AmazonReviewsClassiﬁcation\n",
      "es\n",
      "32.72\n",
      "39.39\n",
      "37.49\n",
      "39.99\n",
      "35.97\n",
      "AmazonReviewsClassiﬁcation\n",
      "fr\n",
      "31.12\n",
      "38.52\n",
      "35.30\n",
      "39.00\n",
      "35.92\n",
      "AmazonReviewsClassiﬁcation\n",
      "ja\n",
      "28.94\n",
      "36.44\n",
      "33.24\n",
      "36.64\n",
      "27.64\n",
      "AmazonReviewsClassiﬁcation\n",
      "zh\n",
      "30.89\n",
      "36.45\n",
      "35.26\n",
      "37.74\n",
      "32.63\n",
      "MassiveIntentClassiﬁcation\n",
      "af\n",
      "38.01\n",
      "56.12\n",
      "45.88\n",
      "52.32\n",
      "47.85\n",
      "MassiveIntentClassiﬁcation\n",
      "am\n",
      "12.70\n",
      "55.71\n",
      "36.75\n",
      "41.55\n",
      "33.30\n",
      "MassiveIntentClassiﬁcation\n",
      "ar\n",
      "37.16\n",
      "50.86\n",
      "45.14\n",
      "51.43\n",
      "59.25\n",
      "MassiveIntentClassiﬁcation\n",
      "az\n",
      "19.98\n",
      "58.97\n",
      "47.42\n",
      "56.98\n",
      "45.24\n",
      "MassiveIntentClassiﬁcation\n",
      "bn\n",
      "42.51\n",
      "58.22\n",
      "35.34\n",
      "48.79\n",
      "61.59\n",
      "MassiveIntentClassiﬁcation\n",
      "cy\n",
      "17.33\n",
      "50.16\n",
      "26.12\n",
      "27.87\n",
      "44.92\n",
      "MassiveIntentClassiﬁcation\n",
      "da\n",
      "45.61\n",
      "58.25\n",
      "57.73\n",
      "62.77\n",
      "51.23\n",
      "MassiveIntentClassiﬁcation\n",
      "de\n",
      "44.79\n",
      "56.21\n",
      "50.71\n",
      "59.57\n",
      "56.10\n",
      "MassiveIntentClassiﬁcation\n",
      "el\n",
      "46.71\n",
      "57.03\n",
      "58.70\n",
      "62.62\n",
      "46.13\n",
      "MassiveIntentClassiﬁcation\n",
      "es\n",
      "45.44\n",
      "58.32\n",
      "59.66\n",
      "64.43\n",
      "66.35\n",
      "MassiveIntentClassiﬁcation\n",
      "fa\n",
      "45.01\n",
      "62.33\n",
      "61.02\n",
      "65.34\n",
      "51.20\n",
      "MassiveIntentClassiﬁcation\n",
      "ﬁ\n",
      "45.94\n",
      "60.12\n",
      "57.54\n",
      "62.28\n",
      "45.33\n",
      "MassiveIntentClassiﬁcation\n",
      "fr\n",
      "46.13\n",
      "60.47\n",
      "60.25\n",
      "64.82\n",
      "66.95\n",
      "MassiveIntentClassiﬁcation\n",
      "he\n",
      "42.55\n",
      "56.55\n",
      "52.51\n",
      "58.21\n",
      "43.18\n",
      "MassiveIntentClassiﬁcation\n",
      "hi\n",
      "40.20\n",
      "59.40\n",
      "58.37\n",
      "62.77\n",
      "63.54\n",
      "MassiveIntentClassiﬁcation\n",
      "hu\n",
      "42.77\n",
      "59.52\n",
      "60.41\n",
      "63.87\n",
      "44.73\n",
      "MassiveIntentClassiﬁcation\n",
      "hy\n",
      "28.07\n",
      "56.20\n",
      "51.60\n",
      "57.74\n",
      "38.13\n",
      "MassiveIntentClassiﬁcation\n",
      "id\n",
      "45.81\n",
      "61.12\n",
      "59.85\n",
      "65.43\n",
      "64.06\n",
      "MassiveIntentClassiﬁcation\n",
      "is\n",
      "39.86\n",
      "54.90\n",
      "30.83\n",
      "37.05\n",
      "44.35\n",
      "MassiveIntentClassiﬁcation\n",
      "it\n",
      "48.25\n",
      "59.83\n",
      "59.61\n",
      "64.68\n",
      "60.77\n",
      "MassiveIntentClassiﬁcation\n",
      "ja\n",
      "45.30\n",
      "63.11\n",
      "60.89\n",
      "63.74\n",
      "61.22\n",
      "MassiveIntentClassiﬁcation\n",
      "jv\n",
      "24.30\n",
      "50.98\n",
      "32.37\n",
      "36.49\n",
      "50.94\n",
      "MassiveIntentClassiﬁcation\n",
      "ka\n",
      "22.70\n",
      "48.35\n",
      "43.03\n",
      "49.85\n",
      "33.84\n",
      "MassiveIntentClassiﬁcation\n",
      "km\n",
      "22.48\n",
      "48.55\n",
      "40.04\n",
      "45.47\n",
      "37.34\n",
      "MassiveIntentClassiﬁcation\n",
      "kn\n",
      "4.32\n",
      "56.24\n",
      "40.98\n",
      "50.63\n",
      "53.54\n",
      "MassiveIntentClassiﬁcation\n",
      "ko\n",
      "44.26\n",
      "60.99\n",
      "50.30\n",
      "61.82\n",
      "53.36\n",
      "MassiveIntentClassiﬁcation\n",
      "lv\n",
      "39.75\n",
      "57.10\n",
      "54.68\n",
      "61.29\n",
      "46.50\n",
      "MassiveIntentClassiﬁcation\n",
      "ml\n",
      "41.33\n",
      "57.91\n",
      "42.41\n",
      "54.34\n",
      "58.27\n",
      "MassiveIntentClassiﬁcation\n",
      "mn\n",
      "16.20\n",
      "58.50\n",
      "51.77\n",
      "56.59\n",
      "40.28\n",
      "MassiveIntentClassiﬁcation\n",
      "ms\n",
      "43.23\n",
      "58.60\n",
      "54.76\n",
      "60.70\n",
      "59.65\n",
      "MassiveIntentClassiﬁcation\n",
      "my\n",
      "25.37\n",
      "57.35\n",
      "52.01\n",
      "57.09\n",
      "37.42\n",
      "MassiveIntentClassiﬁcation\n",
      "nb\n",
      "37.74\n",
      "57.91\n",
      "55.50\n",
      "62.60\n",
      "49.41\n",
      "MassiveIntentClassiﬁcation\n",
      "nl\n",
      "45.00\n",
      "59.37\n",
      "59.51\n",
      "63.57\n",
      "52.09\n",
      "MassiveIntentClassiﬁcation\n",
      "pl\n",
      "44.99\n",
      "59.71\n",
      "59.43\n",
      "64.30\n",
      "50.48\n",
      "MassiveIntentClassiﬁcation\n",
      "pt\n",
      "48.55\n",
      "60.16\n",
      "61.27\n",
      "64.89\n",
      "66.69\n",
      "MassiveIntentClassiﬁcation\n",
      "ro\n",
      "44.30\n",
      "57.92\n",
      "58.39\n",
      "62.80\n",
      "50.53\n",
      "MassiveIntentClassiﬁcation\n",
      "ru\n",
      "44.29\n",
      "60.67\n",
      "59.04\n",
      "63.26\n",
      "58.32\n",
      "MassiveIntentClassiﬁcation\n",
      "sl\n",
      "44.72\n",
      "59.37\n",
      "57.36\n",
      "63.51\n",
      "47.74\n",
      "MassiveIntentClassiﬁcation\n",
      "sq\n",
      "46.12\n",
      "58.03\n",
      "56.59\n",
      "62.49\n",
      "48.94\n",
      "MassiveIntentClassiﬁcation\n",
      "sv\n",
      "45.95\n",
      "59.66\n",
      "59.43\n",
      "64.73\n",
      "50.79\n",
      "MassiveIntentClassiﬁcation\n",
      "sw\n",
      "31.89\n",
      "51.62\n",
      "29.57\n",
      "31.95\n",
      "49.81\n",
      "MassiveIntentClassiﬁcation\n",
      "ta\n",
      "29.63\n",
      "55.04\n",
      "36.77\n",
      "50.17\n",
      "56.40\n",
      "MassiveIntentClassiﬁcation\n",
      "te\n",
      "36.03\n",
      "58.32\n",
      "40.72\n",
      "52.82\n",
      "54.71\n",
      "MassiveIntentClassiﬁcation\n",
      "th\n",
      "43.39\n",
      "56.58\n",
      "58.97\n",
      "61.11\n",
      "44.43\n",
      "MassiveIntentClassiﬁcation\n",
      "tl\n",
      "29.73\n",
      "55.28\n",
      "33.67\n",
      "38.83\n",
      "50.21\n",
      "MassiveIntentClassiﬁcation\n",
      "tr\n",
      "43.93\n",
      "60.91\n",
      "59.90\n",
      "64.54\n",
      "46.56\n",
      "MassiveIntentClassiﬁcation\n",
      "ur\n",
      "26.11\n",
      "56.70\n",
      "52.80\n",
      "56.37\n",
      "56.75\n",
      "MassiveIntentClassiﬁcation\n",
      "vi\n",
      "44.33\n",
      "56.67\n",
      "56.61\n",
      "59.68\n",
      "64.53\n",
      "MassiveIntentClassiﬁcation\n",
      "zh-CN\n",
      "40.62\n",
      "63.86\n",
      "61.99\n",
      "65.33\n",
      "67.07\n",
      "MassiveIntentClassiﬁcation\n",
      "zh-TW\n",
      "32.93\n",
      "59.51\n",
      "58.77\n",
      "62.35\n",
      "62.89\n",
      "MassiveScenarioClassiﬁcation\n",
      "af\n",
      "47.10\n",
      "63.39\n",
      "53.64\n",
      "59.67\n",
      "51.47\n",
      "MassiveScenarioClassiﬁcation\n",
      "am\n",
      "17.70\n",
      "62.02\n",
      "41.89\n",
      "48.97\n",
      "34.87\n",
      "MassiveScenarioClassiﬁcation\n",
      "ar\n",
      "45.21\n",
      "57.72\n",
      "51.74\n",
      "57.78\n",
      "65.21\n",
      "MassiveScenarioClassiﬁcation\n",
      "az\n",
      "28.21\n",
      "63.48\n",
      "52.06\n",
      "61.53\n",
      "45.58\n",
      "MassiveScenarioClassiﬁcation\n",
      "bn\n",
      "50.52\n",
      "61.84\n",
      "41.17\n",
      "54.53\n",
      "67.30\n",
      "MassiveScenarioClassiﬁcation\n",
      "cy\n",
      "22.58\n",
      "56.13\n",
      "31.72\n",
      "35.26\n",
      "46.29\n",
      "MassiveScenarioClassiﬁcation\n",
      "da\n",
      "54.87\n",
      "65.24\n",
      "66.87\n",
      "71.00\n",
      "53.52\n",
      "MassiveScenarioClassiﬁcation\n",
      "de\n",
      "54.34\n",
      "62.39\n",
      "57.40\n",
      "67.34\n",
      "61.74\n",
      "MassiveScenarioClassiﬁcation\n",
      "el\n",
      "55.47\n",
      "64.58\n",
      "66.14\n",
      "68.81\n",
      "48.96\n",
      "MassiveScenarioClassiﬁcation\n",
      "es\n",
      "52.77\n",
      "63.61\n",
      "65.04\n",
      "70.42\n",
      "73.34\n",
      "MassiveScenarioClassiﬁcation\n",
      "fa\n",
      "52.50\n",
      "67.46\n",
      "65.86\n",
      "69.88\n",
      "53.17\n",
      "MassiveScenarioClassiﬁcation\n",
      "ﬁ\n",
      "52.63\n",
      "64.58\n",
      "63.75\n",
      "67.60\n",
      "44.69\n",
      "MassiveScenarioClassiﬁcation\n",
      "fr\n",
      "54.32\n",
      "65.10\n",
      "66.06\n",
      "70.69\n",
      "72.91\n",
      "MassiveScenarioClassiﬁcation\n",
      "he\n",
      "52.41\n",
      "63.53\n",
      "59.20\n",
      "65.16\n",
      "43.10\n",
      "MassiveScenarioClassiﬁcation\n",
      "hi\n",
      "47.37\n",
      "64.40\n",
      "65.21\n",
      "67.92\n",
      "69.27\n",
      "MassiveScenarioClassiﬁcation\n",
      "hu\n",
      "53.43\n",
      "65.82\n",
      "66.56\n",
      "70.30\n",
      "45.16\n",
      "MassiveScenarioClassiﬁcation\n",
      "hy\n",
      "33.57\n",
      "61.25\n",
      "56.11\n",
      "63.02\n",
      "38.73\n",
      "MassiveScenarioClassiﬁcation\n",
      "id\n",
      "54.38\n",
      "65.84\n",
      "66.16\n",
      "70.73\n",
      "70.13\n",
      "MassiveScenarioClassiﬁcation\n",
      "is\n",
      "49.78\n",
      "61.94\n",
      "37.52\n",
      "44.16\n",
      "44.21\n",
      "MassiveScenarioClassiﬁcation\n",
      "it\n",
      "54.84\n",
      "64.09\n",
      "65.00\n",
      "69.73\n",
      "65.57\n",
      "MassiveScenarioClassiﬁcation\n",
      "ja\n",
      "54.12\n",
      "67.72\n",
      "66.50\n",
      "69.69\n",
      "65.76\n",
      "MassiveScenarioClassiﬁcation\n",
      "jv\n",
      "32.71\n",
      "58.29\n",
      "38.60\n",
      "44.20\n",
      "54.79\n",
      "MassiveScenarioClassiﬁcation\n",
      "ka\n",
      "26.92\n",
      "53.38\n",
      "50.66\n",
      "57.30\n",
      "32.99\n",
      "MassiveScenarioClassiﬁcation\n",
      "km\n",
      "27.23\n",
      "56.18\n",
      "46.96\n",
      "53.14\n",
      "39.34\n",
      "MassiveScenarioClassiﬁcation\n",
      "kn\n",
      "10.06\n",
      "61.74\n",
      "45.73\n",
      "56.08\n",
      "60.50\n",
      "MassiveScenarioClassiﬁcation\n",
      "ko\n",
      "52.01\n",
      "67.26\n",
      "55.66\n",
      "68.52\n",
      "55.69\n",
      "MassiveScenarioClassiﬁcation\n",
      "lv\n",
      "44.82\n",
      "61.87\n",
      "59.80\n",
      "66.28\n",
      "44.35\n",
      "MassiveScenarioClassiﬁcation\n",
      "ml\n",
      "49.10\n",
      "62.26\n",
      "47.69\n",
      "60.13\n",
      "65.53\n",
      "MassiveScenarioClassiﬁcation\n",
      "mn\n",
      "21.51\n",
      "62.60\n",
      "57.07\n",
      "60.85\n",
      "38.72\n",
      "MassiveScenarioClassiﬁcation\n",
      "ms\n",
      "53.60\n",
      "65.63\n",
      "61.71\n",
      "65.81\n",
      "64.99\n",
      "MassiveScenarioClassiﬁcation\n",
      "my\n",
      "29.72\n",
      "62.94\n",
      "59.10\n",
      "63.03\n",
      "36.84\n",
      "MassiveScenarioClassiﬁcation\n",
      "nb\n",
      "43.90\n",
      "64.29\n",
      "64.25\n",
      "70.24\n",
      "51.80\n",
      "MassiveScenarioClassiﬁcation\n",
      "nl\n",
      "53.33\n",
      "65.16\n",
      "65.52\n",
      "70.37\n",
      "56.32\n",
      "MassiveScenarioClassiﬁcation\n",
      "pl\n",
      "52.92\n",
      "64.56\n",
      "65.04\n",
      "68.99\n",
      "49.98\n",
      "MassiveScenarioClassiﬁcation\n",
      "pt\n",
      "53.41\n",
      "63.28\n",
      "65.79\n",
      "70.09\n",
      "71.46\n",
      "MassiveScenarioClassiﬁcation\n",
      "ro\n",
      "50.48\n",
      "62.41\n",
      "64.17\n",
      "67.95\n",
      "53.69\n",
      "MassiveScenarioClassiﬁcation\n",
      "ru\n",
      "51.84\n",
      "65.25\n",
      "65.24\n",
      "69.92\n",
      "61.60\n",
      "MassiveScenarioClassiﬁcation\n",
      "sl\n",
      "51.29\n",
      "64.25\n",
      "64.01\n",
      "70.81\n",
      "48.04\n",
      "MassiveScenarioClassiﬁcation\n",
      "sq\n",
      "55.65\n",
      "64.54\n",
      "64.31\n",
      "69.63\n",
      "50.06\n",
      "MassiveScenarioClassiﬁcation\n",
      "sv\n",
      "54.64\n",
      "66.01\n",
      "67.14\n",
      "71.60\n",
      "51.73\n",
      "MassiveScenarioClassiﬁcation\n",
      "sw\n",
      "42.04\n",
      "58.36\n",
      "34.86\n",
      "37.29\n",
      "54.22\n",
      "MassiveScenarioClassiﬁcation\n",
      "ta\n",
      "36.72\n",
      "59.08\n",
      "42.62\n",
      "55.96\n",
      "62.77\n",
      "MassiveScenarioClassiﬁcation\n",
      "te\n",
      "42.08\n",
      "64.13\n",
      "46.46\n",
      "58.81\n",
      "62.59\n",
      "MassiveScenarioClassiﬁcation\n",
      "th\n",
      "52.15\n",
      "64.34\n",
      "67.01\n",
      "69.44\n",
      "45.18\n",
      "MassiveScenarioClassiﬁcation\n",
      "tl\n",
      "37.34\n",
      "60.23\n",
      "37.37\n",
      "43.99\n",
      "52.06\n",
      "MassiveScenarioClassiﬁcation\n",
      "tr\n",
      "52.56\n",
      "65.43\n",
      "66.55\n",
      "70.4\n",
      "47.21\n",
      "MassiveScenarioClassiﬁcation\n",
      "ur\n",
      "32.60\n",
      "61.52\n",
      "60.43\n",
      "62.9\n",
      "64.26\n",
      "MassiveScenarioClassiﬁcation\n",
      "vi\n",
      "50.97\n",
      "61.05\n",
      "60.72\n",
      "65.71\n",
      "70.61\n",
      "MassiveScenarioClassiﬁcation\n",
      "zh-CN\n",
      "50.22\n",
      "70.85\n",
      "67.44\n",
      "71.23\n",
      "73.95\n",
      "MassiveScenarioClassiﬁcation\n",
      "zh-TW\n",
      "42.32\n",
      "67.08\n",
      "65.70\n",
      "68.73\n",
      "70.30\n",
      "MTOPDomainClassiﬁcation\n",
      "de\n",
      "74.08\n",
      "86.95\n",
      "79.20\n",
      "85.73\n",
      "82.05\n",
      "MTOPDomainClassiﬁcation\n",
      "es\n",
      "73.47\n",
      "84.07\n",
      "83.04\n",
      "86.96\n",
      "93.55\n",
      "MTOPDomainClassiﬁcation\n",
      "fr\n",
      "72.26\n",
      "84.14\n",
      "78.63\n",
      "81.21\n",
      "90.98\n",
      "MTOPDomainClassiﬁcation\n",
      "hi\n",
      "72.95\n",
      "85.11\n",
      "81.36\n",
      "84.76\n",
      "89.33\n",
      "MTOPDomainClassiﬁcation\n",
      "th\n",
      "72.68\n",
      "81.24\n",
      "79.99\n",
      "82.51\n",
      "60.49\n",
      "MTOPIntentClassiﬁcation\n",
      "de\n",
      "51.62\n",
      "63.42\n",
      "54.23\n",
      "61.27\n",
      "61.92\n",
      "MTOPIntentClassiﬁcation\n",
      "es\n",
      "52.75\n",
      "64.44\n",
      "60.28\n",
      "66.59\n",
      "74.49\n",
      "MTOPIntentClassiﬁcation\n",
      "fr\n",
      "50.12\n",
      "62.01\n",
      "54.05\n",
      "59.76\n",
      "69.12\n",
      "MTOPIntentClassiﬁcation\n",
      "hi\n",
      "45.55\n",
      "62.58\n",
      "59.90\n",
      "62.37\n",
      "64.85\n",
      "MTOPIntentClassiﬁcation\n",
      "th\n",
      "50.07\n",
      "64.61\n",
      "61.96\n",
      "64.80\n",
      "49.36\n",
      "Average\n",
      "mix\n",
      "42.85\n",
      "60.77\n",
      "54.87\n",
      "60.39\n",
      "54.4\n",
      "Table 13: Multilingual classiﬁcation results. Scores are accuracy.\n",
      "Dataset\n",
      "Language\n",
      "Komninos\n",
      "LASER2\n",
      "LaBSE\n",
      "MiniLM-L12-multilingual\n",
      "MPNet-multilingual\n",
      "SGPT-BLOOM-7.1B-msmarco\n",
      "STS17\n",
      "ko-ko\n",
      "2.54\n",
      "70.52\n",
      "71.32\n",
      "77.03\n",
      "83.41\n",
      "66.89\n",
      "STS17\n",
      "ar-ar\n",
      "13.78\n",
      "67.47\n",
      "69.07\n",
      "79.16\n",
      "79.10\n",
      "76.42\n",
      "STS17\n",
      "en-ar\n",
      "9.08\n",
      "65.05\n",
      "74.51\n",
      "81.22\n",
      "80.85\n",
      "78.07\n",
      "STS17\n",
      "en-de\n",
      "-3.11\n",
      "66.66\n",
      "73.85\n",
      "84.22\n",
      "83.28\n",
      "59.10\n",
      "STS17\n",
      "en-tr\n",
      "-0.45\n",
      "70.05\n",
      "72.07\n",
      "76.74\n",
      "74.90\n",
      "11.80\n",
      "STS17\n",
      "es-en\n",
      "-8.18\n",
      "55.30\n",
      "65.71\n",
      "84.44\n",
      "86.11\n",
      "78.22\n",
      "STS17\n",
      "es-es\n",
      "48.23\n",
      "79.67\n",
      "80.83\n",
      "85.56\n",
      "85.14\n",
      "86.00\n",
      "STS17\n",
      "fr-en\n",
      "5.81\n",
      "70.82\n",
      "76.98\n",
      "76.59\n",
      "81.17\n",
      "80.46\n",
      "STS17\n",
      "it-en\n",
      "3.64\n",
      "70.98\n",
      "76.99\n",
      "82.35\n",
      "84.24\n",
      "51.58\n",
      "STS17\n",
      "nl-en\n",
      "-0.44\n",
      "68.12\n",
      "75.22\n",
      "81.71\n",
      "82.51\n",
      "45.85\n",
      "STS22\n",
      "de\n",
      "33.04\n",
      "25.69\n",
      "48.58\n",
      "44.64\n",
      "46.70\n",
      "30.05\n",
      "STS22\n",
      "es\n",
      "48.53\n",
      "54.92\n",
      "63.18\n",
      "56.56\n",
      "59.91\n",
      "65.41\n",
      "STS22\n",
      "pl\n",
      "12.47\n",
      "18.34\n",
      "39.30\n",
      "33.74\n",
      "33.65\n",
      "31.13\n",
      "STS22\n",
      "tr\n",
      "47.38\n",
      "36.97\n",
      "58.15\n",
      "53.39\n",
      "56.30\n",
      "47.14\n",
      "STS22\n",
      "ar\n",
      "32.42\n",
      "42.57\n",
      "57.67\n",
      "46.2\n",
      "52.19\n",
      "58.67\n",
      "STS22\n",
      "ru\n",
      "19.44\n",
      "39.24\n",
      "57.49\n",
      "57.08\n",
      "58.74\n",
      "43.36\n",
      "STS22\n",
      "zh\n",
      "4.78\n",
      "49.41\n",
      "63.02\n",
      "58.75\n",
      "61.75\n",
      "66.78\n",
      "STS22\n",
      "fr\n",
      "49.43\n",
      "58.61\n",
      "77.95\n",
      "70.55\n",
      "74.30\n",
      "80.38\n",
      "STS22\n",
      "de-en\n",
      "28.65\n",
      "32.35\n",
      "50.14\n",
      "52.65\n",
      "50.81\n",
      "51.16\n",
      "STS22\n",
      "es-en\n",
      "26.97\n",
      "54.34\n",
      "71.86\n",
      "67.33\n",
      "70.26\n",
      "75.06\n",
      "STS22\n",
      "it\n",
      "57.77\n",
      "60.31\n",
      "72.22\n",
      "55.22\n",
      "60.65\n",
      "65.65\n",
      "STS22\n",
      "pl-en\n",
      "45.55\n",
      "53.63\n",
      "69.41\n",
      "69.02\n",
      "73.07\n",
      "53.31\n",
      "STS22\n",
      "zh-en\n",
      "14.05\n",
      "46.19\n",
      "64.02\n",
      "65.71\n",
      "67.96\n",
      "68.45\n",
      "STS22\n",
      "es-it\n",
      "41.10\n",
      "42.21\n",
      "69.69\n",
      "47.67\n",
      "53.70\n",
      "65.50\n",
      "STS22\n",
      "de-fr\n",
      "14.77\n",
      "37.41\n",
      "53.28\n",
      "51.73\n",
      "62.34\n",
      "53.28\n",
      "STS22\n",
      "de-pl\n",
      "11.21\n",
      "15.67\n",
      "58.69\n",
      "44.22\n",
      "40.53\n",
      "43.05\n",
      "STS22\n",
      "fr-pl\n",
      "39.44\n",
      "39.44\n",
      "61.98\n",
      "50.71\n",
      "84.52\n",
      "28.17\n",
      "Average\n",
      "mix\n",
      "22.14\n",
      "51.55\n",
      "65.67\n",
      "64.23\n",
      "67.71\n",
      "57.81\n",
      "Table 14: Multilingual STS Results. Scores are Spearman correlations of cosine similarities.\n",
      "\n",
      "######## MarginMSELoss.pdf ######## \n",
      "\n",
      "Improving Efficient Neural Ranking Models with\n",
      "Cross-Architecture Knowledge Distillation\n",
      "Sebastian Hofstätter, Sophia Althammer, Michael Schröder, Mete Sertkan and Allan Hanbury\n",
      "first.last@tuwien.ac.at\n",
      "TU Wien, Vienna, Austria\n",
      "ABSTRACT\n",
      "Retrieval and ranking models are the backbone of many applications\n",
      "such as web search, open domain QA, or text-based recommender\n",
      "systems. The latency of neural ranking models at query time is\n",
      "largely dependent on the architecture and deliberate choices by\n",
      "their designers to trade-off effectiveness for higher efficiency. This\n",
      "focus on low query latency of a rising number of efficient ranking\n",
      "architectures make them feasible for production deployment. In\n",
      "machine learning an increasingly common approach to close the\n",
      "effectiveness gap of more efficient models is to apply knowledge\n",
      "distillation from a large teacher model to a smaller student model.\n",
      "We find that different ranking architectures tend to produce output\n",
      "scores in different magnitudes. Based on this finding, we propose a\n",
      "cross-architecture training procedure with a margin focused loss\n",
      "(Margin-MSE), that adapts knowledge distillation to the varying\n",
      "score output distributions of different BERT and non-BERT pas-\n",
      "sage ranking architectures. We apply the teachable information\n",
      "as additional fine-grained labels to existing training triples of the\n",
      "MSMARCO-Passage collection. We evaluate our procedure of dis-\n",
      "tilling knowledge from state-of-the-art concatenated BERT models\n",
      "to four different efficient architectures (TK, ColBERT, PreTT, and a\n",
      "BERT CLS dot product model). We show that across our evaluated\n",
      "architectures our Margin-MSE knowledge distillation significantly\n",
      "improves re-ranking effectiveness without compromising their effi-\n",
      "ciency. Additionally, we show our general distillation method to\n",
      "improve nearest neighbor based index retrieval with the BERT dot\n",
      "product model, offering competitive results with specialized and\n",
      "much more costly training methods. To benefit the community, we\n",
      "publish the teacher-score training files in a ready-to-use package.\n",
      "1\n",
      "INTRODUCTION\n",
      "The same principles that applied to traditional IR systems to achieve\n",
      "low query latency also apply to novel neural ranking models: We\n",
      "need to transfer as much computation and data transformation to\n",
      "the indexing phase as possible to require less resources at query\n",
      "time [33, 34]. For the most effective BERT-based [11] neural ranking\n",
      "models, which we refer to as BERTCAT, this transfer is simply\n",
      "not possible, as the concatenation of query and passage require\n",
      "all Transformer layers to be evaluated at query time to receive a\n",
      "ranking score [36].\n",
      "To overcome this architecture restriction the neural-IR com-\n",
      "munity proposed new architectures by deliberately choosing to\n",
      "trade-off effectiveness for higher efficiency. Among these low query\n",
      "latency approaches are: TK [18] with shallow Transformers and\n",
      "separate query and document contextualization; ColBERT [21]\n",
      "with late-interactions of BERT term representations; PreTT [29]\n",
      "with a combination of query-independent and query-dependent\n",
      "Transformer layers; and a BERT-CLS dot product scoring model\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "Training Batch Count (Thousands)\n",
      "−50\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "Output Score\n",
      "Pos. Average\n",
      "Neg. Average\n",
      "ColBERT\n",
      "BERTDOT\n",
      "BERTCAT\n",
      "TK\n",
      "Figure 1: Raw query-passage pair scores during training of\n",
      "different ranking models. The margin between the positive\n",
      "and negative samples is shaded.\n",
      "which we refer to as BERTDOT, also known in the literature as\n",
      "Tower-BERT [4], BERT-Siamese [44], or TwinBERT [27].1 Each\n",
      "approach has unique characteristics that make them suitable for\n",
      "production-level query latency which we discuss in Section 2.\n",
      "An increasingly common way to improve smaller or more effi-\n",
      "cient models is to train them, as students, to imitate the behavior\n",
      "of larger or ensemble teacher models via Knowledge Distillation\n",
      "(KD) [15]. This is typically applied to the same architecture with\n",
      "fewer layers and dimensions [20, 38] via the output or layer-wise\n",
      "activations [39]. KD has been applied in the ranking task for the\n",
      "same architecture with fewer layers [5, 14, 25] and in constrained\n",
      "sub-tasks, such as keyword-list matching [27].\n",
      "In this work we propose a model-agnostic training procedure\n",
      "using cross-architecture knowledge distillation from BERTCAT with\n",
      "the goal to improve the effectiveness of efficient passage ranking\n",
      "models without compromising their query latency benefits.\n",
      "A unique challenge for knowledge distillation in the ranking\n",
      "task is the possible range of scores, i.e. a ranking model outputs a\n",
      "single unbounded decimal value and the final result solely depends\n",
      "on the relative ordering of the scores for the candidate documents\n",
      "per query. We make the crucial observation, depicted in Figure 1,\n",
      "that different architectures during their training gravitate towards\n",
      "unique range patterns in their output scores. The BERTCAT model\n",
      "exhibits positive relevant-document scores, whereas on average\n",
      "the non-relevant documents are below zero. The TK model solely\n",
      "1Yes, we see the irony: https://xkcd.com/927/\n",
      "arXiv:2010.02666v2  [cs.IR]  22 Jan 2021\n",
      "produces negative averages, and the BERTDOT and ColBERT mod-\n",
      "els, due to their dot product scoring, show high output scores. This\n",
      "leads us to our main research question:\n",
      "RQ1 How can we apply knowledge distillation in retrieval across\n",
      "architecture types?\n",
      "To optimally support the training of cross-architecture knowl-\n",
      "edge distillation, we allow our models to converge to a free scoring\n",
      "range, as long as the margin is alike with the teacher. We make\n",
      "use of the common triple (q, relevant doc, non-relevant doc) training\n",
      "regime, by distilling knowledge via the margin of the two scoring\n",
      "pairs. We train the students to learn the same margin as their teach-\n",
      "ers, which leaves the models to find the most comfortable or natural\n",
      "range for their architecture. We optimize the student margin to the\n",
      "teacher margin with a Mean Squared Error loss (Margin-MSE). We\n",
      "confirm our strategy with an ablation study of different knowledge\n",
      "distillation losses and show the Margin-MSE loss to be the most\n",
      "effective.\n",
      "Thanks to the rapid advancements and openness of the Natural\n",
      "Language Processing community, we have a number of pre-trained\n",
      "BERT-style language models to choose from to create different vari-\n",
      "ants of the BERTCAT architecture to study, allowing us to answer:\n",
      "RQ2 How effective is the distillation with a single teacher model\n",
      "in comparison to an ensemble of teachers?\n",
      "We train three different BERTCAT versions as teacher models\n",
      "with different initializations: BERT-Base [11], BERT-Large with\n",
      "whole word masking [11], and ALBERT-large [24]. To understand\n",
      "the behavior that the different language models bring to the BERTCAT\n",
      "architecture, we compare their training score margin distributions\n",
      "and find that the models offer variability suited for an ensemble.\n",
      "We created the teacher ensemble by averaging each of the three\n",
      "scores per query-document pair. We conduct the knowledge distil-\n",
      "lation with a single teacher and a teacher ensemble. The knowledge\n",
      "distillation has a general positive effect on all retrieval effectiveness\n",
      "metrics of our student models. In most cases the teacher ensemble\n",
      "further improves the student models’ effectiveness in the re-ranking\n",
      "scenario above the already improved single teacher training.\n",
      "The dual-encoder BERTDOT model can be used for full collec-\n",
      "tion indexing and retrieval with a nearest neighbor vector search\n",
      "approach, so we study:\n",
      "RQ3 How effective is our distillation for dense nearest neighbor\n",
      "retrieval?\n",
      "We observe similar trends in terms of effectiveness per teacher\n",
      "strategy, with increased effectiveness of BERTDOT models for a\n",
      "single teacher and again a higher increase for the ensemble of\n",
      "teachers. Even though we do not add dense retrieval specific train-\n",
      "ing methods, such as index-based passage sampling [44] or in-batch\n",
      "negatives [26] we observe very competitive results compared to\n",
      "those much more costly training approaches.\n",
      "To put the improved models in the perspective of the efficiency-\n",
      "effectiveness trade-off, we investigated the following question:\n",
      "RQ4 By how much does effective knowledge distillation shift the\n",
      "balance in the efficiency-effectiveness trade-off?\n",
      "We show how the knowledge distilled efficient architectures\n",
      "outperform the BERTCAT baselines on several metrics. There is\n",
      "no longer a compromise in utilizing PreTT or ColBERT and the\n",
      "effectiveness gap, i.e. the difference between the most effective and\n",
      "the other models, of BERTDOT and TK is significantly smaller.\n",
      "The contributions of this work are as follows:\n",
      "• We propose a cross-architecture knowledge distillation pro-\n",
      "cedure with a Margin-MSE loss for a range of neural retrieval\n",
      "architectures\n",
      "• We conduct a comprehensive study of the effects of cross-\n",
      "architecture knowledge distillation in the ranking scenario\n",
      "• We publish our source code as well as ready-to-use teacher\n",
      "training files for the community at:\n",
      "https://github.com/sebastian-hofstaetter/neural-ranking-kd\n",
      "2\n",
      "RETRIEVAL MODELS\n",
      "We study the effects of knowledge distillation on a wide range of\n",
      "recently introduced Transformer- & BERT-based ranking models.\n",
      "We describe their architectures in detail below and summarize them\n",
      "in Table 1.\n",
      "2.1\n",
      "BERTCAT Concatenated Scoring\n",
      "The common way of utilizing the BERT pre-trained Transformer\n",
      "model in a re-ranking scenario [31, 36, 47] is by concatenating query\n",
      "and passage input sequences. We refer to this base architecture\n",
      "as BERTCAT. In the BERTCAT ranking model, the query 𝑞1:𝑚 and\n",
      "passage 𝑝1:𝑛 sequences are concatenated with special tokens (using\n",
      "the ; operator) and the CLS token representation computed by BERT\n",
      "(selected with 1) is scored with single linear layer 𝑊𝑠:\n",
      "BERTCAT(𝑞1:𝑚, 𝑝1:𝑛) = BERT([CLS;𝑞1:𝑚; SEP;𝑝1:𝑛])1 ∗𝑊𝑠\n",
      "(1)\n",
      "We utilize BERTCAT as our teacher architecture, as it represents\n",
      "the current state-of-the art in terms of effectiveness, however it\n",
      "requires substantial compute at query time and increases the query\n",
      "latency by seconds [16, 44]. Simply using smaller BERT variants\n",
      "does not change the design flaw of having to compute every repre-\n",
      "sentation at query time.\n",
      "2.2\n",
      "BERTDOT Dot Product Scoring\n",
      "In contrast to BERTCAT, which requires a full online computation,\n",
      "the BERTDOT model only matches a single CLS vector of the query\n",
      "with a single CLS vector of a passage [27, 28, 44]. The BERTDOT\n",
      "model uses two independent BERT computations as follows:\n",
      "ˆ𝑞 = BERT([CLS;𝑞1:𝑚])1 ∗𝑊𝑠\n",
      "ˆ𝑝 = BERT([CLS;𝑝1:𝑛])1 ∗𝑊𝑠\n",
      "(2)\n",
      "which allows us to pre-compute every contextualized passage rep-\n",
      "resentation ˆ𝑝. After this, the model computes the final scores as the\n",
      "dot product · of ˆ𝑞 and ˆ𝑝:\n",
      "BERTDOT(𝑞1:𝑚, 𝑝1:𝑛) = ˆ𝑞 · ˆ𝑝\n",
      "(3)\n",
      "BERTDOT, with its bottleneck of comparing single vectors, com-\n",
      "presses information much more strongly than BERTCAT, which\n",
      "brings large query time improvements at the cost of lower effec-\n",
      "tiveness, as can be seen in Table 1.\n",
      "2.3\n",
      "ColBERT\n",
      "The ColBERT model [21] is similar in nature to BERTDOT, by de-\n",
      "laying the interactions between query and document to after the\n",
      "Table 1: Comparison of model characteristics using DistilBERT instances. Effectiveness compares the baseline nDCG@10 of\n",
      "MSMARCO-DEV. NN Index refers to indexing the passage representations in a nearest neighbor index. |𝑃| refers to the number\n",
      "of passages; |𝑇 | to the total number of term occurrences in the collection; 𝑚 the query length; and 𝑛 the document length.\n",
      "Model\n",
      "Effectiveness\n",
      "Query\n",
      "GPU\n",
      "Query-Passage Interaction\n",
      "Passage\n",
      "NN\n",
      "Storage Req.\n",
      "Latency\n",
      "Memory\n",
      "Cache\n",
      "Index\n",
      "(× Vector Size)\n",
      "BERTCAT\n",
      "1\n",
      "950 ms\n",
      "10.4 GB\n",
      "All TF layers\n",
      "–\n",
      "–\n",
      "–\n",
      "BERTDOT\n",
      "× 0.87\n",
      "23 ms\n",
      "3.6 GB\n",
      "Single dot product\n",
      "✓\n",
      "✓\n",
      "|𝑃|\n",
      "ColBERT\n",
      "× 0.97\n",
      "28 ms\n",
      "3.4 GB\n",
      "𝑚 ∗ 𝑛 dot products\n",
      "✓\n",
      "✓\n",
      "|𝑇 |\n",
      "PreTT\n",
      "× 0.97\n",
      "455 ms\n",
      "10.9 GB\n",
      "Min. 1 TF layer (here 3)\n",
      "✓\n",
      "–\n",
      "|𝑇 |\n",
      "TK\n",
      "× 0.89\n",
      "14 ms\n",
      "1.8 GB\n",
      "𝑚 ∗ 𝑛 dot products + Kernel-pooling\n",
      "✓\n",
      "–\n",
      "|𝑇 |\n",
      "BERT computation. ColBERT uses every query and document rep-\n",
      "resentation:\n",
      "ˆ𝑞1:𝑚 = BERT([CLS;𝑞1:𝑚; rep(MASK)]) ∗𝑊𝑠\n",
      "ˆ𝑝1:𝑛 = BERT([CLS;𝑝1:𝑛]) ∗𝑊𝑠\n",
      "(4)\n",
      "where the rep(𝑀𝐴𝑆𝐾) method repeats the MASK token a num-\n",
      "ber of times, set by a hyperparameter. Khattab and Zaharia [21]\n",
      "introduced this query augmentation method to increase the com-\n",
      "putational capacity of the BERT model for short queries. We inde-\n",
      "pendently confirmed that adding these MASK tokens improves the\n",
      "effectiveness of ColBERT. The interactions in the ColBERT model\n",
      "are aggregated with a max-pooling per query term and sum of\n",
      "query-term scores as follows:\n",
      "ColBERT(𝑞1:𝑚, 𝑝1:𝑛) =\n",
      "𝑚\n",
      "∑︁\n",
      "1\n",
      "max\n",
      "1..𝑛 ˆ𝑞𝑇\n",
      "1:𝑚 · ˆ𝑝1:𝑛\n",
      "(5)\n",
      "The aggregation only requires 𝑛 ∗ 𝑚 dot product computations,\n",
      "making it roughly as efficient as BERTDOT, however the storage\n",
      "cost of pre-computing passage representations is much higher and\n",
      "depends on the total number of terms in the collection. Khattab and\n",
      "Zaharia [21] proposed to compress the dimensions of the represen-\n",
      "tation vectors by reducing the output features of 𝑊𝑠. We omitted\n",
      "this compression, as storage space is not the focus of our study and\n",
      "to better compare results across different models.\n",
      "2.4\n",
      "PreTT\n",
      "The PreTT architecture [29] is conceptually between BERTCAT and\n",
      "ColBERT, as it allows to compute 𝑏 BERT-layers separately for\n",
      "query and passage:\n",
      "ˆ𝑞1:𝑚 = BERT\n",
      "1:𝑏 ([CLS;𝑞1:𝑚])\n",
      "ˆ𝑝1:𝑛 = BERT\n",
      "1:𝑏 ([CLS;𝑝1:𝑛])\n",
      "(6)\n",
      "Then PreTT concatenates the sequences with a SEP separator\n",
      "token and computes the remaining layers to compute a total of ˆ𝑏\n",
      "BERT-layers. Finally, the CLS token output is pooled with single\n",
      "linear layer 𝑊𝑠:\n",
      "PreTT(𝑞1:𝑚, 𝑝1:𝑛) = BERT\n",
      "𝑏: ˆ𝑏\n",
      "([ ˆ𝑞1:𝑚; SEP; ˆ𝑝1:𝑛])1 ∗𝑊𝑠\n",
      "(7)\n",
      "Concurrently to PreTT, DC-BERT [48] and EARL [13] have been\n",
      "proposed with very similar approaches to split Transformer layers.\n",
      "We selected PreTT simply as a representative of this group of mod-\n",
      "els. Similar to ColBERT, we omitted the optional compression of\n",
      "representations for better comparability.\n",
      "2.5\n",
      "Transformer-Kernel\n",
      "The Transformer-Kernel (TK) model [18] is not based on BERT pre-\n",
      "training, but rather uses shallow Transformers. TK independently\n",
      "contextualizes query 𝑞1:𝑚 and passage 𝑝1:𝑛 based on pre-trained\n",
      "word embeddings, where the intensity of the contextualization\n",
      "(Transformers as TF) is set by a gate 𝛼:\n",
      "ˆ𝑞𝑖 = 𝑞𝑖 ∗ 𝛼 + TF(𝑞1:𝑚)𝑖 ∗ (1 − 𝛼)\n",
      "ˆ𝑝𝑖 = 𝑝𝑖 ∗ 𝛼 + TF(𝑝1:𝑛)𝑖 ∗ (1 − 𝛼)\n",
      "(8)\n",
      "The sequences ˆ𝑞1:𝑚 and ˆ𝑝1:𝑛 interact in a match-matrix with a\n",
      "cosine similarity per term pair and each similarity is activated by a\n",
      "set of Gaussian kernels [43]:\n",
      "𝐾𝑘\n",
      "𝑖,𝑗 = exp\n",
      " \n",
      "−\n",
      "BERTCAT\n",
      "❶ Teacher Training \n",
      "Ranknet\n",
      "BERTCAT\n",
      "Result\n",
      "Store\n",
      "Passage -\n",
      "Passage -\n",
      "Passage +\n",
      "Passage +\n",
      "Query\n",
      "Query\n",
      "<q, p+> <q, p->\n",
      "BERTCAT\n",
      "BERTCAT\n",
      "❷ Teacher Inference \n",
      "Student\n",
      "❸ Student Training \n",
      "Margin-MSE\n",
      "Student\n",
      "Figure 2: Our knowledge distillation process, re-visiting the same training triples in all steps: ➊ Training the BERTCAT model;\n",
      "➋ Using the trained BERTCAT to create scores for all training triples; ➌ Individually training the student models with Margin-\n",
      "MSE using the teacher scores.\n",
      "• BERTDOT is the most efficient BERT-based model with re-\n",
      "gards to storage and query latency, at the cost of lower ef-\n",
      "fectiveness compared to ColBERT and PreTT.\n",
      "• PreTT highly depends on the choice of the concatenation-\n",
      "layer hyperparameter, which we set to 3 to be between\n",
      "BERTCAT and ColBERT.\n",
      "• ColBERT is especially suited for small collections, as it re-\n",
      "quires a large passage cache.\n",
      "• TK is less effective overall, however it is much cheaper to\n",
      "run than the other models.\n",
      "The most suitable neural ranking model ultimately depends on\n",
      "the exact scenario. To allow people to make the choice, we evaluated\n",
      "all presented models. we use BERTCAT as our teacher architecture\n",
      "and the other presented architectures as students.\n",
      "3\n",
      "CROSS-ARCHITECTURE\n",
      "KNOWLEDGE DISTILLATION\n",
      "The established approach to training deep neural ranking models is\n",
      "mainly based on large-scale annotated data. Here, the MSMARCO\n",
      "collection is becoming the de-facto standard. The MSMARCO col-\n",
      "lection only contains binary annotations for fewer than two positive\n",
      "examples per query, and no explicit annotations for non-relevant\n",
      "passages. The approach proposed by Bajaj et al. [1] is to utilize ran-\n",
      "domly selected passages retrieved from the top 1000 candidates of\n",
      "a traditional retrieval system as negative examples. This approach\n",
      "works reasonably well, but accidentally picking relevant passages\n",
      "is possible.\n",
      "Neural retrieval models are commonly trained on triples of bi-\n",
      "nary relevance assignments of one relevant and one non-relevant\n",
      "passage. However, they are used in a setting that requires a much\n",
      "more nuanced view of relevance when they re-rank a thousand\n",
      "possibly relevant passages. The BERTCAT architecture shows the\n",
      "strongest generalization capabilities, which other architectures do\n",
      "not posses.\n",
      "Following our observation of distinct scoring ranges of different\n",
      "model architectures in Figure 1, we propose to utilize a knowledge\n",
      "distillation loss by only optimizing the margin between the scores of\n",
      "the relevant and the non-relevant sample passage per query. We call\n",
      "our proposed approach Margin Mean Squared Error (Margin-MSE).\n",
      "We train ranking models on batches containing triples of queries\n",
      "𝑄, relevant passages 𝑃+, and non-relevant passages 𝑃−. We utilize\n",
      "the output margin of the teacher model 𝑀𝑡 as label to optimize the\n",
      "weights of the student model 𝑀𝑠:\n",
      "L(𝑄, 𝑃+, 𝑃−) = MSE(𝑀𝑠 (𝑄, 𝑃+) − 𝑀𝑠 (𝑄, 𝑃−),\n",
      "𝑀𝑡 (𝑄, 𝑃+) − 𝑀𝑡 (𝑄, 𝑃−))\n",
      "(11)\n",
      "MSE is the Mean Squared Error loss function, calculating the\n",
      "mean of the squared differences between the scores𝑆 and the targets\n",
      "𝑇 over the batch size:\n",
      "MSE(𝑆,𝑇) = 1\n",
      "|𝑆|\n",
      "∑︁\n",
      "𝑠 ∈𝑆,𝑡 ∈𝑇\n",
      "(𝑠 − 𝑡)2\n",
      "(12)\n",
      "The Margin-MSE loss discards the original binary relevance\n",
      "information, in contrast to other knowledge distillation approaches\n",
      "[25], as the margin of the teacher can potentially be negative, which\n",
      "would indicate a reverse ordering from the original training data.\n",
      "We observe that the teacher models have a very high pairwise\n",
      "ranking accuracy during training of over 98%, therefore we view it\n",
      "as redundant to add the binary information in the ranking loss.2\n",
      "In Figure 2 we show the staged process of our knowledge distilla-\n",
      "tion. For simplicity and ease of re-use, we utilize the same training\n",
      "triples for every step. The process begins with training a BERTCAT\n",
      "teacher model on the collection labels with a RankNet loss [3]. After\n",
      "the teacher training is finished, we use the teacher model again to\n",
      "infer all scores for the training data, without updating its weights.\n",
      "This allows us to store the teacher scores once, for an efficient ex-\n",
      "perimentation and sharing workflow. Finally, we train our student\n",
      "model of a different architecture, by using the teacher scores as\n",
      "labels with our proposed Margin-MSE loss.\n",
      "2We do not analyze this statistic further in this paper, as we did not see a correlation\n",
      "or interesting difference between models on this pairwise training accuracy metric.\n",
      "4\n",
      "EXPERIMENT DESIGN\n",
      "For our neural re-ranking training and inference we use PyTorch [37]\n",
      "and the HuggingFace Transformer library [42]. For the first stage\n",
      "indexing and retrieval we use Anserini [46].\n",
      "4.1\n",
      "Collection & Query Sets\n",
      "We use the MSMARCO-Passage [1] collection with sparsely-judged\n",
      "MSMARCO-DEV query set of 49,000 queries as well as the densely-\n",
      "judged query set of 43 queries derived from TREC-DL’19 [7]. For\n",
      "TREC graded relevance labels we use a binarization point of 2\n",
      "for MRR and MAP. MSMARCO is based on sampled Bing queries\n",
      "and contains 8.8 million passages with a proposed training set of\n",
      "40 million triples sampled. We evaluate our teachers on the full\n",
      "training set, so to not limit future work in terms of the number\n",
      "of triples available. We cap the query length at 30 tokens and the\n",
      "passage length at 200 tokens.\n",
      "4.2\n",
      "Training Configuration\n",
      "We use the Adam [22] optimizer with a learning rate of 7 ∗ 10−6 for\n",
      "all BERT layers, regardless of the number of layers trained. TK is\n",
      "the only model trained on a higher rate of 10−5. We employ early\n",
      "stopping, based on the best nDCG@10 value of the validation set.\n",
      "We use a training batch size of 32.\n",
      "4.3\n",
      "Model Parameters\n",
      "All student language models use a 6-layer DistilBERT [38] as their\n",
      "initialization standpoint. We chose DistilBERT over BERT-Base,\n",
      "as it has been shown to provide a close lower bound on the re-\n",
      "sults at half the runtime [29, 38]. For our ColBERT implementation\n",
      "we repeat the query MASK augmentation 8 times, regardless of\n",
      "the amount of padding in a batch in contrast to Khattab and Za-\n",
      "haria [21]. For PreTT we decided to concatenate sequences after\n",
      "3 layers of the 6 layer DistilBERT, as we want to evaluate it as a\n",
      "mid-choice between ColBERT and BERTCAT. For TK we use the\n",
      "standard 2 layer configuration with 300 dimensional embeddings.\n",
      "For the traditional BM25 we use the tuned parameters from the\n",
      "Anserini documentation.\n",
      "5\n",
      "RESULTS\n",
      "We now discuss our research questions, starting with the study of\n",
      "our proposed Margin-MSE loss function; followed by an analysis of\n",
      "different teacher model results and their impact on the knowledge\n",
      "distillation; and finally examining what the knowledge distillation\n",
      "improvement means for the efficiency-effectiveness trade-off.\n",
      "5.1\n",
      "Optimization Study\n",
      "We validate our approach presented in Section 3 and our research\n",
      "question RQ1 How can we apply knowledge distillation in retrieval\n",
      "across architecture types? by comparing Margin-MSE with different\n",
      "knowledge distillation losses using the same training data. We com-\n",
      "pare our approach with a pointwise MSE loss, defined as follows:\n",
      "L(𝑄, 𝑃+, 𝑃−) = MSE(𝑀𝑠 (𝑄, 𝑃+), 𝑀𝑡 (𝑄, 𝑃+)) +\n",
      "MSE(𝑀𝑠 (𝑄, 𝑃−), 𝑀𝑡 (𝑄, 𝑃−))\n",
      "(13)\n",
      "Table 2: Loss function ablation results on MSMARCO-DEV,\n",
      "using a single teacher (T1 in Table 3). The original training\n",
      "baseline is indicated by –.\n",
      "Model\n",
      "KD Loss\n",
      "nDCG@10 MRR@10 MAP@100\n",
      "ColBERT\n",
      "–\n",
      ".417\n",
      ".357\n",
      ".361\n",
      "Weighted RankNet\n",
      ".417\n",
      ".356\n",
      ".360\n",
      "Pointwise MSE\n",
      ".428\n",
      ".365\n",
      ".369\n",
      "Margin-MSE\n",
      ".431\n",
      ".370\n",
      ".374\n",
      "BERTDOT\n",
      "–\n",
      ".373\n",
      ".316\n",
      ".321\n",
      "Weighted RankNet\n",
      ".384\n",
      ".326\n",
      ".332\n",
      "Pointwise MSE\n",
      ".387\n",
      ".328\n",
      ".332\n",
      "Margin-MSE\n",
      ".388\n",
      ".330\n",
      ".335\n",
      "TK\n",
      "–\n",
      ".384\n",
      ".326\n",
      ".331\n",
      "Weighted RankNet\n",
      ".387\n",
      ".328\n",
      ".333\n",
      "Pointwise MSE\n",
      ".394\n",
      ".335\n",
      ".340\n",
      "Margin-MSE\n",
      ".398\n",
      ".339\n",
      ".344\n",
      "This is a standard approach already used by Vakili Tahami et al.\n",
      "[41] and Li et al. [25]. Additionally, we utilize a weighted RankNet\n",
      "loss, where we weight the samples in a batch according to the\n",
      "teacher margin:\n",
      "L(𝑄, 𝑃+, 𝑃−) = RankNet(𝑀𝑠 (𝑄, 𝑃+) − 𝑀𝑠 (𝑄, 𝑃−)) ∗\n",
      "||𝑀𝑡 (𝑄, 𝑃+) − 𝑀𝑡 (𝑄, 𝑃−)||\n",
      "(14)\n",
      "We show the results of our ablation study in Table 2 for three\n",
      "distinct ranking architectures that significantly differ from the\n",
      "BERTCAT teacher model. We use a single (BERT-Base𝐶𝐴𝑇 ) teacher\n",
      "model for this study. For each of the three architectures the Margin-\n",
      "MSE loss outperforms the pointwise MSE and weighted RankNet\n",
      "losses on all metrics. However, we also note that applying knowl-\n",
      "edge distillation in general improves each model’s result over the\n",
      "respective original baseline. Our aim in proposing to use the Margin-\n",
      "MSE loss was to create a simple yet effective solution that does not\n",
      "require changes to the model architectures or major adaptions to\n",
      "the training procedure.\n",
      "5.2\n",
      "Knowledge Distillation Results\n",
      "Utilizing our proposed Margin-MSE loss in connection with our\n",
      "trained teacher models, we follow the procedure laid out in Section 3\n",
      "to train our knowledge-distilled student models. Table 3 first shows\n",
      "our baselines, then in the second section the results of our teacher\n",
      "models, and in the third section our student architectures. Each\n",
      "student has a baseline result without teacher training (depicted by\n",
      "–) and a single teacher T1 as well as the teacher ensemble denoted\n",
      "with T2. With these results we can now answer:\n",
      "RQ2 How effective is the distillation with a single teacher model\n",
      "in comparison to an ensemble of teachers?\n",
      "We selected BERT-BaseCAT as our single teacher model, as it is\n",
      "a commonly used instance in neural ranking models. The ensemble\n",
      "of different larger BERTCAT models shows strong and consistent\n",
      "improvements on all MSMARCO DEV metrics and MAP@1000 of\n",
      "TREC-DL’19. When we compare our teacher model results with the\n",
      "best re-ranking entry [45] of TREC-DL’19, we see that our teachers,\n",
      "Table 3: Effectiveness results for both query sets of our baselines (results copied from cited models), teacher model results\n",
      "(with the teacher signs left of the model name), and using those teachers for our student models.\n",
      "Model\n",
      "Teacher\n",
      "TREC DL Passages 2019\n",
      "MSMARCO DEV\n",
      "nDCG@10\n",
      "MRR@10\n",
      "MAP@1000\n",
      "nDCG@10\n",
      "MRR@10\n",
      "MAP@1000\n",
      "Baselines\n",
      "BM25\n",
      "–\n",
      ".501\n",
      ".689\n",
      ".295\n",
      ".241\n",
      ".194\n",
      ".202\n",
      "TREC Best Re-rank [45]\n",
      "–\n",
      ".738\n",
      ".882\n",
      ".457\n",
      "–\n",
      "–\n",
      "–\n",
      "BERTCAT (6-Layer Distilled Best) [14]\n",
      "–\n",
      ".719\n",
      "–\n",
      "–\n",
      "–\n",
      ".356\n",
      "–\n",
      "BERT-BaseDOT ANCE [44]\n",
      "–\n",
      ".677\n",
      "–\n",
      "–\n",
      "–\n",
      ".330\n",
      "–\n",
      "Teacher Models\n",
      "𝑇1\n",
      "BERT-BaseCAT\n",
      "–\n",
      ".730\n",
      ".866\n",
      ".455\n",
      ".437\n",
      ".376\n",
      ".381\n",
      "BERT-Large-WMCAT\n",
      "–\n",
      ".742\n",
      ".860\n",
      ".484\n",
      ".442\n",
      ".381\n",
      ".385\n",
      "ALBERT-LargeCAT\n",
      "–\n",
      ".738\n",
      ".903\n",
      ".477\n",
      ".446\n",
      ".385\n",
      ".388\n",
      "𝑇2\n",
      "Top-3 Ensemble\n",
      "–\n",
      ".743\n",
      ".889\n",
      ".495\n",
      ".460\n",
      ".399\n",
      ".402\n",
      "Student Models\n",
      "DistilBERTCAT\n",
      "–\n",
      ".723\n",
      ".851\n",
      ".454\n",
      ".431\n",
      ".372\n",
      ".375\n",
      "T1\n",
      ".739\n",
      ".889\n",
      ".473\n",
      ".440\n",
      ".380\n",
      ".383\n",
      "T2\n",
      ".747\n",
      ".891\n",
      ".480\n",
      ".451\n",
      ".391\n",
      ".394\n",
      "PreTT\n",
      "–\n",
      ".717\n",
      ".862\n",
      ".438\n",
      ".418\n",
      ".358\n",
      ".362\n",
      "T1\n",
      ".748\n",
      ".890\n",
      ".475\n",
      ".439\n",
      ".378\n",
      ".382\n",
      "T2\n",
      ".737\n",
      ".859\n",
      ".472\n",
      ".447\n",
      ".386\n",
      ".389\n",
      "ColBERT\n",
      "–\n",
      ".722\n",
      ".874\n",
      ".445\n",
      ".417\n",
      ".357\n",
      ".361\n",
      "T1\n",
      ".738\n",
      ".862\n",
      ".472\n",
      ".431\n",
      ".370\n",
      ".374\n",
      "T2\n",
      ".744\n",
      ".878\n",
      ".478\n",
      ".436\n",
      ".375\n",
      ".379\n",
      "BERT-BaseDOT\n",
      "–\n",
      ".675\n",
      ".825\n",
      ".396\n",
      ".376\n",
      ".320\n",
      ".325\n",
      "T1\n",
      ".677\n",
      ".809\n",
      ".427\n",
      ".378\n",
      ".321\n",
      ".327\n",
      "T2\n",
      ".724\n",
      ".876\n",
      ".448\n",
      ".390\n",
      ".333\n",
      ".338\n",
      "DistilBERTDOT\n",
      "–\n",
      ".670\n",
      ".841\n",
      ".406\n",
      ".373\n",
      ".316\n",
      ".321\n",
      "T1\n",
      ".704\n",
      ".821\n",
      ".441\n",
      ".388\n",
      ".330\n",
      ".335\n",
      "T2\n",
      ".712\n",
      ".862\n",
      ".453\n",
      ".391\n",
      ".332\n",
      ".337\n",
      "TK\n",
      "–\n",
      ".652\n",
      ".751\n",
      ".403\n",
      ".384\n",
      ".326\n",
      ".331\n",
      "T1\n",
      ".669\n",
      ".813\n",
      ".414\n",
      ".398\n",
      ".339\n",
      ".344\n",
      "T2\n",
      ".666\n",
      ".797\n",
      ".415\n",
      ".399\n",
      ".341\n",
      ".345\n",
      "especially the ensemble outperform the TREC results to represent\n",
      "state-of-the-art results in terms of effectiveness.\n",
      "Overall, we observe that either a single teacher or an ensemble of\n",
      "teachers improves the model results over their respective original\n",
      "baselines. The ensemble T2 improves over T1 for all models on\n",
      "the sparse MSMARCO-DEV labels with many queries. Only on\n",
      "the TREC-DL’19 query set does T2 fail to improve over T1 for\n",
      "TK and PreTT. The only outlier in our results is BERT-BaseDOT\n",
      "trained on T1, where there is no improvement over the baseline,\n",
      "T2 however does show a substantial improvement. This leads us\n",
      "to the conclusion that utilizing an ensemble of teachers is overall\n",
      "preferred to a single teacher model.\n",
      "Furthermore, when we compare the BERT type for the BERTCAT\n",
      "architecture, we see that DistilBERTCAT-T2 outperforms any single\n",
      "teacher model with twice and four times the layers on almost all\n",
      "metrics. For the BERTDOT architecture we also compared BERT-\n",
      "Base and DistilBERT, both as students, and here BERT-Base has\n",
      "a slight advantage trained on T2. However, its T1 results are in-\n",
      "consistent, where almost no improvement is observable, whereas\n",
      "DistilBERTDOT exhibits consistent gains first for T1 and then an-\n",
      "other step for T2.\n",
      "Our T2 training improves both instances of the BERTDOT archi-\n",
      "tecture in comparison to the ANCE [44] trained BERTDOT model\n",
      "and evaluated in the re-ranking setting.\n",
      "To also compare the BERTDOT model in the full collection vector\n",
      "retrieval setting we set out to answer:\n",
      "RQ3 How effective is our distillation for dense nearest neighbor\n",
      "retrieval?\n",
      "The difference to previous results in Table 3 is that now we only\n",
      "use the score of a nearest neighbor search of all indexed passages,\n",
      "without re-ranking BM25. Because we no longer re-rank first-stage\n",
      "results, the pipeline overall becomes more efficient and less com-\n",
      "plex, however the chance of false positives becomes greater and\n",
      "Table 4: Dense retrieval results for both query sets, using a flat Faiss index without compression.\n",
      "Model\n",
      "Index\n",
      "Teacher\n",
      "TREC DL Passages 2019\n",
      "MSMARCO DEV\n",
      "Size\n",
      "nDCG@10\n",
      "MRR@10\n",
      "Recall@1K\n",
      "nDCG@10\n",
      "MRR@10\n",
      "Recall@1K\n",
      "Baselines\n",
      "BM25\n",
      "2 GB\n",
      "–\n",
      ".501\n",
      ".689\n",
      ".739\n",
      ".241\n",
      ".194\n",
      ".868\n",
      "BERT-BaseDOT ANCE [44]\n",
      "–\n",
      ".648\n",
      "–\n",
      "–\n",
      "–\n",
      ".330\n",
      ".959\n",
      "TCT-ColBERT [26]\n",
      "–\n",
      ".670\n",
      "–\n",
      ".720\n",
      "–\n",
      ".335\n",
      ".964\n",
      "RocketQA [12]\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      ".370\n",
      ".979\n",
      "Our Dense Retrieval Student Models\n",
      "BERT-BaseDOT\n",
      "12.7 GB\n",
      "–\n",
      ".593\n",
      ".757\n",
      ".664\n",
      ".347\n",
      ".294\n",
      ".913\n",
      "T1\n",
      ".631\n",
      ".771\n",
      ".702\n",
      ".358\n",
      ".304\n",
      ".931\n",
      "T2\n",
      ".668\n",
      ".826\n",
      ".737\n",
      ".371\n",
      ".315\n",
      ".947\n",
      "DistilBERTDOT\n",
      "12.7 GB\n",
      "–\n",
      ".626\n",
      ".836\n",
      ".713\n",
      ".354\n",
      ".299\n",
      ".930\n",
      "T1\n",
      ".687\n",
      ".818\n",
      ".749\n",
      ".379\n",
      ".321\n",
      ".954\n",
      "T2\n",
      ".697\n",
      ".868\n",
      ".769\n",
      ".381\n",
      ".323\n",
      ".957\n",
      "less interpretable in a dense vector space retrieval. The ColBERT ar-\n",
      "chitecture also includes the possibility to conduct a dense retrieval,\n",
      "however at the expense of increasing the storage requirements of\n",
      "2GB plain text to a 2TB index, which stopped us from conducting\n",
      "extensive experiments with ColBERT.\n",
      "We show nearest neighbor retrieval results of our BERTDOT mod-\n",
      "els (using both BERT-Base and DistilBERT encoders) and baselines\n",
      "for dense retrieval in Table 4. Training with a teacher ensemble\n",
      "is again more effective than training with a single teacher, which\n",
      "is still more effective than training the BERTDOT alone without\n",
      "teachers. Interestingly, DistilBERT outperforms BERT-Base across\n",
      "the board with half the Transformer layers. As we let the models\n",
      "train as long as they improved the early stopping set, it suggests,\n",
      "for the retrieval task we may not need more model capacity, which\n",
      "is a sure bet to improve results on the BERTCAT architecture.\n",
      "Our dense retrieval results are competitive with related meth-\n",
      "ods, even though they specifically train for the dense retrieval task.\n",
      "Our approach, while not specific to dense retrieval training is com-\n",
      "petitive with the more costly and complex approaches ANCE and\n",
      "TCT-ColBERT. On MSMARCO DEV MRR@10 we are at a slight\n",
      "disadvantage, however we outperform the models that also pub-\n",
      "lished TREC-DL’19 results. RocketQA, the current state-of-the-art\n",
      "dense retrieval result on MSMARCO DEV requires a batch size of\n",
      "4,000 and enormous computational resources, which are hardly\n",
      "comparable to our technique that only requires a batch size of 32\n",
      "and can be trained on a single GPU.\n",
      "5.3\n",
      "Closing the Efficiency-Effectiveness Gap\n",
      "We round off our results with a thorough look at the effects of\n",
      "knowledge distillation on the relation between effectiveness and\n",
      "efficiency in the re-ranking scenario. We measure the median query\n",
      "latency under the conditions that we have our cached document\n",
      "representation in memory, contextualize a single query, and com-\n",
      "puted the respective model’s interaction pattern for 1 query and\n",
      "1000 documents in a single batch on a TITAN RTX GPU with 24GB\n",
      "of memory. The large GPU memory allows us to also compute the\n",
      "same batch size for BERTCAT, which for inference requires 16GB\n",
      "of total reserved GPU memory in the BERT-Base case. We measure\n",
      "the latency of the neural model in PyTorch inference mode (without\n",
      "101\n",
      "102\n",
      "103\n",
      "Query Latency (ms)\n",
      "0.64\n",
      "0.66\n",
      "0.68\n",
      "0.70\n",
      "0.72\n",
      "0.74\n",
      "0.76\n",
      "nDCG@10\n",
      "T1\n",
      "T2\n",
      "T1\n",
      "T2\n",
      "T1\n",
      "T2\n",
      "T1\n",
      "T2\n",
      "T1\n",
      "T2\n",
      "T1\n",
      "T2\n",
      "BERT-BaseCAT\n",
      "BERT-BaseDOT\n",
      "ColBERT\n",
      "DistilBERTCAT\n",
      "DistilBERTDOT\n",
      "PreTT\n",
      "TK\n",
      "Figure 3: Query latency vs. nDCG@10 on TREC’19\n",
      "101\n",
      "102\n",
      "103\n",
      "Query Latency (ms)\n",
      "0.30\n",
      "0.32\n",
      "0.34\n",
      "0.36\n",
      "0.38\n",
      "0.40\n",
      "MRR@10\n",
      "T1\n",
      "T2\n",
      "T1\n",
      "T2\n",
      "T1\n",
      "T2\n",
      "T1\n",
      "T2\n",
      "T1\n",
      "T2\n",
      "T1\n",
      "T2\n",
      "BERT-BaseCAT\n",
      "BERT-BaseDOT\n",
      "ColBERT\n",
      "DistilBERTCAT\n",
      "DistilBERTDOT\n",
      "PreTT\n",
      "TK\n",
      "Figure 4: Query latency vs. MRR@10 on MSMARCO DEV\n",
      "accounting for pre-processing or disk access times, as those are\n",
      "highly dependent on the use of optimized inference libraries) to\n",
      "answer:\n",
      "RQ4 By how much does effective knowledge distillation shift the\n",
      "balance in the efficiency-effectiveness trade-off?\n",
      "In Figures 3 and 4, we plot the median query latency on the\n",
      "log-scaled x-axis versus the effectiveness on the y-axis. The teacher\n",
      "trained models are indicated with T1 and T2. The latency for differ-\n",
      "ent teachers does not change, as we do not change the architecture,\n",
      "only the weights of the models. The T1 teacher model BERTCAT\n",
      "is indicated with the red square. The TREC-DL’19 results in Fig-\n",
      "ure 3 show how DistilBERTCAT, PreTT, and ColBERT not only\n",
      "close the gap to BERT-BaseCAT, but improve on the single instance\n",
      "BERT-BaseCAT results. The BERTDOT and TK models, while not\n",
      "reaching the effectiveness of the other models, are also improved\n",
      "over their baselines and are more efficient in terms of total runtime\n",
      "(TK) and index space (BERTDOT). The MSMARCO DEV results in\n",
      "Figure 4 differ from Figure 3 in DistilBERTCAT and PreTT outper-\n",
      "forming BERT-BaseCAT as well as the evaluated BERTDOT variants\n",
      "under-performing overall in comparison to TK and ColBERT.\n",
      "Even though in this work we measure the inference time on\n",
      "a GPU, we believe that the most efficient models — namely TK,\n",
      "ColBERT, and BERTDOT — allow for production CPU inference, as-\n",
      "suming the document collection has been pre-computed on GPUs.\n",
      "Furthermore, in a cascading search pipeline, one can hide most of\n",
      "the remaining computation complexity of the query contextualiza-\n",
      "tion during earlier stages.\n",
      "6\n",
      "TEACHER ANALYSIS\n",
      "Finally, we analyse the distribution of our teacher score margins,\n",
      "to validate the intuition of using a teacher ensemble and we look at\n",
      "per-query nDCG changes for two models between teacher-trained\n",
      "instances and the baseline.\n",
      "6.1\n",
      "Teacher Score Distribution Analysis\n",
      "To validate the use of an ensemble of teachers for RQ2, we analyze\n",
      "the output score margin distribution of our teacher models in Figure\n",
      "5, to see if they bring diversity to the ensemble mix. This is the\n",
      "margin used in the Margin-MSE loss. We observe that the same\n",
      "BERTCAT architecture, differing only in the BERT language model\n",
      "used, shows three distinct score patterns. We view this as a good\n",
      "sign for the applicability of an ensemble of teachers, indicating\n",
      "that the different teachers have different viewpoints to offer. To\n",
      "ensemble our teacher models we computed a mean of their scores\n",
      "per example used for the knowledge distillation, to not introduce\n",
      "more complexity in the process.\n",
      "An interesting quirk of our Margin-MSE definition is the possi-\n",
      "bility to reverse orderings if the margin between a pair is negative.\n",
      "In Figure 5 we can see the reversal of the ordering of pairs in the\n",
      "distribution for the < 0 margin. It happens rarely and if a swap\n",
      "occurs the score difference is small. We investigated this issue by\n",
      "qualitatively analyzing a few dozen cases and found that the teacher\n",
      "models are most of the time correct in their determination to re-\n",
      "verse or equalize the margin. Because it only affects a few percent\n",
      "of the training data we retained those samples as well to not change\n",
      "the training data.\n",
      "-5\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "Score Margin (Relevant - Non Relevant)\n",
      "0.00\n",
      "0.02\n",
      "0.04\n",
      "0.06\n",
      "0.08\n",
      "0.10\n",
      "0.12\n",
      "0.14\n",
      "0.16\n",
      "Rel. Occurence\n",
      "BERT-BaseCAT\n",
      "ALBERT-LargeCAT\n",
      "BERT-LargeCAT\n",
      "Figure 5: Distribution of the margins between relevant and\n",
      "non-relevant documents of the three teacher models on MS\n",
      "MARCO-Passage training data\n",
      "−0.2\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "DistilBERTDOT-T1 to DistilBERTDOT\n",
      "DistilBERTDOT-T2 to DistilBERTDOT\n",
      "Sorted Queries\n",
      "−0.2\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "ColBERT-T1 to ColBERT\n",
      "ColBERT-T2 to ColBERT\n",
      "nDCG@10 Change\n",
      "Figure 6: A detailed comparison between T1 and T2 training\n",
      "ndcg@10 changes per query of the TREC-DL’19 query set\n",
      "6.2\n",
      "Per-Query Teacher Impact Analysis\n",
      "In addition to the aggregated results presented in Table 3, we now\n",
      "take a closer look at the impact of T1 and T2 teachers in a per-query\n",
      "analysis for ColBERT and DistilBERTDOT in Figure 6. We plot the\n",
      "differences in nDCG@10 per query on the TREC-DL’19 set between\n",
      "the original training results and the T1 and T2 training respectively.\n",
      "A positive change means the T1/T2 trained model does better on\n",
      "this particular query. We sorted the queries by the T2 changes for\n",
      "both plots, and plotted the corresponding query results for T1 at\n",
      "the same position. Overall, the T1 & T2 training for both models\n",
      "roughly improves 60 % of queries and decreases results on 33 % with\n",
      "the rest of queries unchanged. Interestingly, the average change\n",
      "in each direction between the T1 and T2 training shows that T2\n",
      "results become more extreme, as they improve more on average\n",
      "(DistilBERTDOT from T1 +10% to T2 +13%; ColBERT from T1 +6%\n",
      "to T2 +9%), but also decrease stronger on average (DistilBERTDOT\n",
      "from T1 −6.8% to T2 −7.2%; ColBERT from T1 −4.3% to T2 −7.8%).\n",
      "As we saw in Table 3 the aggregated results, still put T2 in front\n",
      "of T1 overall. However, we caution, that these stronger decreases\n",
      "show a small limitation of our knowledge distillation approach.\n",
      "7\n",
      "RELATED WORK\n",
      "Efficient relevance models. Recent studies have investigated\n",
      "different approaches for improving the efficiency of relevance mod-\n",
      "els. Ji et al. [19] demonstrate that approximations of interaction-\n",
      "based neural ranking algorithms using kernels with locality-sensitive\n",
      "hashing accelerate the query-document interaction computation.\n",
      "In order to reduce the query processing latency, Mackenzie et al.\n",
      "[33] propose a static index pruning method when augmenting the\n",
      "inverted index with precomputed re-weighted terms [8]. Several\n",
      "approaches aim to improve the efficiency of transformer models\n",
      "with windowed self-attention [17], using locality-sensitive hashing\n",
      "[23], replacing the self-attention with a local windowed and global\n",
      "attention [2] or by combining an efficient transformer-kernel model\n",
      "with a conformer layer [35].\n",
      "Adapted training procedures. In order to tackle the challenge\n",
      "of a small annotated training set, Dehghani et al. [10] propose weak\n",
      "supervision controlled by full supervision to train a confident model.\n",
      "Subsequently they demonstrate the success of a semi-supervised\n",
      "student-teacher approach for an information retrieval task using\n",
      "weakly labelled data where the teacher has access to the high quality\n",
      "labels [9]. Examining different weak supervision sources, MacA-\n",
      "vaney et al. [32] show the beneficial use of headline - content pairs\n",
      "as pseudo-relevance judgements for weak supervision. Considering\n",
      "the success of weak supervision strategies for IR, Khattab and Za-\n",
      "haria [21] train ColBERT [21] for OpenQA with guided supervision\n",
      "by iteratively using ColBERT to extract positive and negative sam-\n",
      "ples as training data. Similarly Xiong et al. [44] construct negative\n",
      "samples from the approximate nearest neighbours to the positive\n",
      "sample during training and apply this adapted training procedure\n",
      "for dense retrieval training. Cohen et al. [6] demonstrate that the\n",
      "sampling policy for negative samples plays an important role in the\n",
      "stability of the training and the overall performance with respect\n",
      "to IR metrics. MacAvaney et al. [30] adapt the training procedure\n",
      "for answer ranking by reordering the training samples and shifting\n",
      "samples to the beginning which are estimated to be easy.\n",
      "Knowledge distillation. Large pretrained language models ad-\n",
      "vanced the state-of-the-art in natural language processing and in-\n",
      "formation retrieval, but the performance gains come with high com-\n",
      "putational cost. There are numerous advances in distilling these\n",
      "models to smaller models aiming for little effectiveness loss.\n",
      "Creating smaller variants of the general-purpose BERT mode,\n",
      "Jiao et al. [20] distill TinyBert and Sanh et al. [38] create DistilBERT\n",
      "and demonstrate how to distill BERT while maintaining the models’\n",
      "accuracy for a variety of natural language understanding tasks.\n",
      "In the IR setting, Tang and Wang [40] distill sequential recom-\n",
      "mendation models for recommender systems with one teacher\n",
      "model. Vakili Tahami et al. [41] study the impact of knowledge\n",
      "distillation on BERT-based retrieval chatbots. Gao et al. [14] and\n",
      "Chen et al. [5] distilled different sizes of the same BERTCAT ar-\n",
      "chitecture and the TinyBert library [20]. As part of the PARADE\n",
      "document ranking model Li et al. [25] showed a similar BERTCAT\n",
      "to BERTCAT same-architecture knowledge distillation for different\n",
      "layer and dimension hyperparameters. A shortcoming of these dis-\n",
      "tillation approaches is that they are only applicable to the same\n",
      "architecture which restricts the retrieval model to full online in-\n",
      "ference of the BERTCAT model. Lu et al. [27] utilized knowledge\n",
      "distillation from BERTCAT to BERTDOT in the setting of keyword\n",
      "matching to select ads for sponsored search. They first showed, that\n",
      "a knowledge transfer from BERTCAT to BERTDOT is possible, albeit\n",
      "in a more restricted setting of keyword list matching in comparison\n",
      "to our fulltext ranking setting.\n",
      "8\n",
      "CONCLUSION\n",
      "We proposed to use cross-architecture knowledge distillation to\n",
      "improve the effectiveness of query latency efficient neural pas-\n",
      "sage ranking models taught by the state-of-the-art full interaction\n",
      "BERTCAT model. Following our observation that different architec-\n",
      "tures converge to different scoring ranges, we proposed to optimize\n",
      "not the raw scores, but rather the margin between a pair of relevant\n",
      "and non-relevant passages with a Margin-MSE loss. We showed\n",
      "that this method outperforms a simple pointwise MSE loss. Further-\n",
      "more, we compared the performance of a single teacher model with\n",
      "an ensemble of large BERTCAT models and find that in most cases\n",
      "using an ensemble of teachers is beneficial in the passage retrieval\n",
      "task. Trained with a teacher ensemble, single instances of efficient\n",
      "models even outperform their single instance teacher models with\n",
      "much more parameters and interaction capacity. We observed a\n",
      "drastic shift in the effectiveness-efficiency trade-off of our evaluated\n",
      "models towards more effectiveness for efficient models. In addition\n",
      "to re-ranking models, we show our general distillation method to\n",
      "produce competitive effectiveness compared to specialized training\n",
      "techniques for the dual-encoder BERTDOT model in the nearest\n",
      "neighbor retrieval setting. We published our teacher training files,\n",
      "so the community can use them without significant changes to their\n",
      "setups. For future work we plan to combine our knowledge distilla-\n",
      "tion approach with other neural ranking training adaptations, such\n",
      "as curriculum learning or dynamic index sampling for end-to-end\n",
      "neural retrieval.\n",
      "REFERENCES\n",
      "[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu,\n",
      "Rangan Majumder, Andrew Mcnamara, Bhaskar Mitra, and Tri Nguyen. 2016.\n",
      "MS MARCO : A Human Generated MAchine Reading COmprehension Dataset.\n",
      "In Proc. of NIPS.\n",
      "[2] Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-\n",
      "document transformer. arXiv preprint arXiv:2004.05150 (2020).\n",
      "[3] Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An\n",
      "overview. MSR-Tech Report (2010).\n",
      "[4] Wei-Cheng Chang, Felix X Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar.\n",
      "2020. Pre-training tasks for embedding-based large-scale retrieval. In Proc. of\n",
      "ICLR.\n",
      "[5] Xuanang Chen, Ben He, Kai Hui, Le Sun, and Yingfei Sun. 2020. Simplified Tiny-\n",
      "BERT: Knowledge Distillation for Document Retrieval. arXiv:cs.IR/2009.07531\n",
      "[6] Daniel Cohen, Scott M. Jordan, and W. Bruce Croft. 2019. Learning a Better\n",
      "Negative Sampling Policy with Deep Neural Networks for Search. In Proc. of\n",
      "ICTIR.\n",
      "[7] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2019. Overview\n",
      "of the TREC 2019 deep learning track. In TREC.\n",
      "[8] Zhuyun Dai and Jamie Callan. 2020. Context-Aware Document Term Weighting\n",
      "for Ad-Hoc Search. In Proc. of WWW.\n",
      "[9] Mostafa Dehghani, Arash Mehrjou, Stephan Gouws, Jaap Kamps, and Bernhard\n",
      "Schölkopf. 2018. Fidelity-weighted learning. Proc. of ICLR (2018).\n",
      "[10] Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, and Jaap Kamps. 2017. Learn-\n",
      "ing to learn from weak supervision by full supervision. Proc. of NIPS Workshop\n",
      "on Meta-Learning (2017).\n",
      "[11] J. Devlin, M. Chang, K. Lee, and K. Toutanova. 2019. BERT: Pre-training of Deep\n",
      "Bidirectional Transformers for Language Understanding. In Proc. of NAACL.\n",
      "[12] Yingqi Qu Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Xin Zhao, Daxiang Dong,\n",
      "Hua Wu, and Haifeng Wang. 2020. RocketQA: An Optimized Training Approach\n",
      "to Dense Passage Retrieval for Open-Domain Question Answering. arXiv preprint\n",
      "arXiv:2010.08191 (2020).\n",
      "[13] Luyu Gao, Zhuyun Dai, and Jamie Callan. 2020. EARL: Speedup Transformer-\n",
      "based Rankers with Pre-computed Representation. arXiv preprint arXiv:2004.13313\n",
      "(2020).\n",
      "[14] Luyu Gao, Zhuyun Dai, and Jamie Callan. 2020. Understanding BERT Rankers\n",
      "Under Distillation. arXiv preprint arXiv:2007.11088 (2020).\n",
      "[15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in\n",
      "a neural network. arXiv preprint arXiv:1503.02531 (2015).\n",
      "[16] Sebastian Hofstätter and Allan Hanbury. 2019. Let’s measure run time! Extending\n",
      "the IR replicability infrastructure to include performance aspects. In Proc. of\n",
      "OSIRRC.\n",
      "[17] Sebastian Hofstätter, Hamed Zamani, Bhaskar Mitra, Nick Craswell, and Allan\n",
      "Hanbury. 2020. Local Self-Attention over Long Text for Efficient Document\n",
      "Retrieval. In Proc. of SIGIR.\n",
      "[18] Sebastian Hofstätter, Markus Zlabinger, and Allan Hanbury. 2020. Interpretable\n",
      "& Time-Budget-Constrained Contextualization for Re-Ranking. In Proc. of ECAI.\n",
      "[19] Shiyu Ji, Jinjin Shao, and Tao Yang. 2019. Efficient Interaction-based Neural\n",
      "Ranking with Locality Sensitive Hashing. In Proc of. WWW.\n",
      "[20] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,\n",
      "and Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding.\n",
      "arXiv preprint arXiv:1909.10351 (2019).\n",
      "[21] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage\n",
      "Search via Contextualized Late Interaction over BERT. In Proc. of SIGIR.\n",
      "[22] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\n",
      "mization. arXiv preprint arXiv:1412.6980 (2014).\n",
      "[23] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The Efficient\n",
      "Transformer. In Proc. of ICLR.\n",
      "[24] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush\n",
      "Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning\n",
      "of language representations. arXiv preprint arXiv:1909.11942 (2019).\n",
      "[25] Canjia Li, Andrew Yates, Sean MacAvaney, Ben He, and Yingfei Sun. 2020. PA-\n",
      "RADE: Passage Representation Aggregation for Document Reranking. arXiv\n",
      "preprint arXiv:2008.09093 (2020).\n",
      "[26] Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2020. Distilling Dense\n",
      "Representations for Ranking using Tightly-Coupled Teachers. arXiv preprint\n",
      "arXiv:2010.11386 (2020).\n",
      "[27] Wenhao Lu, Jian Jiao, and Ruofei Zhang. 2020. TwinBERT: Distilling knowl-\n",
      "edge to twin-structured BERT models for efficient retrieval. arXiv preprint\n",
      "arXiv:2002.06275 (2020).\n",
      "[28] Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2020. Sparse,\n",
      "Dense, and Attentional Representations for Text Retrieval.\n",
      "arXiv preprint\n",
      "arXiv:2005.00181 (2020).\n",
      "[29] Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli\n",
      "Goharian, and Ophir Frieder. 2020. Efficient Document Re-Ranking for Trans-\n",
      "formers by Precomputing Term Representations. arXiv preprint arXiv:2004.14255\n",
      "(2020).\n",
      "[30] Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli\n",
      "Goharian, and Ophir Frieder. 2020. Training Curricula for Open Domain Answer\n",
      "Re-Ranking. In Proc. of SIGIR.\n",
      "[31] Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019. CEDR:\n",
      "Contextualized Embeddings for Document Ranking. In Proc. of SIGIR.\n",
      "[32] Sean MacAvaney, Andrew Yates, Kai Hui, and Ophir Frieder. 2019. Content-Based\n",
      "Weak Supervision for Ad-Hoc Re-Ranking. In Proc. of SIGIR.\n",
      "[33] Joel Mackenzie, Zhuyun Dai, Luke Gallagher, and Jamie Callan. 2020. Efficiency\n",
      "implications of term weighting for passage retrieval. In Proc. of SIGIR.\n",
      "[34] Christopher D Manning, Hinrich Schütze, and Prabhakar Raghavan. 2008. Intro-\n",
      "duction to information retrieval. Cambridge university press.\n",
      "[35] Bhaskar Mitra, Sebastian Hofstatter, Hamed Zamani, and Nick Craswell. 2020.\n",
      "Conformer-Kernel with Query Term Independence for Document Retrieval. arXiv\n",
      "preprint arXiv:2007.10434 (2020).\n",
      "[36] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.\n",
      "arXiv preprint arXiv:1901.04085 (2019).\n",
      "[37] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,\n",
      "Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.\n",
      "2017. Automatic differentiation in PyTorch. In Proc. of NIPS-W.\n",
      "[38] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-\n",
      "tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv\n",
      "preprint arXiv:1910.01108 (2019).\n",
      "[39] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient knowledge distilla-\n",
      "tion for bert model compression. arXiv preprint arXiv:1908.09355 (2019).\n",
      "[40] Jiaxi Tang and Ke Wang. 2018. Ranking distillation: Learning compact ranking\n",
      "models with high performance for recommender system. In Proc. of SIGKDD.\n",
      "[41] Amir Vakili Tahami, Kamyar Ghajar, and Azadeh Shakery. 2020.\n",
      "Distilling\n",
      "Knowledge for Fast Retrieval-based Chat-bots. In Proc. of SIGIR.\n",
      "[42] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\n",
      "Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al.\n",
      "2019. HuggingFace’s Transformers: State-of-the-art Natural Language Processing.\n",
      "ArXiv (2019), arXiv–1910.\n",
      "[43] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017.\n",
      "End-to-End Neural Ad-hoc Ranking with Kernel Pooling. In Proc. of SIGIR.\n",
      "[44] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,\n",
      "Junaid Ahmed, and Arnold Overwijk. 2020.\n",
      "Approximate Nearest Neigh-\n",
      "bor Negative Contrastive Learning for Dense Text Retrieval. arXiv preprint\n",
      "arXiv:2007.00808 (2020).\n",
      "[45] Ming Yan, Chenliang Li, et al. 2020. IDST at TREC 2019 Deep Learning Track:\n",
      "Deep Cascade Ranking with Generation-based Document Expansion and Pre-\n",
      "trained Language Modeling. In TREC.\n",
      "[46] Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini: Enabling the use of Lucene\n",
      "for information retrieval research. In Proc. of SIGIR.\n",
      "[47] Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang, and Jimmy Lin. 2019.\n",
      "Cross-domain modeling of sentence-level evidence for document retrieval. In\n",
      "Proc. of EMNLP-IJCNLP.\n",
      "[48] Yuyu Zhang, Ping Nie, Xiubo Geng, Arun Ramamurthy, Le Song, and Daxin Jiang.\n",
      "2020. DC-BERT: Decoupling Question and Document for Efficient Contextual\n",
      "Encoding. arXiv preprint arXiv:2002.12591 (2020).\n",
      "\n",
      "######## Attention over pre-trained Sentence Embeddings for Long Document Classification.pdf ######## \n",
      "\n",
      "Attention over pre-trained Sentence Embeddings for\n",
      "Long Document Classification\n",
      "Amine Abdaoui1,2,*, Sourav Dutta1\n",
      "1Huawei Ireland Research Center, Dublin, Ireland\n",
      "2Oracle, Paris, France\n",
      "Abstract\n",
      "Despite being the current de-facto models in most NLP tasks, transformers are often limited to short\n",
      "sequences due to their quadratic attention complexity on the number of tokens. Several attempts to\n",
      "address this issue were studied, either by reducing the cost of the self-attention computation or by\n",
      "modeling smaller sequences and combining them through a recurrence mechanism or using a new\n",
      "transformer model. In this paper, we suggest to take advantage of pre-trained sentence transformers\n",
      "to start from semantically meaningful embeddings of the individual sentences, and then combine\n",
      "them through a small attention layer that scales linearly with the document length. We report the\n",
      "results obtained by this simple architecture on three standard document classification datasets. When\n",
      "compared with the current state-of-the-art models using standard fine-tuning, the studied method obtains\n",
      "competitive results (even if there is no clear best model in this configuration). We also showcase that the\n",
      "studied architecture obtains better results when freezing the underlying transformers. A configuration\n",
      "that is useful when we need to avoid complete fine-tuning (e.g. when the same frozen transformer is\n",
      "shared by different applications). Finally, two additional experiments are provided to further evaluate\n",
      "the relevancy of the studied architecture over simpler baselines.\n",
      "Keywords\n",
      "Transformers, Sentence Embedddings, Attention, Long Document Classification.\n",
      "1. Introduction\n",
      "The Transformer model [1] is now established as the standard architecture in Natural Language\n",
      "Processing (NLP). Several variants of the original model achieved state-of-the-art results in\n",
      "many tasks [2, 3, 4] including document classification. In addition to their accurate results,\n",
      "transformers are also efficient when compared to recurrent neural network encoders. However,\n",
      "this efficiency drops significantly on long sequences. Indeed, transformers compute 𝑛 * 𝑛\n",
      "self-attention matrices to get the contextualized representations. Therefore, the memory and\n",
      "computational requirements grow-up quadratically with the number of tokens 𝑛. For this\n",
      "reason, most transformer-based models are limited to a fixed number of tokens (usually 512\n",
      "tokens).\n",
      "ReNeuIR’23: Workshop on Reaching Efficiency in Neural Information Retrieval\n",
      "*This co-author is currently employed by Oracle but this work was conducted when he was at the Huawei Ireland\n",
      "Research Center.\n",
      "$ amin.abdaoui@oracle.com (A. Abdaoui); surav.dutta2@huawei.com (S. Dutta)\n",
      "\u001a 0000-0002-6160-8461 (A. Abdaoui); 0000-0002-8934-9166 (S. Dutta)\n",
      "© 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\n",
      "CEUR\n",
      "Workshop\n",
      "Proceedings\n",
      "http://ceur-ws.org\n",
      "ISSN 1613-0073\n",
      "CEUR Workshop Proceedings (CEUR-WS.org)\n",
      "arXiv:2307.09084v1  [cs.CL]  18 Jul 2023\n",
      "To address this limitation, several attempts were made to improve the transformer efficiency\n",
      "on longer sequences. A first family of methods tries to simplify the self-attention complexity by\n",
      "reducing the number of computed weights. Concretely, instead of letting each token attend\n",
      "to every other token in the sequence, these methods restrict the computation of the attention\n",
      "weights to a small number of locations [5, 6]. Another popular approach consists in splitting the\n",
      "long input into smaller chunks that can be modeled more efficiently with a transformer. Then,\n",
      "the obtained representations can be combined using a recurrent neural network or another\n",
      "document-level transformer [7, 8]. Finally, instead of combining the outputs of different chunks\n",
      "using a new model, [9] and [4] implemented a caching mechanism that allows the first tokens\n",
      "of chunk 𝑖 to have access to the hidden states of the last tokens of chunk 𝑖 − 1.\n",
      "In this paper, we suggest to take advantage of pre-trained sentence transformers to get meaning-\n",
      "ful sentence representations without the need of any further pre-training [10]. The availability\n",
      "and variety of these models allow to easily adapt our framework to different domains and\n",
      "languages1. Based on these sentence representations, we evaluate the use of a small attention\n",
      "layer to form a document representation by giving higher weights to more important sentences.\n",
      "Note that we do not compute full self-attention matrices between all sentence pairs but only\n",
      "attention weights between the unique document representation and the different sentence\n",
      "embeddings. Indeed, we believe that sentence representations are less sensitive to external\n",
      "context than token embeddings. Similar architectures that also use linear weighted aggregations\n",
      "were evaluated on other tasks [11, 12].\n",
      "To evaluate these assumptions in the case of long document classification, we compare our\n",
      "proposed architecture with the current state-of-the-art models on three standard datasets. To our\n",
      "knowledge, this is the first detailed evaluation of these models on the same datasets. In addition\n",
      "to complete fine-tuning, we include a setting where the underlying transformers are frozen.\n",
      "Such scenario might be useful when the same transformer is shared by different applications\n",
      "(each application trains only its own task-specific layers).\n",
      "2. Related Work\n",
      "Most of the work that tried to adapt transformers to long documents were evaluated on language\n",
      "generation [13, 9, 6]. In this section, we will mainly focus on methods than can be used in\n",
      "language understanding tasks such as classification.\n",
      "The easiest way to deal with long sequences is to truncate them at the maximum sequence\n",
      "length supported by the model. Usually, the first 512 tokens are used and the following ones are\n",
      "just thrown away. Therefore, the first baseline in this paper will be simple truncation using the\n",
      "Roberta model [3], which is a widely used transformer for Natural Language Understanding.\n",
      "Furthermore, Roberta was used to initialize two other models that are also included in our\n",
      "experiments.\n",
      "A more sophisticated approach uses sparse self-attention matrices to reduce the transformer\n",
      "complexity. Instead of computing all the 𝑛 * 𝑛 weights in each matrix, the idea is to compute\n",
      "only the ones that convey important relationships. For example, Longformer [5] combines a\n",
      "windowed local attention for all tokens with a global attention for few important tokens. On\n",
      "1https://github.com/UKPLab/sentence-transformers\n",
      "the one hand, the authors proposed to compute attention weights between each token and all\n",
      "its neighbors that are included in a fixed window. On the other hand, they allow important\n",
      "tokens (e.g. [CLS]) to attend to the whole sequence of 𝑛 tokens. Thanks to these optimisations,\n",
      "the authors were able to pre-train the Longformer model, which is able to handle 4096 tokens,\n",
      "starting from Roberta weights. Another work suggested to choose the computed attention\n",
      "weights dynamically based on the content [6]. However, the model has been designed for\n",
      "character-level language generation as most of the other sparse attention methods. Therefore,\n",
      "we will only include Longformer in our evaluations to represent this family of methods.\n",
      "Another popular research direction is to use a hierarchical architecture in order to reduce\n",
      "the cost of the self-attention computation. Instead of applying one transformer to the whole\n",
      "sequence, the idea is to stack multiple models that handle a smaller number of inputs. Since\n",
      "transformer’s complexity is 𝑂(𝑛2), applying multiple transformers to smaller sequences is\n",
      "better than applying one transformer to the whole sequence. Several studies that used multiple\n",
      "levels of transformers or a recurrent neural network on top of transformers have been proposed\n",
      "[7, 14]. However, most of them have not been shared publicly with the community. SMITH,\n",
      "which is able to handle 2048 tokens, is the one of the rare pre-trained hierarchical models that\n",
      "is available online [8]. The proposed architecture is composed of two levels of abstraction: a\n",
      "sentence-level and a document-level. Each level uses a small transformer that has 4 and 3 layers\n",
      "respectively, 4 attention heads and 256 hidden dimensions. Therefore, the resulting model\n",
      "has less parameters than all the other models studied here which follow the common base\n",
      "architectures (12 layers, 12 attention-heads and 768 hidden dimensions). It was pre-trained\n",
      "using the usual masked word prediction and a novel masked sentence prediction task. Then, it\n",
      "was fine-tuned for document matching using a siamese architecture. In this paper, SMITH will\n",
      "be considered as a baseline in our experiments despite of its small size. To our knowledge, this\n",
      "is the first evaluation of SMITH on the document classification datasets considered.\n",
      "Finally, [9] proposed TransformerXL which is able to model an unlimited number of tokens.\n",
      "The proposed auto-regressive model is also applied to smaller chunks extracted from the original\n",
      "long documents. However, the modeling is not conducted independently on each chunk. At each\n",
      "time step, the previous hidden states are reused to compute the current ones introducing a sort\n",
      "of memory that propagates across the different segments. Moreover, the usual absolute position\n",
      "embeddings were replaced by relative positional encoding in order to avoid confusion on token\n",
      "positions when handling different segments. The same authors also pre-trained XLNet [9] in\n",
      "order to improve auto-regressive models in NLU tasks. In addition to handling an unlimited\n",
      "sequence length (thanks to the caching mechanism and relative positional encoding proposed\n",
      "in TransformerXL), the authors used permutation language modeling to capture bidirectional\n",
      "context when pre-training the model. For all these reasons, we will include XLNet as a baseline\n",
      "in our experiments.\n",
      "In this paper, we will compare the above mentioned baselines with an attention-based\n",
      "architecture that relies on pre-trained sentence transformers. These models are usually trained\n",
      "using a siamese architecture on sentence pair datasets to derive semantically meaningful\n",
      "sentence representations [10]. To our knowledge, this is the first attempt to use pre-trained\n",
      "sentence transformers for handling long documents.\n",
      "3. Methods\n",
      "In this section, we will detail the studied Attention over Sentence Embeddings (AoSE) architecture\n",
      "and compare its complexity and size with existing baselines.\n",
      "3.1. AoSE Architecture\n",
      "First, long documents are segmented into sentences using common sentence separators (full\n",
      "stop, line break, etc.). We define a minimum and a maximum number of tokens to avoid\n",
      "generating very small and very long segments that do not correspond to real sentences. Then,\n",
      "a sentence transformer will be used to map each sentence to a fixed dense representation 𝑠𝑖.\n",
      "Relying on such pre-trained models that are already geared towards producing meaningful\n",
      "sentence embeddings is certainly an important advantage. After that, we use an attention layer\n",
      "to combine the normalized sentence embeddings 𝑠𝑖 while giving higher weights to important\n",
      "sentences [15]. To calculate these weights 𝛼𝑖, we rely on a small neural network 𝑊𝑠 and a\n",
      "trainable context vector 𝑢𝑠 that is equivalent to the query in the Transformer’s self-attention\n",
      "definition [1].\n",
      "𝑢𝑖 = 𝑡𝑎𝑛ℎ(𝑊𝑠 × 𝑠𝑖 + 𝑏𝑠)\n",
      "(1)\n",
      "𝛼𝑖 =\n",
      "exp(𝑢𝑇\n",
      "𝑖 × 𝑢𝑠)\n",
      "∑︀𝑡\n",
      "𝑗=1 exp(𝑢𝑇\n",
      "𝑗 × 𝑢𝑠)\n",
      "(2)\n",
      "The document representation 𝑣 is then computed using a weighted sum of the different\n",
      "sentence embeddings 𝑠𝑖.\n",
      "𝑣 =\n",
      "𝑡\n",
      "∑︁\n",
      "𝑖=1\n",
      "𝛼𝑖 × 𝑠𝑖\n",
      "(3)\n",
      "Finally, a dense layer can be added on top of the document embedding 𝑣 in order to perform\n",
      "classification. All these steps are presented in Figure 1 below.\n",
      "3.2. Model Complexity\n",
      "Let’s define the sequence length 𝑛 as the product of the number of sentences 𝑡 and the length of\n",
      "one sentence 𝑙. The complexity of a vanilla transformer (e.g. Roberta) is therefore 𝑂(𝑡2 × 𝑙2).\n",
      "The sentence transformer of SMITH computes full self-attention between all tokens in each\n",
      "sentence, while the document transformer applies a second full self-attention computation\n",
      "between all sentences. Thus, the complexity of the whole SMITH encoder is 𝑂(𝑡 × 𝑙2 + 𝑡2).\n",
      "Longformer computes global attention for 𝑔 important tokens and local attention for the\n",
      "remaining ones. Let 𝑤 be the window size for local attention tokens. The Longformer’s\n",
      "complexity is therefore 𝑂(𝑔 × 𝑡 × 𝑙 + (𝑡 × 𝑙 − 𝑔) × 𝑤).\n",
      "XLNet computes full self-attention for each chunk. Let 𝑐 be the maximum length of one\n",
      "XLNet segment (chunk). Even if all the previous hidden states are cached and reused, a given\n",
      "Sentence 1\n",
      "Sentence \n",
      "Transformer\n",
      "Sentence 2\n",
      "Sentence \n",
      "Transformer\n",
      "Sentence n\n",
      "Sentence \n",
      "Transformer\n",
      "𝑣\n",
      "𝑊𝑠\n",
      "𝑢𝑠\n",
      "Document\n",
      "…\n",
      "𝑠1\n",
      "𝑠2\n",
      "𝑠𝑛\n",
      "+\n",
      "x\n",
      "x\n",
      "x\n",
      "𝛼1\n",
      "𝛼2\n",
      "𝛼3\n",
      "Context \n",
      "(query)\n",
      "Document \n",
      "embedding\n",
      "MLP\n",
      "Class\n",
      "Figure 1: The proposed Attention over Sentence Embeddings (AoSE) architecture.\n",
      "Table 1\n",
      "Complexity of the different models (𝑡 is the number of sentences, 𝑙 is the average number of tokens per\n",
      "sentence, 𝑔 is the number of global attention tokens, 𝑤 is the window size for local attention tokens,\n",
      "and c is the length of one XLNet segment).\n",
      "Model\n",
      "Complexity\n",
      "Roberta\n",
      "𝑂(𝑡2 × 𝑙2)\n",
      "SMITH\n",
      "𝑂(𝑡 × 𝑙2 + 𝑡2)\n",
      "Longformer\n",
      "𝑂(𝑔 × 𝑡 × 𝑙 + (𝑡 × 𝑙 − 𝑔) × 𝑤)\n",
      "XLNet\n",
      "𝑂(𝑡 × 𝑙 × 𝑐)\n",
      "AoSE\n",
      "𝑂(𝑡 × 𝑙2 + 𝑡)\n",
      "token can’t attend to more than 𝑐 locations. Therefore, the complexity of one chunk is 𝑂(𝑐2)\n",
      "and the number of chunks is 𝑡 × 𝑙/𝑐. Consequently, the complexity of XLNet is 𝑂(𝑡 × 𝑙 × 𝑐).\n",
      "The sentence transformer of our proposed architecture is equivalent to the one used in\n",
      "SMITH, but since our document-level attention is linear with the number of sentences, the\n",
      "complexity of our architecture is 𝑂(𝑡 × 𝑙2 + 𝑡).\n",
      "Table 2\n",
      "Different size comparisons of the evaluated models.\n",
      "Model\n",
      "Disk\n",
      "#Params\n",
      "Vocab.\n",
      "#Layers /\n",
      "Hidden\n",
      "Max input\n",
      "size\n",
      "(million)\n",
      "size\n",
      "#Heads\n",
      "dim.\n",
      "#tokens\n",
      "Roberta\n",
      "478 MB\n",
      "125 m\n",
      "50.265\n",
      "12 / 12\n",
      "768\n",
      "512\n",
      "SMITH\n",
      "47 MB\n",
      "12 m\n",
      "30.522\n",
      "4+3 / 4\n",
      "256\n",
      "2.048\n",
      "Longformer\n",
      "570 MB\n",
      "149 m\n",
      "50.265\n",
      "12 / 12\n",
      "768\n",
      "4.096\n",
      "XLNet\n",
      "445 MB\n",
      "117 m\n",
      "32.000\n",
      "12 / 12\n",
      "768\n",
      "unlimited\n",
      "AoSE\n",
      "480 MB\n",
      "126 m\n",
      "50.265\n",
      "12 / 12\n",
      "768\n",
      "unlimited\n",
      "3.3. Model Size\n",
      "Due to hardware limitations, we decided to work with the base versions of Roberta, Longformer\n",
      "and XLNet even if large versions were also available. Similarly, we have chosen a base sentence\n",
      "transformer2 in our AoSE architecture. The chosen sentence transformer was initialized with\n",
      "Roberta base. Therefore, these two models share the exact same number of parameters. SMITH\n",
      "is the only exception as its only available version is much smaller in size. Table 2 presents\n",
      "several size-related measurements for all the models evaluated in this work.\n",
      "We can also notice that our AoSE architecture has slightly more parameters than Roberta. As\n",
      "mentioned before, our architecture is composed of a sentence transformer (that has the same\n",
      "size as Roberta) and a small attention layer that has less than 1 million parameters. Most of\n",
      "them are located in the 𝑊𝑠 matrix that has a shape of 768 × 768. Therefore, our proposed\n",
      "architecture does not add a lot of parameters when compared to a standard transformer.\n",
      "4. Experiments\n",
      "In this section, we will assess the performance of our architecture along with the selected\n",
      "baselines for long document classification.\n",
      "4.1. Datasets\n",
      "Three classification datasets (of several thousands of documents each) were chosen to conduct\n",
      "our experiments. The first one is the widely used IMDB dataset [16]. We used the binary\n",
      "version3 that distinguishes positive and negative movie reviews. The second one is MIND [17],\n",
      "a large-scale dataset for news recommendation. We used the topic classification task from this\n",
      "dataset4 and discarded a couple of topics that have less than 3 documents. The third one is\n",
      "the 20 News Groups dataset [18]. We used the cleaned version5 that do not contain headers,\n",
      "signatures, and quotations. Table 3 below shows several statistics related to the number of\n",
      "documents, the length of these documents and the number of classes for each dataset. The only\n",
      "2https://huggingface.co/sentence-transformers/nli-roberta-base-v2\n",
      "3https://huggingface.co/datasets/imdb\n",
      "4https://msnews.github.io\n",
      "5https://huggingface.co/datasets/SetFit/20_newsgroups\n",
      "Table 3\n",
      "Statistics of the different datasets: (i) the total number of documents, (ii) the number of long documents\n",
      "(having more than 512 tokens), (iii) the average number of tokens per document, (iv) the maximum\n",
      "number of tokens per document, and (v) the number of labels. The number of tokens were computed\n",
      "using the roberta tokenizer.\n",
      "Dataset\n",
      "Split\n",
      "#Docs\n",
      "#Docs\n",
      "Avg\n",
      "Max\n",
      "#Labels\n",
      "all\n",
      ">512\n",
      "#Tokens\n",
      "#Tokens\n",
      "IMDB\n",
      "train\n",
      "25.000\n",
      "7.729\n",
      "323\n",
      "3.240\n",
      "2\n",
      "test\n",
      "25.000\n",
      "3.537\n",
      "314\n",
      "3.257\n",
      "2\n",
      "MIND\n",
      "train\n",
      "101.523\n",
      "48.093\n",
      "696\n",
      "51.662\n",
      "15\n",
      "test\n",
      "28.275\n",
      "13.160\n",
      "651\n",
      "28.287\n",
      "15\n",
      "20 News\n",
      "train\n",
      "11.314\n",
      "1.245\n",
      "398\n",
      "49.561\n",
      "20\n",
      "Groups\n",
      "test\n",
      "7.532\n",
      "784\n",
      "372\n",
      "132.115\n",
      "20\n",
      "additional preprocessing step that was applied to these datasets consisted in removing HTML\n",
      "tags.\n",
      "4.2. Experimental Settings\n",
      "Even if XLNet and AoSE can theoretically handle sequences of unlimited length6, we had to\n",
      "set a maximum sequence length to each one due to practical hardware restrictions. Indeed, we\n",
      "were not able to fine-tune XLNet on very long sequences using our 2 × 16 GB GPUs (even with\n",
      "a batch size of 1). Therefore, we decided to set the maximum sequence length of XLNet to 4096\n",
      "tokens in our experiments (which covers all IMDB and more than 99% of MIND and 20 News\n",
      "Groups). In the case our AoSE model, we were able to input up to 8192 tokens which covers\n",
      "almost all the documents included in the three datasets.\n",
      "We also decided to perform our experiments in two different settings. In the first one, we\n",
      "train all the parameters of every architecture which correspond to standard fine-tuning. In\n",
      "the second setting, we decided to freeze the weights of the underlying transformers. In this\n",
      "case, the different transformers are applied once to produce frozen representations. Then, the\n",
      "training will only happen on the top level parameters of the different architectures. This means\n",
      "that we will only train classifiers for all the baselines, and the classifier along with the attention\n",
      "layer of our AoSE architecture. Training our linear attention layer do not take more time than\n",
      "training the classifier itself.\n",
      "Regarding the other experimental settings, we set the minimum number of tokens per sentence\n",
      "for our AoSE system to 5 and the maximum value to 250. The document representation in the\n",
      "frozen setting is average pooling as it allowed us to obtain better results for all models. When\n",
      "fine-tuning, we use the default pooling strategy implemented by each model. For all models\n",
      "and all datasets, we set the learning rate to 2𝑒−5 and the batch size to 16. When the memory of\n",
      "our GPUs is exceeded, we reduce the batch size but use gradient accumulation to simulate the\n",
      "same parameters update as with a batch size of 16. When freezing the transformers, we train all\n",
      "models for 50 epochs on each dataset. When fine-tuning, we train all models for 20 epochs on\n",
      "6There is no structural limitation caused by the model definition (for example, the size of the position embeddings\n",
      "matrix).\n",
      "Table 4\n",
      "Results on IMDB.\n",
      "Max\n",
      "Fine-tuning\n",
      "Freezing\n",
      "Model\n",
      "seq.\n",
      "Time\n",
      "Acc.\n",
      "Acc.\n",
      "Acc.\n",
      "Time\n",
      "Acc.\n",
      "Acc.\n",
      "Acc.\n",
      "length\n",
      "(hh:mm)\n",
      "all\n",
      "<=512\n",
      ">512\n",
      "(hh:mm)\n",
      "all\n",
      "<=512\n",
      ">512\n",
      "Roberta\n",
      "512\n",
      "05:40\n",
      "95.2\n",
      "95.6\n",
      "93.0\n",
      "00:12\n",
      "91.8\n",
      "92.2\n",
      "88.4\n",
      "SMITH\n",
      "2048\n",
      "04:28\n",
      "91.0\n",
      "91.0\n",
      "90.8\n",
      "00:29\n",
      "74.0\n",
      "74.2\n",
      "73.0\n",
      "Longformer\n",
      "4096\n",
      "22:50\n",
      "95.6\n",
      "95.6\n",
      "95.4\n",
      "00:30\n",
      "92.0\n",
      "92.0\n",
      "91.2\n",
      "XLNet\n",
      "4096\n",
      "22:20\n",
      "95.6\n",
      "95.5\n",
      "95.7\n",
      "00:29\n",
      "92.2\n",
      "92.2\n",
      "91.8\n",
      "AoSE\n",
      "8192\n",
      "15:50\n",
      "95.7\n",
      "95.7\n",
      "95.8\n",
      "00:35\n",
      "93.2\n",
      "93.2\n",
      "93.1\n",
      "Table 5\n",
      "Results on MIND.\n",
      "Max\n",
      "Fine-tuning\n",
      "Freezing\n",
      "Model\n",
      "seq.\n",
      "Time\n",
      "Acc.\n",
      "Acc.\n",
      "Acc.\n",
      "Time\n",
      "Acc.\n",
      "Acc.\n",
      "Acc.\n",
      "length\n",
      "(hh:mm)\n",
      "all\n",
      "<=512\n",
      ">512\n",
      "(hh:mm)\n",
      "all\n",
      "<=512\n",
      ">512\n",
      "Roberta\n",
      "512\n",
      "09:20\n",
      "83.1\n",
      "80.7\n",
      "85.8\n",
      "00:45\n",
      "77.4\n",
      "73.8\n",
      "81.6\n",
      "SMITH\n",
      "2048\n",
      "13:30\n",
      "80.8\n",
      "77.4\n",
      "84.8\n",
      "00:52\n",
      "76.0\n",
      "71.6\n",
      "81.1\n",
      "Longformer\n",
      "4096\n",
      "66:45\n",
      "84.1\n",
      "80.7\n",
      "88.0\n",
      "02:05\n",
      "77.7\n",
      "73.7\n",
      "82.3\n",
      "XLNet\n",
      "4096\n",
      "81:58\n",
      "83.4\n",
      "79.8\n",
      "87.3\n",
      "04:12\n",
      "78.3\n",
      "74.2\n",
      "82.9\n",
      "AoSE\n",
      "8192\n",
      "83:10\n",
      "83.5\n",
      "79.8\n",
      "87.5\n",
      "04:58\n",
      "79.1\n",
      "75.0\n",
      "83.9\n",
      "IMDB and News Groups, and for 10 epochs on MIND.\n",
      "4.3. Evaluations\n",
      "Tables 4, 5 and 6 present the results obtained by the different models on each dataset7.Overall,\n",
      "we can observe that Longformer, XLNet and AoSE obtain better accuracies on long documents\n",
      "when compared with the remaining baselines. When fine-tuning the transformers, there is no\n",
      "clear best model between them across the three datasets, which joins the conclusions drawn\n",
      "in [19]. However, when freezing the transformers, our AoSE model obtains systematically the\n",
      "best results. We believe that this setting is useful for applications that use the same underlying\n",
      "transformer as encoder for multiple tasks or s imply for applications that cannot afford expensive\n",
      "training. Finally, being much smaller than the other models, SMITH obtains the worst accuracies\n",
      "in both settings across all the datasets.\n",
      "Regarding the training speed, we can observe that the frozen setting reduces drastically\n",
      "the training time. Since the Max seq. length differs from one model to the other, comparing\n",
      "their training times is not straightforward. However, we can use the IMDB dataset for a fair\n",
      "comparison between Longformer, XLNet and AoSE as all IMDB documents can be modeled\n",
      "entirely without any truncation by these three models8. In this case, AoSE is faster than the\n",
      "two other models in the fine-tuning setting, but slightly slower in the frozen setting.\n",
      "7For more information about the Max seq. length mentioned in these tables, see subsection 4.2.\n",
      "8the longest IMDB document has less than 4096 tokens.\n",
      "Table 6\n",
      "Results on 20 News Groups.\n",
      "Max\n",
      "Fine-tuning\n",
      "Freezing\n",
      "Model\n",
      "seq.\n",
      "Time\n",
      "Acc.\n",
      "Acc.\n",
      "Acc.\n",
      "Time\n",
      "Acc.\n",
      "Acc.\n",
      "Acc.\n",
      "length\n",
      "(hh:mm)\n",
      "all\n",
      "<=512\n",
      ">512\n",
      "(hh:mm)\n",
      "all\n",
      "<=512\n",
      ">512\n",
      "Roberta\n",
      "512\n",
      "02:15\n",
      "72.5\n",
      "71.5\n",
      "83.2\n",
      "00:05\n",
      "63.7\n",
      "62.6\n",
      "75.0\n",
      "SMITH\n",
      "2048\n",
      "02:30\n",
      "60.0\n",
      "58.6\n",
      "74.5\n",
      "00:07\n",
      "56.4\n",
      "55.0\n",
      "71.8\n",
      "Longformer\n",
      "4096\n",
      "10:20\n",
      "72.7\n",
      "71.4\n",
      "84.3\n",
      "00:10\n",
      "64.1\n",
      "62.8\n",
      "77.7\n",
      "XLNet\n",
      "4096\n",
      "13:36\n",
      "72.8\n",
      "71.7\n",
      "84.2\n",
      "00:17\n",
      "65.3\n",
      "63.8\n",
      "79.5\n",
      "AoSE\n",
      "8192\n",
      "14:30\n",
      "72.7\n",
      "71.5\n",
      "83.9\n",
      "00:25\n",
      "66.0\n",
      "64.4\n",
      "79.9\n",
      "Table 7\n",
      "Ablation study conducted on IMDB: (i) S-Roberta refers to the chosen sentence transformer used alone\n",
      "and applied to the whole sequence (truncated after 512 tokens); (ii) AoSE-xxx refers to the application of\n",
      "the proposed architecture that also uses S-Roberta (input sequences are truncated after xxx tokens).\n",
      "Model\n",
      "Max\n",
      "Fine-tuning\n",
      "Freezing\n",
      "sequence\n",
      "Acc.\n",
      "Acc.\n",
      "Acc.\n",
      "Acc.\n",
      "Acc.\n",
      "Acc.\n",
      "length\n",
      "all\n",
      "<=512\n",
      ">512\n",
      "all\n",
      "<=512\n",
      ">512\n",
      "S-Roberta\n",
      "512\n",
      "95.3\n",
      "95.6\n",
      "92.8\n",
      "92.2\n",
      "92.7\n",
      "89.1\n",
      "AoSE-512\n",
      "512\n",
      "95.4\n",
      "95.7\n",
      "93.0\n",
      "92.6\n",
      "93.0\n",
      "89.9\n",
      "AoSE-1024\n",
      "1024\n",
      "95.7\n",
      "95.7\n",
      "95.7\n",
      "93.0\n",
      "93.1\n",
      "92.5\n",
      "AoSE-2048\n",
      "2048\n",
      "95.7\n",
      "95.7\n",
      "95.8\n",
      "93.1\n",
      "93.1\n",
      "92.9\n",
      "AoSE-4096\n",
      "4096\n",
      "95.7\n",
      "95.7\n",
      "95.8\n",
      "93.2\n",
      "93.2\n",
      "93.1\n",
      "4.4. Ablation Study\n",
      "We conduct an ablation study to further investigate the relevancy of the studied architecture\n",
      "over simpler baselines. We compare the results obtained by the selected sentence transformer\n",
      "alone (S-Roberta) with different versions of our AoSE architecture that use the same sentence\n",
      "transformer as their first component. The new AoSE versions are fed with sequences that\n",
      "are truncated after 512, 1024, 2048 and 4096 tokens respectively.Table 7 shows the obtained\n",
      "results on the IMDB dataset. It appears that the AoSE architecture is relevant and benefits from\n",
      "increasing the sequence length especially on long documents (that have more than 512 tokens).\n",
      "We also observe a slight improvement on short documents (having less than 512 tokens) in\n",
      "the frozen setting, which may be explained by a better attention layer after training on the\n",
      "additional sentences that appear at the end of long documents.\n",
      "4.5. Impact of the chosen sentence transformers\n",
      "Finally, we evaluate the impact of the chosen sentence transformer on the final results. Table\n",
      "8 below shows the results obtained with three different sentence transformers in the same\n",
      "two settings presented earlier (fine-tuning and frozen). In addition to the already evaluated\n",
      "S-Roberta, two other sentence transformers have been included here: S-BERT9 and S-MPNet10.\n",
      "9https://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens\n",
      "10https://huggingface.co/sentence-transformers/paraphrase-mpnet-base-v2\n",
      "Table 8\n",
      "Results of different sentence transformers used either alone and inside our architecture on the IMDB\n",
      "dataset. Ao(S-Roberta) refers to attention over S-Roberta, the same model used in our previous experi-\n",
      "ments. Ao(S-BERT) refers to attention over S-BERT. Ao(S-MPNet) refers to attention over S-MPNet.\n",
      "Model\n",
      "Max\n",
      "Fine-tuning\n",
      "Freezing\n",
      "sequence\n",
      "Acc.\n",
      "Acc.\n",
      "Acc.\n",
      "Acc.\n",
      "Acc.\n",
      "Acc.\n",
      "length\n",
      "all\n",
      "<=512\n",
      ">512\n",
      "all\n",
      "<=512\n",
      ">512\n",
      "S-Roberta\n",
      "512\n",
      "95.3\n",
      "95.6\n",
      "92.8\n",
      "92.2\n",
      "92.7\n",
      "89.1\n",
      "Ao(S-Roberta)\n",
      "8192\n",
      "95.7\n",
      "95.7\n",
      "95.8\n",
      "93.2\n",
      "93.2\n",
      "93.1\n",
      "S-BERT\n",
      "512\n",
      "94.0\n",
      "94.5\n",
      "86.2\n",
      "87.0\n",
      "87.9\n",
      "82.1\n",
      "Ao(S-BERT)\n",
      "8192\n",
      "94.8\n",
      "94.8\n",
      "94.6\n",
      "90.7\n",
      "90.7\n",
      "90.4\n",
      "S-MPNet\n",
      "512\n",
      "95.2\n",
      "95.5\n",
      "93.3\n",
      "91.7\n",
      "92.2\n",
      "88.3\n",
      "Ao(S-MPNet)\n",
      "8192\n",
      "95.6\n",
      "95.5\n",
      "96.0\n",
      "93.3\n",
      "93.3\n",
      "92.8\n",
      "Again, the IMDB dataset is used to conduct these experiments.\n",
      "Each model is first applied directly to the whole sequence (truncated after 512 tokens)11.\n",
      "Then, it is used inside the studied architecture that is able to handle up to 8192 tokens. Overall, it\n",
      "appears that S-BERT obtains lower results than S-Roberta and S-MPNet either when used alone\n",
      "or inside our architecture. Therefore, we can say that the choice of the sentence transformer\n",
      "has an important impact on the final results. But more importantly, we observe that using\n",
      "the same models inside our AoSE architecture allow to improve the results of all models.\n",
      "This improvement is observable in both settings (fine-tuning and freezing), regardless of the\n",
      "underlying sentence transformer.\n",
      "5. Conclusion\n",
      "In this paper, we investigated the relevancy of pre-trained sentence transformers for long\n",
      "document classification. To our knowledge this is the first time these pre-trained models are\n",
      "used to handle long documents. To do so, we combine the sentence representations using\n",
      "a trainable attention layer to give high weights to important sentences. We have shown\n",
      "that this simple method is competitive when compared with current state-of-the-art models\n",
      "in the standard fine-tuning mode. We also considered another mode where the underlying\n",
      "transformers are frozen, which allows to speed-up the training and to share the same underlying\n",
      "transformer between different applications. In this case, the AoSE architecture obtains better\n",
      "results. Additional experiments have shown an improvement over the direct application of the\n",
      "same sentence transformers. Finally, relying on pre-trained sentence transformers allows to\n",
      "easily extend our architecture to different domains and languages. For example, we can simply\n",
      "replace the English sentence transformer used in this paper with a multilingual one12 to handle\n",
      "multilingual texts, whereas XLNet and Longformer need to be pre-trained again on multilingual\n",
      "datasets.\n",
      "11Initial evaluations have shown that applying sentence transformers directly to the whole document gives better\n",
      "results than applying them to each sentence and then averaging the different embeddings.\n",
      "12For example: https://huggingface.co/sentence-transformers/stsb-xlm-r-multilingual\n",
      "References\n",
      "[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser,\n",
      "I. Polosukhin, Attention is all you need, in: I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,\n",
      "R. Fergus, S. Vishwanathan, R. Garnett (Eds.), Advances in Neural Information Processing\n",
      "Systems, volume 30, Curran Associates, Inc., 2017.\n",
      "[2] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-training of deep bidirectional\n",
      "transformers for language understanding, in: Proceedings of the 2019 Conference of\n",
      "the North American Chapter of the Association for Computational Linguistics: Human\n",
      "Language Technologies, Volume 1 (Long and Short Papers), Association for Computational\n",
      "Linguistics, 2019, pp. 4171–4186. doi:10.18653/v1/N19-1423.\n",
      "[3] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer,\n",
      "V. Stoyanov, Roberta: A robustly optimized bert pretraining approach, arXiv preprint\n",
      "arXiv:1907.11692 (2019).\n",
      "[4] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, Q. V. Le, Xlnet: Generalized\n",
      "autoregressive pretraining for language understanding, Advances in neural information\n",
      "processing systems 32 (2019).\n",
      "[5] I. Beltagy, M. E. Peters, A. Cohan, Longformer: The long-document transformer, arXiv\n",
      "preprint arXiv:2004.05150 (2020).\n",
      "[6] A. Roy, M. Saffar, A. Vaswani, D. Grangier, Efficient content-based sparse attention with\n",
      "routing transformers, Transactions of the Association for Computational Linguistics 9\n",
      "(2021) 53–68.\n",
      "[7] R. Pappagari, P. Zelasko, J. Villalba, Y. Carmiel, N. Dehak, Hierarchical transformers for\n",
      "long document classification, in: IEEE Automatic Speech Recognition and Understanding\n",
      "Workshop (ASRU), volume https://ieeexplore.ieee.org/abstract/document/9003958, 2019,\n",
      "pp. 838–844. doi:10.1109/ASRU46091.2019.9003958.\n",
      "[8] L. Yang, M. Zhang, C. Li, M. Bendersky, M. Najork, Beyond 512 tokens: Siamese multi-depth\n",
      "transformer-based hierarchical encoder for long-form document matching, in: Proceedings\n",
      "of the 29th ACM International Conference on Information & Knowledge Management,\n",
      "2020, pp. 1725–1734.\n",
      "[9] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le, R. Salakhutdinov, Transformer-XL: Attentive\n",
      "language models beyond a fixed-length context, in: Proceedings of the 57th Annual\n",
      "Meeting of the Association for Computational Linguistics, Association for Computational\n",
      "Linguistics, Florence, Italy, 2019, pp. 2978–2988. doi:10.18653/v1/P19-1285.\n",
      "[10] N. Reimers, I. Gurevych, Sentence-bert: Sentence embeddings using siamese bert-networks,\n",
      "in: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-\n",
      "cessing, Association for Computational Linguistics, 2019.\n",
      "[11] C. Li, A. Yates, S. MacAvaney, B. He, Y. Sun, Parade: Passage representation aggregation\n",
      "for document reranking, arXiv preprint arXiv:2008.09093 (2020).\n",
      "[12] S. Althammer, S. Hofstätter, M. Sertkan, S. Verberne, A. Hanbury, Parm: A paragraph\n",
      "aggregation retrieval model for dense document-to-document retrieval, in: Advances in\n",
      "Information Retrieval: 44th European Conference on IR Research, ECIR 2022, Stavanger,\n",
      "Norway, April 10–14, 2022, Proceedings, Part I, Springer, 2022, pp. 19–34.\n",
      "[13] N. Kitaev, L. Kaiser, A. Levskaya, Reformer: The efficient transformer, in: International\n",
      "Conference on Learning Representations, 2019.\n",
      "[14] C. Wu, F. Wu, T. Qi, Y. Huang, Hi-transformer: Hierarchical interactive transformer\n",
      "for efficient and effective long document modeling, in: Proceedings of the 59th Annual\n",
      "Meeting of the Association for Computational Linguistics and the 11th International Joint\n",
      "Conference on Natural Language Processing (Volume 2: Short Papers), 2021, pp. 848–853.\n",
      "[15] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, E. Hovy, Hierarchical attention networks for\n",
      "document classification, in: Proceedings of the 2016 conference of the North American\n",
      "chapter of the association for computational linguistics: human language technologies,\n",
      "2016, pp. 1480–1489.\n",
      "[16] A. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, C. Potts, Learning word vectors for\n",
      "sentiment analysis, in: Proceedings of the 49th annual meeting of the association for\n",
      "computational linguistics: Human language technologies, 2011, pp. 142–150.\n",
      "[17] F. Wu, Y. Qiao, J.-H. Chen, C. Wu, T. Qi, J. Lian, D. Liu, X. Xie, J. Gao, W. Wu, et al., Mind: A\n",
      "large-scale dataset for news recommendation, in: Proceedings of the 58th Annual Meeting\n",
      "of the Association for Computational Linguistics, 2020, pp. 3597–3606.\n",
      "[18] K. Lang, Newsweeder: Learning to filter netnews, in: Machine Learning Proceedings 1995,\n",
      "Elsevier, 1995, pp. 331–339.\n",
      "[19] H. Park, Y. Vyas, K. Shah, Efficient classification of long documents using transformers, in:\n",
      "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n",
      "(Volume 2: Short Papers), Association for Computational Linguistics, Dublin, Ireland, 2022,\n",
      "pp. 702–709. doi:10.18653/v1/2022.acl-short.79.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for filename, file_content in files.items():\n",
    "    print(f\"######## {filename} ######## \\n\")\n",
    "    print(file_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".datenanalyse",
   "language": "python",
   "name": ".datenanalyse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

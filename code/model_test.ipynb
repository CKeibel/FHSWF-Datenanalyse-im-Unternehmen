{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb2effe5-c9d2-47fa-993b-23f6aa57795f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -U transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8835f58f-e141-4858-97b3-30a7b5f5f3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.37.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d70e5d6-a297-4eed-9897-5e3bbc6771d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb021b4e-8d67-4960-b8a6-d09f28b4592f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10bef43-1e0d-41f9-a330-4c335c6dad43",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8b02c05-f858-4848-93bf-fa2221831987",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderModel:\n",
    "    \"\"\"Encoder class to encode documents and queries into a vector space.\"\"\"\n",
    "    def __init__(self, model_id, device):\n",
    "        self.model_id = model_id\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "        self.model = AutoModel.from_pretrained(self.model_id).to(self.device)\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    def tokenize(self, document):\n",
    "        return self.tokenizer(document, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    def model_inference(self, tokenized_document):\n",
    "        with torch.no_grad():\n",
    "            outputs = outputs = self.model(**tokenized_document)\n",
    "        return outputs\n",
    "    \n",
    "    def __call__(self, document):\n",
    "        tokenized_document = self.tokenize(document)\n",
    "        outputs = self.model_inference(tokenized_document)\n",
    "        embeddings = self.mean_pooling(outputs, tokenized_document[\"attention_mask\"])\n",
    "        return F.normalize(embeddings, p=2, dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac98febc-202c-45e1-a6fc-83396073305e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "encoder = EncoderModel(model_id, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a316d666-7791-4cd8-8feb-2b160c01844d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectors = encoder([\"Hello World\", \"Hello Mars\", \"Transformers are awesome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "185035c2-2733-4ad2-8821-6c289348b7b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000002 , 0.5999794 , 0.1535384 ],\n",
       "       [0.5999794 , 1.0000002 , 0.17221901],\n",
       "       [0.1535384 , 0.17221901, 1.0000001 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750c1a2a-8ed1-4c02-8131-01a8c21d577e",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c47d85e-4ff5-48f0-a8f5-2681fc61badc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderModel:\n",
    "    \"\"\"Causal language model to generate text based on a prompt.\"\"\"\n",
    "    def __init__(self, model_id, generation_config=None, device=None, **kwargs):\n",
    "        self.model_id = model_id\n",
    "        self.device = device if device else \"auto\"\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device, **kwargs)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        # TODO\n",
    "        self.generation_config=generation_config if generation_config else {}\n",
    "\n",
    "    def tokenize(self, prompt):\n",
    "        _device = \"cuda\" if self.device == \"auto\" else self.device\n",
    "        tokenized_prompt = self.tokenizer(prompt, return_tensors=\"pt\").to(_device)\n",
    "        return tokenized_prompt\n",
    "\n",
    "    def __call__(self, prompt):\n",
    "        tokenized_prompt = self.tokenize(prompt)\n",
    "        outputs = self.model.generate(**tokenized_prompt, max_new_tokens=50) # TODO: generation config\n",
    "        return self.tokenizer.decode(outputs[0][len(tokenized_prompt.input_ids[0]):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de8f1c9b-be36-4da5-98b3-d72bb7115a70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfa010dc51e4e6bb0f69aa5ec10f0c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "causal_lm = DecoderModel(model_id, device=\"cuda:0\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3962a112-aee9-4ec5-b4b6-03ee08c1a4d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " '\\n'\n",
      " 'I’m trying to figure out how to get to the top of this mountain.\\n'\n",
      " '\\n'\n",
      " 'Why are you trying to climb that mountain?\\n'\n",
      " '\\n'\n",
      " 'Because it’s there.\\n'\n",
      " '\\n'\n",
      " 'But why do you want to climb it? What')\n"
     ]
    }
   ],
   "source": [
    "pprint(causal_lm(\"What are you doing?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

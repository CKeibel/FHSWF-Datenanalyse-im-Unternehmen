\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{hyperref} % hyperlinks to reference
\usepackage[onehalfspacing]{setspace} % line spacing
\usepackage{geometry}

% Document Layout ==================================================
\geometry{
   left=3cm,
   right=3cm}

% Title ============================================================
\title{Datenanalyse im Unternehmen}
\author{Christopher Keibel \\ 30341906 \\South Westphalia University of Applied Sciences }
\date{\today}

% Document start ===================================================
\begin{document}

\maketitle

% Abstract =========================================================
\begin{abstract}
    Abstract Text...
\end{abstract}

% Main document ====================================================
\section{Introduction}
Since the arrival of \textit{ChatGPT}, generative AI algorithms have made their way into the mainstream media and the everyday lives of many\footnote{\href{https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/}{Reuters}: \textit{ChatGPT} sets record for fastest-growing user base} people. Every day, numerous people use the chatbot \textit{ChatGPT} developed by \textit{OpenAI} to make everyday tasks easier. Behind its technological success is an algorithm called \textit{Transformer}\cite{attentionIsAllYouNeed}, an \textit{encoder-decoder} network that can efficiently process long text passages. To be precise, \textit{ChatGPT} utilizes only the \textit{decoder}\cite{GPT} component of the original \textit{Transformer}.
In \cite{GPT}, \textit{OpenAI} demonstrates that \textit{large language models} are capable of learning a series of text-based tasks by a so called \textit{generative pretraining} on unlabeled data and a subsequent \textit{supervised} task-specific \textit{fine tuning}.

Even if the capabilities of a model trained in this way are impressive, they have a serious problem. The \textit{world knowledge} learned during the training of such models is stored in the model parameters\cite{RAG}, so that conclusions about the correctness of the generated outputs cannot be made. Here we often speak of model \textit{hallucination}, as it may generate outputs with a high confidence that may not be factually correct.

Another problem is a so-called knowledge cutoff. This means that models can only learn the knowledge that is contained in their training data. Even if these models are trained on a large corpus of data, their knowledge ends with the date on which the data was last collected. In order to gain new knowledge, these models have to be retrained at some expense.

In \cite{RAG}, a framework called \textit{retrieval augmented generation (RAG)} is proposed, which is intended to solve these problems. By adding a non-parameterized memory in the form of a vector database, the knowledge of the \textit{language model} is extended. 

The workflow of this framework is defined by three main components. In addition to the generative model, another \textit{language model} which is called \textit{retriever}, is used to search the additional knowledge that is stored in the vector database on the basis of a query. The found information and the query are then passed to the third component, the generative \textit{language model}, which then answers the query based on the extracted information.

Unlike online web search engines such as \textit{Google} or \textit{Bing}, this setup also allows users to perform searches on their own data by indexing it using a vector database. 
\\
\\
This work discusses and evaluates the various components and the workflow of the framework. The capabilities of this technology are illustrated and possible drawbacks and problems are outlined.

\section{Retrieval Augmented Generation (RAG)}
As we have learned above, the \textit{retrieval augmented generation} framework augments the context of our \textit{prompts} with relevant information to address the knowledge limitations of \textit{generative models}.
In this chapter, we will take a detailed look at the components of the \textit{RAG} framework and how they interact with each other to overcome the limitations of generative models.

\subsection{Retrieving relevant information}
A critical part of making this framework work well is finding relevant information to a user query. If the information passed to the \textit{generative language model} is not relevant to the query, the \textit{generative model} will not be able to return a qualified answer to the query. The search for relevant information therefore seems to be one of the most important components of the framework for it to be successful. In a \textit{RAG} setup, the system receives a user query in the form of a \textit{string} and must find relevant information from a database based on this \textit{string}. Since the query is delivered in \textit{string} form, it is not easily possible to perform a classic \textit{SQL} database query to find the desired information. To be able to easily find and store data in the form of a \textit{string}, we make use of \textit{natural language processing (NLP)} algorithms to convert given texts into vectors.

Transforming texts and documents\footnote{I will use the words document and text interchangeably in this work, both mean the same thing in the context of this work.} into vectors of the same dimension and thus into the same \textit{vector space} helps to perform meaningful retrieval operations \cite{vectorSpaceModel}. A \textit{vector space} offers the property that the similarity between vectors in this \textit{vector space} can be calculated. We make use of this property in \textit{information retrieval} to map documents and user queries in the same \textit{vector space} and calculate the similarity between them. A similarity function indicates how close or distant vectors are from each other in the same \textit{vector space}. The closer together these vectors are, the more likely we assume that they have a similar meaning. Therefore, it is important to find a procedure that transforms similar documents into a similar \textit{vector representation} so that they are close together in the \textit{vector space}.

\subsubsection{Vector representations of text}
There are different ways to represent texts as vectors, which can be divided into two fundamental categories, which we will both use later to perform \textit{information retrieval}, the so called \textit{sparse vectors} and \textit{dense vectors}. Both representations are described as follows.

\subsubsection*{Sparse vector representation} 
To represent documents as sparse vectors, extensions of a \textit{bag of words} model are often used. A \textit{bag of words} model considers the words of a document independently of each other and counts them \cite[pp.~13ff]{eisensteinNLP}. When splitting documents into smaller units such as words or sub words, we also speak of \textit{tokens}\footnote{In the context of \textit{bag of word} models, I continue to write about words instead of \textit{tokens}, as these work on a word basis.}. The dimension of the resulting vector is equal to the number of our unique \textit{words}, the size of the \textit{vocabulary} of the \textit{bag of words} model. Each component of the vector is clearly assigned to a \textit{word} from the \textit{vocabulary}. The exact values of the vector components are created when a document is \textit{vectorized}. For each \textit{word}, the occurrence frequency in the document is counted and stored in the associated vector component. However, the vectors obtained in this way do not take into account the \textit{importance} or \textit{rarity} of a \textit{word} in the entire document.

A first extension of the \textit{bag of word} model is \textit{tf-idf}, short for T\textit{erm Frequency Inverse Document Frequency}. \textit{Tf-idf} adds a weighting to the vector representation to capture the meaning of a word in a document relative to the frequency of the word in the entire corpus (all documents).

\paragraph{Dense vector representation}

\subsubsection{Storing in searching documents}

\section{Implementation}
\subsection{Embedding strategy}
\subsection{Metrics}

% bibliography =====================================================
\newpage
\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c7f65fa-be86-4bb8-89de-2d59ba06d81a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -U langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a412b9c-125a-47da-838b-959f460ea20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from langchain_openai import ChatOpenAI \n",
    "from langchain.evaluation import load_evaluator\n",
    "from pprint import pprint\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "open_ai_auth = os.getenv(\"OPEN_AI\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = open_ai_auth\n",
    "\n",
    "evaluation_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee7cee-7553-480d-9cde-bb1cd6b26647",
   "metadata": {},
   "source": [
    "# Causal models on SQuADv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b604f8-73be-47b8-81f7-ae511cc382b1",
   "metadata": {},
   "source": [
    "## Get scores from GPT-3.5 tubo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca06e042-b545-44da-92bd-eae00a924050",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [20:17<00:00,  2.44s/it]\n",
      "100%|██████████| 500/500 [24:00<00:00,  2.88s/it] \n"
     ]
    }
   ],
   "source": [
    "results_path = Path(\"./results/\")\n",
    "\n",
    "evaluator = load_evaluator(\"labeled_criteria\", criteria=\"correctness\", llm=evaluation_llm,requires_reference=True)\n",
    "\n",
    "results = dict()\n",
    "\n",
    "for file in results_path.glob(\"*csv\"):\n",
    "    # read results csv\n",
    "    df = pd.read_csv(file)\n",
    "    df = df[[\"model\", \"question\", \"answer\", \"context\"]]\n",
    "    model = df[\"model\"].values[0]\n",
    "    results[model] = []\n",
    "    result_rows = []\n",
    "    with tqdm(total=len(df)) as pbar:\n",
    "        for i, (model, question, answer, context) in df.iterrows():\n",
    "            eval_result = evaluator.evaluate_strings(\n",
    "                prediction=answer,\n",
    "                input=question,\n",
    "                reference=context\n",
    "            )\n",
    "            pbar.update(1)\n",
    "            \n",
    "            results[model].append(eval_result)\n",
    "            result_rows.append(eval_result)\n",
    "            # OpenAI API Tokens per minute limit\n",
    "            if i+1 % 50 == 0:\n",
    "                time.sleep(60)\n",
    "            pd.DataFrame(result_rows).to_csv(f\"{model.replace('/', '-')}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ff3768-713e-440d-8a94-4139a0f14a4c",
   "metadata": {},
   "source": [
    "## Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a19eb31-f1de-4e72-85f3-4d5f5aaca4e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>correct</th>\n",
       "      <th>wrong</th>\n",
       "      <th>total</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HuggingFaceH4-zephyr-7b-gemma-v0.1.csv</td>\n",
       "      <td>337.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>500</td>\n",
       "      <td>0.674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google-gemma-2b-it.csv</td>\n",
       "      <td>387.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>500</td>\n",
       "      <td>0.774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HuggingFaceH4-zephyr-7b-beta.csv</td>\n",
       "      <td>427.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>500</td>\n",
       "      <td>0.854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mistralai-Mixtral-8x7B-Instruct-v0.1.csv</td>\n",
       "      <td>396.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>500</td>\n",
       "      <td>0.792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mistralai-Mistral-7B-Instruct-v0.2.csv</td>\n",
       "      <td>400.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>500</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      model  correct  wrong  total  percentage\n",
       "0    HuggingFaceH4-zephyr-7b-gemma-v0.1.csv    337.0  163.0    500       0.674\n",
       "1                    google-gemma-2b-it.csv    387.0  113.0    500       0.774\n",
       "2          HuggingFaceH4-zephyr-7b-beta.csv    427.0   73.0    500       0.854\n",
       "3  mistralai-Mixtral-8x7B-Instruct-v0.1.csv    396.0  104.0    500       0.792\n",
       "4    mistralai-Mistral-7B-Instruct-v0.2.csv    400.0  100.0    500       0.800"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "results_path = Path(\"llm-as-judge-results\")\n",
    "\n",
    "result_rows = []\n",
    "\n",
    "for csv in results_path.glob(\"*.csv\"):\n",
    "    df = pd.read_csv(csv, index_col=0).fillna(0.0)\n",
    "    correct = df.score.sum()\n",
    "    total = len(df)\n",
    "    percentage = correct / total\n",
    "    model = csv.name\n",
    "    \n",
    "    result_rows.append(\n",
    "        {\n",
    "            \"model\": model,\n",
    "            \"correct\": correct,\n",
    "            \"wrong\": total - correct,\n",
    "            \"total\": total,\n",
    "            \"percentage\": percentage\n",
    "        }\n",
    "    )\n",
    "result = pd.DataFrame(result_rows)\n",
    "result.to_csv(\"squad-v2-llm-as-judge-results.csv\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e018ae2f-96cb-4df6-980e-b7954203c90c",
   "metadata": {},
   "source": [
    "# Causal models on ml book dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a87ee7d-9ac3-4866-b308-3eaefc058f03",
   "metadata": {},
   "source": [
    "## Get scores from GPT-3.5 tubo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b1a1b5-ca49-47d6-af9c-332db65ac361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./results/ML_BOOK_RESULTS.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9e26a7d-6502-43b0-8196-4c6bd12f1376",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [06:13<00:00,  3.45s/it]\n"
     ]
    }
   ],
   "source": [
    "result_rows = []\n",
    "\n",
    "evaluator = load_evaluator(\"labeled_criteria\", criteria=\"correctness\", llm=evaluation_llm,requires_reference=True)\n",
    "\n",
    "\n",
    "with tqdm(total=len(df)) as pbar:\n",
    "    for i, (model, context, answer, question) in df.iterrows():\n",
    "        eval_result = evaluator.evaluate_strings(\n",
    "            prediction=answer,\n",
    "            input=question,\n",
    "            reference=context\n",
    "        )\n",
    "        pbar.update(1)\n",
    "\n",
    "        result_rows.append(eval_result)\n",
    "        # OpenAI API Tokens per minute limit\n",
    "        if i+1 % 50 == 0:\n",
    "            time.sleep(60)\n",
    "\n",
    "pd.concat([df, pd.DataFrame(result_rows)], axis = 1).to_csv(\"LLM-as-a-judge-ML-BOOK.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0330ef-6e36-4ba4-b444-2775294e063e",
   "metadata": {},
   "source": [
    "## Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1a93e1f-6414-47a3-947d-3bf48186bff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>correct</th>\n",
       "      <th>wrong</th>\n",
       "      <th>total</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HuggingFaceH4/zephyr-7b-beta</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.925926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HuggingFaceH4/zephyr-7b-gemma-v0.1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.851852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google/gemma-2b-it</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.925926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                model  correct  wrong  total  percentage\n",
       "0        HuggingFaceH4/zephyr-7b-beta     25.0    2.0     27    0.925926\n",
       "1  HuggingFaceH4/zephyr-7b-gemma-v0.1     23.0    4.0     27    0.851852\n",
       "2                  google/gemma-2b-it     21.0    6.0     27    0.777778\n",
       "3  mistralai/Mistral-7B-Instruct-v0.2     25.0    2.0     27    0.925926"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"LLM-as-a-judge-ML-BOOK.csv\", index_col=0)\n",
    "\n",
    "grouped = df.groupby([\"model\"])\n",
    "\n",
    "result_rows = []\n",
    "\n",
    "for name, values in grouped:\n",
    "    model = name[0]\n",
    "    correct = values[\"score\"].sum()\n",
    "    total = len(values[\"score\"])\n",
    "    percentage = correct / total\n",
    "    \n",
    "    \n",
    "    result_rows.append(\n",
    "        {\n",
    "            \"model\": model,\n",
    "            \"correct\": correct,\n",
    "            \"wrong\": total - correct,\n",
    "            \"total\": total,\n",
    "            \"percentage\": percentage\n",
    "        }\n",
    "    )\n",
    "\n",
    "pd.DataFrame(result_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
